How is Login Logout Time Tracking for Employees in Office done by AI?
When people hear AI they often think about sentient robots and magic boxes. AI today is much more mundane and simple—but that doesn’t mean it’s not powerful. Another misconception is that high-profile research projects can be applied directly to any business situation. AI done right can create an extreme return on investments (ROIs)—for instance through automation or precise prediction. But it does take thought, time, and proper implementation. We have seen that success and value generated by AI projects are increased when there is a grounded understanding and expectation of what the technology can deliver from the C-suite down.
“Artificial Intelligence (AI) is a science and a set of computational technologies that are inspired by—but typically operate quite differently from—the ways people use their nervous systems and bodies to sense, learn, reason and take action.”3 Lately there has been a big rise in the day-to-day use of machines powered by AI. These machines are wired using cross-disciplinary approaches based on mathematics, computer science, statistics, psychology, and more.4 Virtual assistants are becoming more common, most of the web shops predict your purchases, many companies make use of chatbots in their customer service and many companies use algorithms to detect fraud.
AI and Deep Learning technology employed in office entry systems will bring proper time tracking of each employee. As this system tries to learn each person with an image processing technology whose data is feed forwarded to a deep learning model where Deep learning isn’t an algorithm per se, but rather a family of algorithms that implements deep networks (many layers). These networks are so deep that new methods of computation, such as graphics processing units (GPUs), are required to train them, in addition to clusters of compute nodes. So using deep learning we can take detect the employee using face and person recognition scan and through which login, logout timing is recorded. Using an AI system we can even identify each employee’s entry time, their working hours, non-working hours by tracking the movement of an employee in the office so that system can predict and report HR for the salary for each employee based on their working hours. Our system can take feed from CCTV to track movements of employees and this system is capable of recognizing a person even he/she is being masked as in this pandemic situation by taking their iris scan. With this system installed inside the office, the following are some of the benefits:
1)Compliance/litigation needs
For several countries, regulations insist that the employer must keep documents available that can demonstrate the working hours performed by each employee. In the event of control from the labor inspectorate or a dispute with an employee, the employer must be able to explain and justify the working hours for the company. This can be made easy as our system is tracking employee movements
2)Information security needs
This is about monitoring user connection times to detect suspicious access times. In the event where compromised credentials are used to log on at 3 a.m. on a Saturday, a notification on this access could alert the IT team that an attack is possibly underway.
3)Employee login logout software
To manage and react to employees’ attendance, overtime thresholds, productivity, and suspicious access times, our system records and stores detailed and interactive reporting on users’ connection times. These records allow you to better manage users’ connection times and provide accurate, detailed data required by management.
4)If you want to avoid paying overtime, make sure that your employees respect certain working time quotas or even avoid suspicious access. Our system will alert the HR officer about each employee’s office in and out time so that they can accordingly take action.
5)Last but not least it reduces human resource needs to keep track of the records and sending the report to HR and HR officials has to check through the report so this system will reduce times and human resource needs
With the use of AI and Deep Learning technologies, we can automate some routines stuff with more functionality which humans need more resources to keep track thereby reducing time spent on manual data entry works rather companies can think of making their position high in the competitive world.
Blackcoffer Insights 33: Suriya E, Vellore Institute of Technology


How does AI help to monitor Retail Shelf watches?
With increasing computing power and more data, the potential value of algorithms became higher. People and companies saw possibilities to embed these smart systems into their companies to drive strategy and innovation. As the power of algorithms, computing, and amounts of data increased companies started to see an increasing amount of use cases. AI started to become an essential value. Companies saw that these systems could move them closer to their customers, drive efficiency, enhance employee experience and capability, automatize tasks, decrease costs and improve revenue. But AI went through some lulls and spikes before it became capable of enabling so many benefits. At a certain point, the general public gave up on AI and machine learning as a whole, which stalled developments and investments.
AI uses computer vision and deep learning to achieve these results. Computer vision incorporates neural networks that help computers detect objects within shelf images and classify them accurately.
Deep learning is a new area that is moving machine learning closer to one of its original goals: AI. In deep learning, a computer system is fed a lot of data, which it then observes and analyzes to recognize emergent patterns and trends, ultimately generating appropriate recommendations and predictions.
Deep learning applications, especially in the field of computer vision, are more than ready for prime time. Recently, a neural network architecture won the ImageNet challenge by classifying a wide variety of objects – like birds, ships, and cats – into the correct categories more accurately than humans did.
Back at the store shelf, even subtle differences between objects, poor lighting, reflections, and background clutter can be overcome with these powerful AI applications.
Our tool recognizes products among thousands of possible retail items and verifies their placement according to the given planogram. This translates into the automation o shelf management activities, centralized monitoring of shelf arrangement over hundreds of branches, and validation of the product, license agreement for suppliers.
The use of ML-powered analytics solutions can help businesses forecast inventory demand with high accuracy. Systems that rely on machine learning are capable of analyzing a multitude of data points, finding subtle patterns (indicating changes in customer preferences, behavior, or satisfaction), which can be non-obvious to humans. What’s more, these systems don’t require explicit programming as machine learning models learn from data. The more data is fed to them, the better they handle it while deriving meaningful insights.
Our system will automate manual tasks like gap and price scanning and free up your staff to help shoppers. It increases availability and provides real-time, accurate alerts to restock shelves faster than ever and increase on-shelf availability.it will transform the supply chain. Drive supply chain decisions with real-time shelf data to understand actual demand and avoid inaccurate inventory management and comply with price and promotions. Automatically detect mismatches at the shelf and avoid customer frustration. Our system will harmonize online and offline channels. Optimize online order picking and reflect your real availability to the shopper as they make their online order
By this system, if it is implemented in a supermarket, where we can use this application to predict the market price and demand and accordingly we can forecast the demand and take actions in prior and this system can also keep track of the expiry of each perishable products and alerts for them to change it accordingly at the correct time. If some new product is released in the market and that is with great demand, our system will try to analyze where it is placed in the store, If it is not present in the store our system will report to the sales manager to add these new products in the store at a much reachable and visible place for the buyers.
Blackcoffer Insights 33: Suriya E, Vellore Institute of Technology


AI and its impact on the Fashion Industry
If you were a fan of the 90’s film Clueless back in the day, then you’ll remember the protagonist, Cher Horowitz’s amazing virtual wardrobe. She used it to browse her clothing and choose a perfectly coordinated ensemble. This virtual application, which was just the brainchild of a writer wanting to make the protagonist look rich, fashionable, and ahead of her time, ignited a buzz and prospected having an automated style device to make everyday dress-up fun and engagingly time-saving.
Times have changed and technological advancement today is transforming everything from probabilities to possibilities. We are in the era where, machines not just facilitate our tasks and demands but rather suggest, forecasts, and analyze thus making lives simpler and smarter.
With the advent of technology, style suggestions are just a fraction of the big picture that AI has painted in the fashion industry today.
What is AI?
Artificial intelligence is the creation of computer programs capable of performing activities and solving issues that would normally need human intelligence. AI has surged across a variety of industries, with the potential to transform businesses through creative technologies and more effective operational processes.
At first, AI automation did not appear to be an appealing tool for fashion leaders to use in an industry focused on creative ability and expression. However, as we enter the hyper-digital era, these apps have the potential to revolutionize enterprises and generate considerable industry growth when compared to competitors that use traditional approaches.
Some of the most well-known brands in the business are now investing in algorithms that assist buyers to choose designs. A slew of AI-based start-ups is also assisting everyone from retailers to customers in removing the guesswork from the equation.
Impact on fashion
This article examines how artificial intelligence has impacted key areas of the fashion industry, as well as explores brands that have benefited from its use.
AI in Apparel designing
Fashion firms are utilizing technology to better understand client wants and produce better garments thanks to more sophisticated data collection.
Tommy Hilfiger pioneered the “Reimagine Retail” project, which trains fashion designers with AI design skills in collaboration with IBM. As a result, fashion students could master a variety of technical skills such as natural language processing (NLP) or computer vision. Fashion students could learn from thousands of fashion-related photos using AI, which increased their inventiveness and shortened lead times for the fashion firm.
AI in the Manufacturing process
Fashion brands are now able to identify fast-changing fashion trends and get the latest fashion accessories to store shelves faster than the “traditional” fashion shop, thanks to AI and machine learning capabilities. As a result, prominent fashion brands such as Zara, Top Shop, and H&M can provide immediate gratification to retail customers by identifying seasonal trends and producing the appropriate quantity of the current items.
AI in Logistics
AI in inventory and supply chain management helps to speed up processes by optimizing routes and lowering logistics and shipping costs. Companies use AI to automate logistics and supply chain procedures for speedier delivery or to locate alternate routes for vehicles that have been detoured due to unanticipated situations like bad weather or road construction.
Owing to lockdown, major fashion firms had to rethink their go-to-market strategy overnight, with actual brick-and-mortar stores closed and people staying away from shopping malls. Myntra finds itself in a unique position to assist them.
Myntra moved its whole data infrastructure, including supply chain management, inventory, and website capabilities, to Microsoft Azure just before the pandemic. Apart from giving Myntra the flexibility to respond to demand spikes, Azure’s built-in Machine Learning technologies sped up the development of advanced analytics capabilities, allowing them to better understand their customers.
AI in Fashion Retail
In retail, AI and machine learning are also providing an automated solution to monitor customer activities while shopping and visualize their sentiments to learn what products they prefer to buy and what products they ignore.
AI can also track traffic in retail stores or record consumer shopping experiences, with the option of receiving feedback on how their experience was while shopping at the store, allowing them to enhance their services.
Uniqlo has AI-powered UMood kiosks that offer clients a selection of products and use neurotransmitters to assess their reaction to color and style. The kiosk then makes product recommendations based on each person’s reactions. Customers don’t even need to press a button for the system to know how they feel about each item; their brain signals are plenty.
AI Fashion Stylist — Styling the Fashion Accessories
Furthermore, AI in fashion is enabling each of us to find those elusive perfect garments that suit our body types and fashion preferences.
These AI-enabled garments and ensembles are personalized to the user’s style, body type, colors, and current fashion trends, as well as different situations and weather.
iLUK is an AI-based personal stylist that uses Computer Vision and 3D Reconstruction technology at its core to provide technology-based personal styling. It’s shaped like a pod that will be placed in a store.
AI in Fast Fashion with Smart Mirror
Similarly, a smart mirror driven by AI is being utilized by a company to streamline consumers’ shopping experiences by allowing them to virtually visualize how items would appear on them without having to put them on their bodies.
The AI smart mirror with touch screen glasses is mounted in the changing room of retail stores and relays information on whether or not a person is inside. Customers can also use this mirror to try on other sizes and colors, as well as receive tailored mix-and-match alternatives to complete the appearance.
Van Heusen designed a storage space that includes a “Virtual Trial” mirror that allows customers to view how clothes might appear on them by scanning the item’s barcode and stepping in front of the mirror while virtual clothing is projected onto their reflection.
AI in Ecommerce
In the same way that AI is revolutionizing retail fashion stores, AI is revolutionizing online purchasing and E-commerce. While browsing or searching for fashion goods on e-commerce sites, AI suggests additional items that are similar to what you’re looking for based on your color preferences, budget, and other factors. It analyses your search history data and suggests more relevant stuff you should look at.
Amazon has undoubtedly transformed the online shopping experience with its AI-powered product suggestion engine. Amazon is implementing an AI-enabled fashion designer algorithm that can create clothing by mimicking the design styles of several popular garments and applying them to a new garment. The Echo Look fashion assistant, which uses machine learning to deliver personalized recommendations, is Amazon’s other use case.
AI in Visual Search — To Find the Products Using Camera
AI-based visual search technology is now employed by E-commerce stores to comprehend the content and context of these photographs and produce a list of related results. You can capture an object using your camera and then search for it online. Retailers can use AI-enabled computer vision-based visual search technology to propose thematically or aesthetically relevant items to customers in a way that would be difficult to do with only a word query.
Neiman Marcus, a high-end department store, employs artificial intelligence to make it easier for shoppers to locate things. The Snap. Find. Shop. the app allows users to photograph items they encounter while out and about, and then search Neiman Marcus’ inventory for the same or a comparable item.
AI in Fashion and Sustainability
One of the most damaging businesses on the earth is the fashion industry. Artificial Intelligence can be used in conjunction with Machine Learning, Deep Learning, Natural Language Processing, Visual Recognition, and Data Analytics to reduce trend prediction errors and more precisely forecast patterns, leading to fewer garments being manufactured and subsequently underutilized.
The H&M Group is using “Amplified Intelligence,” which combines analytics and AI with human intelligence.
Using artificially intelligent technologies, H&M is enhancing its ability to recognize trends and organize logistics, as well as minimizing the frequency of discounted deals and large amounts of unsold goods.
Though intelligence per se is artificial, nevertheless is likely to have an earnest impact, both positive and negative:
The fashion dilemma
Apparel manufacture is a labor-intensive industry in the fashion industry. AI-enabled machines and robots can perfectly stitch fabrics while also detecting fabric flaws and providing quality assurance to ensure that the actual design hues will match the new colors.
In nations like Bangladesh, where the garment industry accounts for 80% of the GDP, this will have a significant influence on the labor force in the long run. It is already feeling the heat from these specialized AI-based devices, thus jeopardizing the jobs of breadwinners.
AI should not be viewed as a rival, but rather as a collaborator. The cost and ethics of AI are currently preventing us from progressing. We must be careful not to use technology to increase inequity or exacerbate social injustice. We also need to find a balance and integrate humanity into the machines we’re constructing in the future.
Virtual wonderland
“We are being monitored and classified by AI in portions of our lives that were not previously watched,” says Sophie Hackford, a futurist and keynote speaker. She wonders if we’ve built the “wrong” internet. We created it intending to monetize our viewers in mind, rather than the dynamic knowledge-sharing area that Tim Berners-Lee envisioned at the start.” She believes that in the future, we will be much more selective in how we obtain information, and that “bots” would be critical in meeting our requirements. As a result, the gains will come at the expense of our privacy.
As numerous as AI’s advantages are, it is impossible to ignore the difficult issues it brings. With everything being data-driven, there is a pressing need to establish boundaries to build a shared ecosystem that benefits both businesses and the general public.
Conclusion
Finding a wise balance between physical stores and online stores will continue to be a crucial issue in an industry where brick-and-mortar retail locations are still an important element of the sector’s approach to business.
The usefulness of chatbots aimed at improving online and in-store product navigation shows significant promise for preserving customer attention and use in the near term. We expect consumers to become accustomed to these capabilities over time, and the field to become more competitive as AI becomes more broadly applied, as prominent brands set the tone with their usage of AI.
It’s too early to tell how these AI applications will affect earnings and cost savings because prominent fashion firms are still in the early stages of AI implementation. However, there will be a learning curve as corporations figure out how consumers react to these innovation efforts, which could have a direct influence on sales.
Blackcoffer Insights 33: Lakshman Upadhyay and Prabhlin Kaur Matta, Welingkar Institute of M D R, Mumbai


How do deep learning models predict old and new drugs that are successfully treated in healthcare?
Understanding exactly how data is ingested, analyzed, and returned to the end-user can have a big impact on expectations for accuracy and reliability, not to mention influencing any investments necessary to whip an organization’s data assets into shape. To efficiently and effectively choose between vendor products or hire the right data science staff to develop algorithms in-house, healthcare organizations should feel confident that they have a firm grasp on the different flavors of artificial intelligence and how they can apply to specific use cases. Deep learning is a good place to start.  This branch of artificial intelligence has very quickly become transformative for healthcare, offering the ability to analyze data with a speed and precision never seen before. Deep learning, also known as hierarchical learning or deep structured learning, is a type of machine learning that uses a layered algorithmic architecture to analyze data. In deep learning models, data is filtered through a cascade of multiple layers, with each successive layer using the output from the previous one to inform its results. Deep learning models can become more and more accurate as they process more data, essentially learning from previous results to refine their ability to make correlations and connections. Deep learning is loosely based on the way biological neurons connect to process information in the brains of animals.  Similar to the way electrical signals travel across the cells of living creates, each subsequent layer of nodes is activated when it receives stimuli from its neighboring neurons. In artificial neural networks (ANNs), the basis for deep learning models, each layer may be assigned a specific portion of a transformation task, and data might traverse the layers multiple times to refine and optimize the ultimate output.  These “hidden” layers serve to perform the mathematical translation tasks that turn raw input into meaningful output. With the crypto anchors and blockchain employed with the medicine for example each pill of medication will be printed with scannable details which contains the time of production, originality, etc, which can be printed in medicine using edible substances. Once the medicine is shipped all over the world we can scan the medicine with a scanner and find out whether the medicine is original or counterfeit with the blockchain details stored at the time of manufacturing. By using deep learning in medicines these neural networks will make sense of all the unrelated data and make them into understandable and scalable data so that can be used for making the medicine efficiency better by analyzing molecules of each component used in the medicine.
Blackcoffer Insights 32: Suriya E, Vellore Institute of Technology


How artificial intelligence can boost your productivity level?
From the stone age to the modern world, from hunting and gathering to cultivating crops, from living in caves to building communities, we have come a long way to become the most evolved creature on the planet. So, what is that one thing which is a guiding force throughout this process? I believe in simple words we can say: to enhance the experience of life, or in other words, to boost our productivity level. Once the basic need of food, cloth, and shelter is taken care of, we want a comfortable life, good education, proper healthcare, and the list goes on. This urgency and endless need pushed us to make breakthrough inventions that made human life more comfortable and productive. First, humans invented electricity, then the computers came, after that portable computing devices like laptops and mobile phones became operative, and now artificial intelligence or AI is a buzzword. Every creation by human beings transformed human life on the face of this planet. From living in the mud house to building skyscrapers, from riding a bullock cart to flying an aircraft, from posting letters to making video calls, and now from computers to AI or Artificial intelligence is yet another groundbreaking invention of humankind, which can revolutionize our lifestyle forever!
#Artificial_Intelligence #productivity #problem_solving #technology #machine #human
So, what is Artificial Intelligence or AI? We can say in simple words that AI is the ability of a computer program to think and learn rationally. It is about pattern recognition, analyzing images and videos and learning, natural language processing, and understanding. It is more about creating a technology that can, in some cases, replace what humans do. However, to me personally, the most crucial part of the definition of AI is the application to solve problems intelligently and rationally using algorithms that enables us to have computers find deep patterns that we may not have even known exist. AI is like a complex series and layers of algorithms that create a neural network that processes data and information coming into the system and derives knowledge from it. AI is a complicated and broad concept, where Machine Learning is a subset of AI, and Deep Learning, a specialized subset of Machine Learning.
According to a study by PWC, $16 trillion of GDP will be added between now and 2030 based on AI. The economic impact of this scale has never been seen before. And it is not just in the IT industry but impacts virtually every industry and aspect of our lives.
So, now the question is how people around us are using this enigmatic technology?
We use AI all the time. But mostly, we are unaware of it. We use AI whenever we type in a search query on a search engine. Every time we use our GPS or whenever we use any voice recognition system. What we can do with artificial intelligence is unlimited. Whenever we do anything on any technology, we are using some form of machine learning or artificial intelligence. AI means different things to different people. AI facilitating machines and humans to understand and interact with each other creating new opportunities and new ways of doing business. For example:
A videogame designer will use AI to jot down the code that affects how bots play and observe their reaction.
A data scientist can use AI to explore and classify data to fulfill specific goals. 
Law enforcement agencies can use AI to identify criminals in video feeds.
Top of Form
Bottom of Form
In the educational sector, AI can provide students with conversational interfaces and on-demand online tutors.
Customer service Chatbots improved customer experience by resolving queries on the spot and freeing up agent time for conversations that add value.
Chatbots powered by natural language processing capabilities are employed in healthcare to interact with patients and run diagnoses like real doctors.
AI-powered advances in speech-to-text technology have made real-time transcription possible.
Self-driving cars, facial recognition in social media apps, and image detection are possible because of AI.
Watson, Alexa, Siri, Cortana, and Google Assistant are also a gift of AI.
In Medicine, AI is helping doctors arrive at more accurate preliminary diagnoses, reading medical imaging, finding appropriate clinical trials for patients. It is not just influencing patient outcomes but also making operational processes less expensive. It is helping patients with Lou Gehrig’s disease, for example, regain their original voice in place of using a computerized voice.
AI is impacting the quality of our lives daily. There’s AI in our Netflix queue, our navigation apps, keeping spam out of our inboxes, and reminding us of important events. AI monitor our investments, detecting fraudulent transactions, identifying credit card fraud, and preventing financial crimes. AI has the potential to access enormous amounts of information, imitate humans, even specific humans, make life-changing recommendations about health and finances, correlate data that may invade privacy, and much more.
So, what we have understood is AI has penetrated our day-to-day life. Now the question is how people can use it in the best possible way to extract value from it and boost their productivity even further? Here are some of my suggestions:
Empowering People with Disabilities: More than one billion people around the globe live with any form of disability, which accounts for around 15% of the population. AI or Robot if used correctly, can turn out to be a blessing for these people. AI can provide preliminary support to make their life a little easier and efficient. For example, the world renounced scientist Stephen Hawkins lived most of his life with the assistance of AI and other technology. Likewise, for the paralyzed people AI or a robot can help to do basic day-to-day activities. For a blind person, AI can work as a personal assistant who can provide text-to-speech translation, writing assistance, and support in other regular activities. Whether a person is blind, deaf, or paralyzed, it doesn’t matter. AI can be a life-changing tool for them.
Boosting the quality of self-study: AI can transform our educational system completely. Just imagine how it would have been if there weren’t any means of doing online education in this pandemic and lockdown? Our entire education would have come to a halt. I believe post-pandemic, the world may shift more to an online medium, both for work and education. Other than that, we sometimes face difficulties in self-study. Their artificial intelligence can help by understanding, explaining, and providing the correct information at the right time. Just imagine how efficient a student’s life would have been if they had a robot as a tutor 24*7!
Women Safety: AI can be of enormous help in providing safety, protection, and security services.
Personal Assistant: It would have been so convenient for us if we had a Personal Assistant. People like us can’t afford to keep a personal assistant for them to have a robot who can keep an account of our daily work would have been so convenient and productive.
Helping Hand in the Household Chores: Household chores is an essential factor that impacts our daily life. It will be a blessing for every household to have a robot to do the basics chores of cleaning, cooking, washing, carrying heavy loads, and doing other repetitive jobs. It will save a lot of time, and we can invest that time in more productive and creative activities.
Blackcoffer Insights 32: Dolon Biswas, Praxis Business School


How are genetic sequencing maps affected by deep learning and AI?
Artificial intelligence (AI) is the development of computer systems that can perform tasks that normally require human intelligence. Advances in AI software and hardware, especially deep learning algorithms and the graphics processing units (GPUs) that power their training, have led to a recent and rapidly increasing interest in medical AI applications. In clinical diagnostics, AI-based computer vision approaches are poised to revolutionize image-based diagnostics, while other AI subtypes have begun to show similar promise in various diagnostic modalities. In some areas, such as clinical genomics, a specific type of AI algorithm known as deep learning is used to process large and complex genomic datasets.  AI has two main applications in genetics: identification of harmful genes and treatment of disease. Let’s take a look at how each of these applications works. Also, with some good GPS trackers, you can implement them on your vehicle. AI is also being used to identify genetic mutations within tumors using 3D imaging. AI is very useful in personalized medicine, which requires treatments to be specified to one patient’s needs versus another. The 0.1% of our DNA that is unique to us has more than three million differences.
The rise of AI in genomics is unsurprising. Genomics – a ‘big data’ field – requires computational approaches to interrogate the enormous volume of data generated by sequencing technologies and to marry it in meaningful ways with other biological and clinical data. Analyzing these datasets for new biological insights can be especially difficult when the rules have to be explicitly predefined, step by step, within the computer code. Instead, machine learning techniques can learn from data without the need to specify explicit rules.
Although a highly useful tool, AI is not without challenge the key issue of AI in genomics is scale – with the amount of genomics data being generated exponentially growing. Slavé Petrovski, Head of Genome Analytics and Informatics at AstraZeneca’s Centre for Genomics Research (GCR) says that – “AI in genomics can be extended across different omic studies, such as transcriptomics.” Although there are challenges of having the infrastructure and resources to cope with large datasets and mine it effectively if managed correctly the problem is eliminated.
Slavé Petrovski highlighted a key piece of research, conducted by AstraZeneca, which presented a multi-dimensional machine learning framework, taking into account 52 layers of information including gene expression, human disease literature, and mouse phenotypes. This approach is proposed as a “support framework for objectively and quantitatively triaging potential novel disease target genes.
Another focus within genomic AI is the strengthening of data. Petrovski remarks that this area is “constantly evolving,” but a key observation is that the method adopted is often not as important as the underlying data. This means that the information inputted into AI systems must be of high quality or it cannot be used to its full potential.
Genetics is always a data-driven science and largely utilizes machine learning to capture dependencies in data and derive biological hypotheses. However, the ability to extract new insights from the exponentially increasing volume of genomics data requires more expressive machine learning models. By effectively combining large data sets, deep learning has transformed the field, and now, it is becoming the method of choice for many genomics modeling tasks, including predicting the impact of genetic variation on gene regulatory mechanisms such as DNA accessibility and splicing.
In recent years, deep learning has been widely used in diverse fields of biology. In biology, applications of deep learning are gaining increasing popularity in predicting the structure and function of genomic elements, such as promoters, enhancers, or gene expression levels.
The application of deep learning to genomic datasets is an exciting rapidly developing area and is primed to revolutionize genome analysis. Deep learning has been successfully implemented in areas such as image recognition or robotics (e.g., self-driving cars) and is most useful when large amounts of data are available. In this respect, using deep learning as a tool in the field of genomics is entirely apt. Although it is still in somewhat early stages, deep learning in genomics has the potential to inform fields such as cancer diagnosis and treatment, clinical genetics, crop improvement, epidemiology and public health, population genetics, evolutionary or phylogenetic analyses, and functional genomics. Also, in recent years, deep learning (DL) methods have been considered in the context of genomic prediction. The DL methods are nonparametric models that provide flexibility to adapt to complicated associations between data and output and adapt to very complex patterns. The applications of deep learning (DL) methods in genomic selection (GS) to obtain a meta-picture of GS performance and highlight how these tools can help solve challenging plant breeding problems. We also provide general guidance for the effective use of DL methods including the fundamentals of DL and the requirements for its appropriate use. The main requirement for using DL is the quality and sufficiently large training data. There is clear evidence that DL algorithms capture nonlinear patterns more efficiently than conventional genome-based. Deep learning algorithms can integrate data from different sources as is usually needed in GS and it shows the ability for improving prediction accuracy for large breeding data. It is important to apply DL to large training-testing data sets.
Functional genomic analysis in the field in which deep learning has made the most inroads to date. The availability of vast troves of data of various types (DNA, RNA, methylation, chromatin accessibility, histone modifications, chromosome interactions, and so forth) ensures that there are enough training datasets to build accurate prediction models relating to gene expression, genomic regulation, or variant interpretation. Other features are identification of long noncoding RNAs or splice-site prediction can also be analyzed. As more data becomes available, better models will be able to be trained, thus resulting in even more precise and accurate predictions of genomic features and functions.
Although deep learning holds enormous promise for advancing new genomics discoveries, it should also be implemented mindfully and with appropriate caution. Deep learning should be applied to biological datasets of sufficient size, usually on the order of thousands of samples.
Even though the use of AI and deep learning can solve many problems related to genomic data, the use is challenging and not cost-effective at all. It needs high-quality machines and skilled experts. But as it was said that – ” If it’s worth it then it’s nothing to worry about.” But if we succeed in applying the concept of deep learning and AI in genetics with cost-effectiveness it will create a history in the world of science and biology and it is possible that humans can reach new heights of genetics and the human genome.
Blackcoffer Insights 32: Manika Gupta, Shivaji College, University of Delhi


How is AI used to solve traffic management?
Artificial intelligence (AI) is the most important branch of computer science in this era of big data. AI was born 50 years ago and came a long way, making encouraging progress, especially in machine learning, data mining, computer vision, expert systems, natural language processing, robotics, and related applications. Machine learning is the most popular branch of AI. Other classes of AI include probabilistic models, deep learning, artificial neural network systems, and game theory. These classes are developed and applied in a wide range of sectors. Recently, it has been the leading research area in transportation engineering, especially in traffic congestion prediction. Traffic congestion prediction provides the authorities with the required time to plan in the allocation of resources to make the journey smooth for travelers.
Fig: Traffic management
AI TO TACKLE ROAD TRAFFIC CONGESTION:                        
AI traffic management is poised to revamp urban transportation, relieving bottlenecks and choke-points that routinely snarl our urban traffic. This will help us reduce not only congestion and travel time but will also reduce emissions. Traffic congestion mostly occurs due to the negligence of certain factors like distance maintained between two moving vehicles, traffic lights, and road signs. congestion leads to higher fuel consumption, increased air pollution, unnecessarily wastage of time & energy, chronic stress & other physiological problems, whereas higher traffic violations are the major cause of road fatalities. Intelligent traffic management systems refer to the usage of AI, machine learning, computer vision, sensors, and data analysis tools to collect and analyze traffic data, generate solutions, and apply them to the traffic infrastructure. AI can use live camera feeds, sensors, and even Google Maps to develop traffic management solutions that feature predictive algorithms to speed up traffic flow. Siemens Mobility recently built an AI-based monitoring system that processes video feeds from traffic cameras. It automatically detects traffic anomalies and alerts traffic management authorities. The system is effective at estimating road traffic density to modulate the traffic signals accordingly for smoother movement. The integration of artificial intelligence and machine learning will support many smart transportation applications including emergency vehicle preemption, transit signal priority, and pedestrian safety.
AI IN AVIATION TRAFFIC CONTROL
Air traffic management stands to benefit significantly from artificial intelligence (AI) under its reliance on repetitive activity – which lends itself to analysis and machine learning. In addition, much of the complexity is embedded in the driving factors that deliver safe air traffic control: for example flight planning, flow management, safety assessments, and conflict prediction.
FUTURE OF AI IN TRAFIC MANAGEMENT
Every year a large number of new vehicles appear on streets worldwide, contributing to traffic congestion. AI and deep learning technologies can play a vital role in managing traffic by collating and analyzing data from a variety of sources. Insights from them can accelerate traffic movements. The future of AI in traffic management would give birth to smart AI applications that automate and accelerate processes.
ADVANTAGES OF AI IN TRAFFIC CONGESTION :
The use of Artificial Intelligence (AI) and Intelligent Traffic Management (ITMS) can help tackle & solve many such issues related to traffic law enforcement. It can facilitate a smooth, convenient & safer experience for commuters by improving road traffic discipline. Such a system can’t only help law enforcement agencies in penalizing the offenders but can act as a deterrent as well. Intelligent Traffic Management System (ITMS) is an Artificial Intelligence (AI) based application coupled with cameras installed at the traffic junctions, which provide a tool to detect and identify vehicles disobeying traffic rules and generate real-time alerts at the Central Command Center. It can automatically challan the offenders as per law of the land and can send it electronically to the violators. Each e-challan is associated with supporting evidence data in terms of snapshots & videos. This system can also track the flow and pace of traffic movement to provide real-time traffic management. In addition to improving traffic conditions, ITMS also contributes towards smart city goals like environmental sustainability. To improve traffic behavior & avoid congestions in a city, a Traffic management & enforcement system can be deployed to solve traffic-related issues, including but not limited to – Automated Number Plate Reading, Red Light Violation Detection, Speed Violation Detection, Triple Riding & No Helmet Detection, Free Left Turn Obstruction, Wrong-Way Driving, No Seat Belt & Mobile phone usage while driving, Hot listed vehicle detection, Traffic Flow & Congestion detection, etc.
Such an ITMS system can be integrated with the existing CCTV & Traffic Control system to provide a holistic solution towards preventing the current traffic menace. Not only this, various AI-based traffic analytic can be deployed over existing Traffic enforcement and surveillance system also, with minor tweaking & adjustments, resulting in a highly optimized solution at a lower cost to the public exchequer.
An AI-powered system, trained in deep learning can reduce traffic. Commuters can plan their journeys as per system updates. Since it takes every small detail of roads into account, an AI-powered system will keep the drivers and road authorities one step ahead. Here are a few significant advantages of having an AI-powered system for traffic analysis:-
1) Lane-by-lane analysis
The system can analyze traffic in different lanes with enough precision to ease traffic movements. Using the data, drivers can reduce their commuting time significantly. Effectively, drivers and road authorities can make better decisions to avoid traffic congestion.
2) Real-time updates to keep driver one step ahead
Every critical information about road conditions, from road accidents to congestion, puts driver and road authorities one step ahead.
3) Wide coverage
An AI-powered system has the potential to analyze traffic across multiple regions. It can map the most efficient routes and alter traffic signals to improve traffic conditions.
4) Fuel minimization
AI and machine learning algorithms identify the least-efficient vehicles, track their path and speed and change the traffic signals ahead of the vehicles.  This eliminates much of the inefficient starting and stopping at intersections and minimizes fuel consumption.
APPLICATIONS OF AI IN TRAFFIC CONTROL
As far as using Artificial Intelligence in open space, such as roads, is concerned, it can be considered to be a possibility to install an AI-enabled traffic management system as part of the infrastructure before launching the self-driving vehicles. An AI-enabled traffic management system can provide greater leeway to self-driving vehicles as they can then be directed and controlled more by the external environment.  Recently, the Delhi Traffic Police has obtained permission from the Ministry of Home Affairs to set up an intelligent traffic management system (ITMS) which will work on radar-based monitoring with the help of Artificial Intelligence. One of the key tools in the system will be automated traffic signals. It will help the Delhi Traffic Police to analyze the traffic pattern, volume, number of vehicles and collect them on a cloud which will be further used to manage the traffic. It will also contain features like high-resolution CCTV cameras to capture commuters and motorists breaking the laws and automated number plate recognition cameras (ANPR) to directly send the challan (fine slip) to the homes of lawbreakers. With Artificial Intelligence working with traffic signals based on traffic volume, the congested roads can be opened up for a longer duration for more traffic to clear up leading to a reduction in congestion.
CONCLUSION
Traffic congestion prediction is getting more attention from the last few decades. With the development of infrastructure, every country is facing a traffic congestion problem. The development of artificial intelligence and the availability of big data have led researchers to apply different models in this field. Deep learning algorithms became more popular with time as they can assess a large data set. However, a wide range of machine learning algorithms is yet to be applied. Therefore, a vast opportunity for research in the field of traffic congestion prediction still prevails. The power of AI that propels a lot of the data analysis of these systems, is also what powers the navigation systems of ride-hailing as well as last-mile delivery operators. Deliveries or order fulfillment that had to be delayed in the past, can now be enhanced significantly as cities around Asia continue taking up such intelligent traffic systems.
Blackcoffer Insights 32: Swapna. G, Sri Manakula Vinayagar Engineering College(SMVEC), Puducherry


Benefits of Big Data in Different fields
“In God we trust, all others must bring data.” – W. Edwards Deming.
2,000,000,000,000,000,000 bytes. The figure is roughly the amount of data generated per day across all industries
Every individual in 2020 created approximately 1.7 megabytes in a second. Which when seen in a bigger picture every day, internet users create 2.5 quintillion bytes of data. Looking at this gigantic source and generation of raw data, the requirement turns out to manage unstructured data by organizations. This seems to be a point of concern by 95 percent of organizations and hence a major share of organizations is (around 97.2 percent) are investing in big data and artificial intelligence.
According to IBM, in 2020, every person generated 1.7 megabytes per second
According to big data statistics by Forbes, data production, capture, copying, and consumption increased by 5000% between 2010 and 2020.
Statista say that in 2020, the big data market grew by 14%
As per reports, banking and financial sector generates tremendous amount of data, which if calculated on a per second manner will grow 700% in 2021
Big Data
The concept of “big data” means data sets that are in terms of quantity are complicated and huge. Big data are information that includes hidden information such as trends in the market, undiscovered relationships, patterns, consumer preferences, which are required to be discovered. Big data sometimes represents how huge volumes of data are collected and analyzed, enabling companies to make informed business choices. Traditional data processing apps cannot handle big data.
Big data has from many sources, including social media networks, sites (browsers), mobile applications, text messages, geographic sites of wearable devices, other media files, etc., creates and aggregates it as we saw earlier. Sensors and Internet-of-Thing’s (IoT) devices are the major drivers for the growth of the market in Big Data.
The big data market is expected to be valued at 103 billion US dollars globally by the end of 2021. Which is double the size of the market from 2018. The software business, which is 45 percent of the market share, is expected to become the largest data market sector by 2027.
Viz 1 – Growth of data in last 11 years (Percentage of total).
Note: The amount of data taken for 2021 is the predicted total at the end of this year.
The amount of data on the other hand is growing at an alarming rate. We can see from the above Gantt chart that more than 75% (i.e., a total of 2017, 2018, 2019, 2020, 2021 is 78.06%) of the total amount of data present in the world was generated in the last 5 years.
Viz. 2 – Predicted exponential growth of data volumes in zettabytes, up to 2025. Information Source: Statista
From this bar chart above we can clearly see the exponential rise in the volume of data in the world is unprecedented and predictably will reach more than 180 Zettabytes of data by 2025.
Data are created daily and companies are seeking an innovative approach to organize, manage, analyze and save data so that the competitive advantage is achieved, procedures are simplified and competition remains ahead of time. In a 2016 report from Gartner, companies are using big data to tackle complex business problems and decision making, and over 48% of the companies worldwide were investing in the domain.
The impact of big data on business
With the ever-increasing quantity of data, companies seek creative ways of optimizing Big Data and making decisions using the insights they create. One of the biggest impacts of Big Data on companies is that they are increasingly dependent on the Internet and the amount of data that technology is creating rapidly. The worldwide atmosphere by 2025 is forecast by Seagate IDC to be 175 zettabytes.
Big data delivers accurate information (if properly processed) and brings forth the latest insights for companies to make intelligent and quicker decisions. The big data analysis platforms also enable companies to increase company efficiency, in terms of operations, marketing as well as product development, and gaining a competitive advantage over their competitors.
In addition, companies are striving to improve their income through the use of big data to provide better services to their clients. The basic objective of most organizations is undoubtedly is to enhance the client experience. Among additional goals that may be reached with Big Data must include enhanced marketing strategy, cheaper costs, and more operational efficiency.
Big data has provided companies with social and financial benefits in recent decades, leading to policies that support the growth of Big Data in various governmental bodies.
In various industries such as banking, finance and insurance, retail, manufacturing, logistics, media and business, oil and gas big data is being employed.
The impact of big data on the workforce
Apart from the core business processes big data has a tremendous influence on the workforce of today.
Many companies are currently using the Internet to communicate with their clients, sell and boost productivity. Experienced data scientists, Big Data Engineers, and analysts are thus on demand because they can exploit the huge data sets that are created by the internet.
Numerous colleges and universities are already offering Big Data online classes. This is because data scientists and big data engineers are increasingly in demand. The need for data scientists is predicted to increase by 16 percent by 2028, according to research from India.
Due to the shortage of data scientists and engineers, there is a significant technological gap. The analytical roles of companies are thus still empty in many businesses because they lack technical ability. In the future, we will be able to manage a large number of unstructured data, new software, and technologies, and encounter fewer organizations that lack qualified people to do achieve the same.
As a result, companies depend more on bots and RPAs to automate dull repetitive operations such as data collection, data input, and cleaning data, thus freeing up valuable human resources to do more subjective tasks and gaining insights from big data collected by the company.
The impact of big data on society
Big data influences organizations as we saw, that impacts the economy and that in turn can impact society as a whole as well as the big data technologies themselves. Thus, creating a circle of innovation that benefits all parties. Big data utilization has enhanced the degree of industrial automation, safety, and confidentiality in the manufacturing and healthcare industries.
The weather forecasts, natural catastrophes, urban and community planning, traffic control, logistics and machine efficiency, tailored medical treatment, self-sustaining cars, fraud detention, translation, intelligent housing, robotics, etc. may also be exploited with the help of big data in the coming years and decades.
Big Data Analytics Combined with Machine Learning
The future of big data isn’t solely dependent on the innovations in that specific field. The advancement of big data analytics is heavily linked with the advancements in other related emerging technologies, such as machine learning, artificial intelligence, deep learning, etc.
Focusing on machine learning, we can say that big data is very nicely compatible with the workings of machine learning algorithms. Machine learning algorithms are useful in data analysis and big organization’s integration. It may be performed in many aspects of big data operations such as data labeling and segmentation, data analysis, and simulation of scenarios.
Here are some examples of ways to examine big data using machine learning algorithms:
Conducting market research and help in segmentation: All enterprises require consumers and understanding their target audience is the top priority for every successful enterprise. Market research and segmentation study are thus a key activity for further exploration of new and present consumers and for gaining valuable knowledge. Machine learning can properly analyse the patterns and behaviour of the target audience utilising supervised and unsupervised algorithms. Machine learning is used by the media and entertainment sector to obtain audiences’ preferences and thus generating content that catches the attention of the customers.
Customer Behaviour Exploration: Machine learning will continue to assist organisations learn about their consumers and provide a strong foundation for their customers once you have done a research on market and segmentation. This is termed user modelling, a direct outcome of interactions between people and the computer. It leverages data to collect people’ ideas and makes data driven decision making possible for companies. In order to help its users, by comprehending appropriate recommendations, Tech giants such as Facebook, Twitter or Google rely on user modelling systems.
Personalization of recommendations: Nowadays, customers demand customization. Whether it is a smartphone or a network series, companies must build strong contacts with their consumers so that their experiences are relevant and personalised. Thus, organisations may view their user’s behaviour forecasts in a specific environment in combination with the power of big data and machine learning, thus offering correct suggestions, making modifications at the appropriate moment and enabling consumers to discover interesting suggestions. Netflix is an example of a company that utilises machine learning techniques coupled with big data to provide viewers with appropriate recommendations.
Trends in forecasting: We can learn and predict future trends as a business via machine learning technologies to analyse Big Data. Machine learning networks can constantly autonomously learn new things and enhance their analytical abilities on a daily basis using networked computers. This allows for data calculation and the application of past experience as an intelligent system for the future.
Helping decisions to be taken: Machine learning utilises a time series analysis approach that can jointly evaluate a large amount of data. It is a fantastic instrument in which to summarise and analyse data so that management can decide more swiftly in future and take data driven decisions.
According to statistics, digital transformation, technological advancements, and breakthroughs continue to be the primary drivers of the rising amount of big data.  Big data is already enormous, but it is anticipated to rise exponentially as new technologies emerge, such as more widespread IoT devices, drones, and wearables, and computing devices increase. Around Ninety percent of the world’s big data has been created in the previous two to three years. With the enormous bulk of raw data; businesses and industries require proper analysis of those unstructured data sources to draw more insights by effectively and efficiently converting them into useful information. Modernization coupled up with rapid industrialization has put much pressure on data culture and mining, hence paving the way to complex analytical tools to culture and introspect data thereby extracting out the essence out it, those recent advancements in deep learning are also helping businesses decipher this valuable trove of information. Big data and business analytics solutions have become mainstream technologies, and they, together with AI and automation, provide the foundation for the digital transformation process.
Blackcoffer Insights 31: Sourojyoti Ghosh and Soumadip Sen, International Management Institute, Kolkata


How Big Data Will Impact the Future of Business?
Big data refers to large sets of unstructured, semi-structured, or structured data obtained from numerous sources. Among the sources are customer databases, medical records, business transaction systems, social networks, mobile applications, and scientific experiments. No matter what business you’re in, chances are big data has already transformed it — and if not, it soon will.
Big data has already changed our lives in dramatic ways and is poised to become an even more crucial component of the business world. If you’re not prepared for how big data will impact your company, it’s time to start looking forward.
Today, companies are focusing on overhauling their data architecture, consolidating data, and discarding legacy systems. Big data has a great impact on businesses since it helps companies efficiently manage large volumes of data. Usually, this type of data is aggregated over social media networks, web browsers, geo-locations, fitness apps, text messages, and other media files. Some of the key drivers of big data market growth are sensors and IoT devices.
Big data is already everywhere. You’re contributing to it every time you watch a show on Netflix, post something on Twitter, or have a quick phone call with a business. In this blog, we’re going to explore how big data will affect the future of business.
Why is Big Data Important?
Data plays a huge role in understanding valuable insights about target demographics and customer preferences. From every interaction with technology, regardless of whether it’s active or passive, we are creating new data that can describe us. With data being captured through products, video cameras, credit cards, cell phones, and other touchpoints, our data profile is growing exponentially. If analyzed correctly, these data points can explain a lot about our behavior, personalities, and life events. Companies can leverage these insights for product improvements, business strategy, and marketing campaigns to cater to the target customers.
Big data will change how even the smallest companies do business as data collection and interpretation become more accessible. New, innovative, and cost-effective technologies are constantly emerging and improving which makes it incredibly easy for any organization to seamlessly implement big data solutions. Interpreting big data allows companies to create new growth opportunities by better understanding their target audience. Your data can also offer an insight into which of your team members are the most productive, or how you can make your office more efficient.
Netflix is an excellent example of the value of big data. In October 2019, Netflix had over 163.5 million subscribers. The company revealed that using big data saves up to $1 billion a year on customer retention. That’s because Netflix better understands how to tempt you with the shows you’ll actually want to watch.
Big data contributes to the business intelligence that allows companies to improve their return on investment. With every insight you glean from your data, you’re one step closer to reducing the amount you waste on marketing campaigns that don’t work or strategies that don’t deliver. Big data:
Gives you a complete end-to-end view of your customer’s journey
Helps you understand your customer’s sentiment towards you
Shows you how to improve employee performance
Boosts your chances of enhancing your product’s potential
Highlights gaps in your market strategy
Impact of Big Data on Business
With the help of big data, companies aim at offering improved customer services, which can help increase profit. Enhanced customer experience is the primary goal of most companies. Other goals include better target marketing, cost reduction, and improved efficiency of existing processes.
Big data technologies help companies store large volumes of data while enabling significant cost benefits. Such technologies include cloud-based analytics and Hadoop. They help businesses analyze information and improve decision-making. Furthermore, data breaches pose the need for enhanced security, which technology applications can solve.
Big data has the potential to bring social and economic benefits to businesses. Therefore, several government agencies have formulated policies for promoting the development of big data.
Over the years, big data analytics has evolved with the adoption of agile technologies and the increase of focus on advanced analytics. There is no single technology that encompasses big data analytics. Several technologies work together to help companies procure optimum value from the information. Earlier, big data was mainly deployed by businesses that could afford the technologies and channels used to gather and analyze data. Nowadays, both large and small business enterprises are increasingly relying on big data. Thereby, they boost the demand for big data. Enterprises from all industries contemplate ways of how big data can be used in business. Its uses are poised to improve productivity, identify customer needs, offer a competitive advantage, and scope for sustainable economic development.
As global data continue to increase, organizations are even looking for new ways to optimize these data and make an informed business decision while remaining competitive. One of the major impacts that big data has on businesses is the growing amount of data generated due to the increased reliance on the internet and the rapid advancement in technology. In fact, a report by Seagate IDC forecast that the global data sphere will reach 175 zettabytes by 2025. Big data also provides fast and actionable data insight to help businesses make informed decisions. By using analytical data platforms, companies can improve their operation, automate tasks, remain competitive, and design more big data use cases
Businesses will have to learn to manage their data.
Privacy is paramount
Communication skills will be key.
Storing and managing data will get more challenging
There may be a big data talent crunch on the horizon
How Big Data Is Used in Businesses Across Industries
Banking, Financial Services, and Insurance (BFSI)
Retail
E-commerce
Manufacturing
Logistics, Media, and Entertainment
Oil and Gas
Big Data Applications in Business:
Improving the customer experience-Automation
One of the most popular ways to leverage big data in the current landscape is to access information as a way of improving customer experience. Experts agree that customer experience is the key to success in today’s landscape. With big data analytics, you can unlock the sentiment, preference, and requirements hidden in your conversations with customers.
Transforming the hiring process-In depth insights
People are the most important asset in any business. It’s not just your customers that matter, but the employees that serve those clients daily. Today’s companies are often struggling to find the right talent to add to their teams. Unfortunately, slipping up and making the wrong choice with a candidate is a costly issue. Experts say that the cost of a bad hire is up to 30% of that person’s annual earnings. With big data, however, you can feed analytics engines and AI algorithms that tell you exactly who to hire based on the skills you need.
Improving decision making
Big data can improve your ability to make the right decisions in almost every area of your business. Because data is the key to fuelling things like analytical apps and artificial intelligence, it’s also crucial to delivering the kind of insights leaders need to move their company forwards. When plugged into the right applications and cloud computing services, big data analytics can give you answers to some of your most pressing questions. For instance, you could use data from a workforce optimization tool to determine what your employees need to do their best work. You can also use big data analytics to decide where to invest money into extra products for potential peaks in demand. Your data tools could even tell you when you’re likely to need more staff in your contact center based on previous trends.
How to take advantage of the power of big data
Start with a goal
Experienced big data team
Identify end goals
Capture the right data
Apply proper analytic methods like CRM and contact centre tools, Workforce analytics, Business intelligence
Implement strategies to drive innovation
Conclusion
Finally, big data is indeed the future of the enterprise; it provides an endless opportunity for organizations to outperform their peers. The use of big data will also lead to an increase in more job categories, new regulatory structures and standards, and a shift by organizations from data generation to the use of actionable data in gaining insight.
The escalating need for analyzing data will lead to the rise of demand for big data over the forecast period. Furthermore, the number of online businesses in the industry is also growing, owing to enhanced profit margins. Other industries, such as healthcare, utilities, and banking, will widely use online platforms to provide improved services to customers.
Smart application of big data in everyday business life allows making data-driven decisions and responding promptly to market developments having its direct business impacts to the bottom line. The competition in all industries is constantly increasing giving businesses no chance for mistakes and requiring end-to-end analytics and technology-based tactics implemented to stay competitive.
Big data analytics is the future of business
Ultimately, in a world where experience is key and businesses are constantly fighting to differentiate themselves, there’s nothing more valuable than data.
Big data provides companies with the scale and insights they need to develop a realistic path for growth.
The right big data technologies will show you how to connect with your customers on a deeper level, and what you can do to make your team more efficient.
Data gathering systems also give you the power to streamline business operations and reduce your outgoing costs, while building your bottom line.
In the years to come, any business that isn’t taking advantage of big data will be missing out on endless opportunities.
Blackcoffer Insights 31: Keshav Gupta, Dr. Akhilesh Das Gupta Institute of Technology and Management


How Will AI Make Decisions In Tomorrow’s Wars?
Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by humans or animals. Leading AI textbooks define the field as the study of “intelligent agents”: any system that perceives its environment and takes actions that maximize its chance of achieving its goals.
AI applications include advanced web search engines, recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri or Alexa), self-driving cars (e.g. Tesla), and competing at the highest level in strategic game systems (such as chess and Go). As machines become increasingly capable, tasks considered to require “intelligence” are often removed from the definition of AI, a phenomenon known as the AI effect. But as it is said that everything comes at a price, the same is the case with AI. It has many advantages but at the same time, it has disadvantages. And one of the most prominent ones is USE of AI in WARS.
Artificial intelligence isn’t only a consumer and business-centric technology. Yes, companies use AI to automate various tasks, while consumers use AI to make their daily routines easier. But governments–and in particular militaries–also have a massive interest in the speed and scale offered by AI. Nation-states are already using artificial intelligence to monitor boundaries and defenses. Militaries are doing more research and development of artificial intelligence (AI), and are looking to implement AI systems. In early August of this year, the U.S. Defense Advanced Research Projects Agency announced that later in the month a human fighter pilot would face off against an AI algorithm in virtual combat. The leading military powers have already made it clear that they want to profit from the opportunities offered by this promising technology.
Many nations like the U.S.A, U.K, China have already started investing in AI for their military.
If the nations keep investing their military funds in AI military development then consequences can be devastating and may lead to the elimination of the entire human species. The usage of robots who are never tiring and non demanding in wars can be fatal. During war times the difficulties that are most faced are shortage of food, weapons, human resources, and tiredness of soldiers. But AI solves them all. The machines don’t need food, they are never tired and the loss of human lives can also be prevented.
If AI will be introduced in wars the wars will become ever-ending. The powerful nations will try to suppress the weak ones. The wars will never end. The huge impact of wars will destroy all human civilization.
Even though AI is a revolutionary step in the IT and research industry its devastating steps are not far from what is seen.
If AI is introduced in futuristic wars, wars will be unimaginable. Robotic armies and humanoids will be used in wars which will. The robotic armies will invade territories like wild creatures leaving no mercy on anyone. This all will lead to only devastation and destruction. As years ago in Hiroshima and Nagasaki, nuclear blasts have caused havoc similar havoc will be caused by the use of AI in militaries and wars but the consequences will be far worse than before and they may be as worse as human civilization’s elimination from the Earth. Or the consequences can be worse as taking an example of the establishment of robotic territories that will start destroying humans or want them to be slaves.
We all can only make assumptions. None of us have seen the future.
Blackcoffer Insights 31: Manika Gupta 


Which one is better AI or big data?
The big data industry has grown at an incredible rate as businesses realize the importance of insightful data analysis. We will compare how big data corresponds to artificial intelligence, what are the similarities and what are the differences between them, whether any one of them is better than the other or how the combination of both leads to results beyond traditional human capability.
Artificial Intelligence
Artificial Intelligence is defined as the ability of a machine to apply logic and reason to analyze, interpret inputs, and, ultimately, make decisions. It is the recreation of human intelligence with machines.
There are numerous, real-world applications of AI systems today. A few of them are online chatbots, Recommendation engines, Speech Recognition, Computer vision, etc. We all have AI, assistants, on our phones like Google Assistant, Apple’s Siri, Samsung’s Bixby, Amazon’s Alexa, and many more applications that are using speech recognition systems. Online retailers are using recommendation engines to suggest relevant recommendations to customers during the checkout process. computer vision has applications within photo tagging in social media, radiology imaging in healthcare, and self-driving cars within the automotive industry.
Big Data
Big data comes into play when the volume of data is too large for traditional data management practices to be effective. Big Data is a field focused on managing a large volume of data from a variety of sources. 
The essence of big data can be broken into “the three v’s of big data”:
Volume: The amount of data being collected
Velocity: The rate at which data is received and acted upon
Variety: The different forms of data collected, (structured, Semi-Structured, and unstructured data sources).
Similarity
AI and Big Data are both data-driven technologies.
Differences
The difference between artificial intelligence and big data lies in the output of each. Artificial intelligence analyses inputs to learn and improve its sorting or patterning processes over time, using data that it gathers to provide a more accurate solution. In contrast, big data is the information that is accumulated from various data sources, to then be analyzed by artificial intelligence. Big data and artificial intelligence are often used in conjunction with one another, but each fulfills very different roles, one is information and the other is the application of that information.
The collection and storage of large volume data i.e., Big Data is used by Machine learning models. Based on these Machine learning models Artificial Intelligence makes decisions.
Comparison
AI uses data but its ability to analyze, learn from this data, make decisions, solve complex problems depends upon the quality of information that is fed to the system. Big Data provides this vast of this information to train AI. By harnessing big data resources, artificial intelligence systems can make more informed decisions, provide better user recommendations. Most big companies like Google, YouTube, Amazon, Netflix, Spotify, and many others constantly collect and analyze user interaction data with their platforms maintaining user data security rules. Then based on these user data they run data models to predict user demand, taste, and behavioral patterns. As a result of which, these platforms are able to provide optimized recommendations for what to watch or listen to next. Google’s own feature phone, the Google Pixel, has the best camera because it uses AI to improve the quality of the pictures. This AI is so good because it was trained on data Google got from a trove of images on the Internet.
Conclusions
Big data makes it possible for Artificial Intelligent (AI) to reach its fullest potential. There is no artificial intelligence without big data.
However, Big data and artificial intelligence are interdependent meaning they cannot exist without one another. If one is taken out of the picture, then another one cannot exist. Big Data and AI are two technologies of posterity which do not compete but complement each other.
Blackcoffer Insights 31: Indrani Chakraborty, Developer at TCS 


How robots can help in e-learning platforms?
If I asked what is the one thing that this pandemic taught us? I believe in one word it will be: Adaptability, the ability to change, upgrade and adapt yourself to the changing circumstances without losing your individuality. The crisis caused by Covid-19 tested our perseverance, strength, and adaptability in the best possible way. In other words, the survival of the fittest has begun!
The COVID-19 has resulted in the shutdown of schools and colleges worldwide, and more than 1.2 billion students are away from the physical classroom. As a result, education has dramatically changed with the rise of e-learning.
Most of The Educational Institutions, Universities, Schools, and Colleges across the world, specifically in India, followed the Traditional setup of face-to-face learning in the classroom before the outbreak of deadly coronavirus. Even though in the last ten years, various e-learning platforms emerged, like Byjus, Unacademy, Vedantu, Upgrade, and other international e-learning platforms like edx, udemy, Coursera but the pandemic and lockdown worked as a catalyst and amplified the speed of the process, and boosted the demand of the e-learning platform manifold times. The Platforms like BYJU’S, which is now the world’s most highly valued ed-tech company and have seen a 200% increase in the number of new students using its product, according to the company’s Chief Operating Officer, Mrinal Mohit. The school, colleges, and universities that were reluctant to change their traditional pedagogical approaches had to shift to an online mode of teaching and learning overnight. The teachers, students, and government are now dependent on this online mode of education, and even if we come out of this pandemic e-learning, and online mode of education is the future!
Fig: Online teaching is done by Robo
So what can we do to make the experience of online teaching and learning better, and how robots or AI can help in it?
There is no doubt that e-learning is flexible, time-saving, and can revolutionize the education system, but there are subsequent challenges as well, where we have to work upon to make the user experience better and effective. While talking about the challenges, the main problems are:
Connectivity: The crucial problem students and teachers faced was the internet connectivity issues. Internet is the most vital part of e-learning. A stable and good internet connection is compulsory to teach and learn through online mode, but poor and unstable network connectivity is the major obstacle, specifically in India, in learning and teaching online.
Gadgets and Technology: Effective online education is not possible without the right equipment and gadgets. The technical glitch we face can interrupt the process of learning. Therefore, we need sustainable, efficient, and affordable tools to support e-learning. 
Cost: The main problem with e-learning is the cost and the maintenance charges it brings with it, which is unaffordable for many. 
Personal Interaction: According to the survey I have conducted for writing this article, the main hurdle in online education is the Personal Interaction and connection between a teacher and a student. Face-to-face teaching and learning help to build that ecosystem and environment because it is not just about learning the subjects, but the network and relationship we build with the teachers and our classmates, which makes us the part of the community around which our life revolves as a student, and we develop as an individual. 
Authenticity: As we all know, an examination is a crucial part of learning, but taking exams through an online mode is of great difficulty. It is impossible to monitor so many students by a single teacher through a virtual screen. Therefore, students can easily take help of unethical means to give exams through an online mode. Hence the relevance of taking exams will get lost if we don’t find a solution to this problem.
Digital Literacy: Many people, particularly ones from the older generations and others, are neither habituated nor exposed to this technology from an early age. Therefore, they cannot teach or learn through online mode efficiently. Last year due to lockdown, many senior school teachers faced a lot of difficulties in teaching online. The sudden countrywide lockdown made them shift online without any prior experience. Hence created a lot of challenges for them.
Adaptability: E-learning is popular among adults and teenagers but not amongst young kids. They are not well trained to learn online. They get easily distracted and face way more difficulties learning and understanding the concepts through a virtual screen. They need undivided attention and an engaging environment to learn, which is not available in an e-learning platform.
Health: Constantly looking at the virtual screen puts a strain on our eyes which causes eye discomfort and vision problems. This problem is more severe amongst young kids. Children have to spend a prolonged period in front of the computer screen in e-learning platforms which might affect their eyes in the long run.
Inclusion: E-learning is not inclusive in many ways. Most of the e-learning platforms don’t take into consideration specially-abled individuals. And if they want to use e-learning platforms, it isn’t so user-friendly for them.
Copyright Issues: There is a copyright issue in e-learning portals. Someone can try to record the classes and use them for their benefit. A student who hasn’t register for a particular course can sit beside a friend who has enrolled for the same and gain knowledge without paying for it. In this way, there can be several other misuses of e-learning platforms.
I have conducted a survey where I have collected information from the teachers and students from all age groups who are active users of the e-learning platforms. The ten points mentioned above are the relevant issues they are facing in an e-learning platform.
So now the question is, how can we make their experience better by using AI and robots?
At first, I would start with the copyright issue. We can take the help of AI and robots to build systems that can detect the presence of a camera in the room where the student is present. This process can prevent any duplication. Secondly, we can use a robot to detect any other individual staring at the screen or taking class notes.
Therefore, adopting this kind of method will prevent the copyright issues that we face.
Next comes the point of inclusion. As I have already stated, most of the e-learning platforms do not consider specially-abled individuals. So to solve this problem, we can take the help of the robot to translate the teachings into code languages that they can understand. Robots can also give them dictations or assist them in writing. In this way, we can create a more inclusive ecosystem.
A robot can be an effective tool to overcome the language barrier. We can convert the teachings into different languages with the help of a robot. Then it can reach a larger audience, and people can learn in their mother tongue. This can be a revolutionizing step to promote diversity.
Another key aspect that we discussed is health! By taking the help of robots, and AI we can minimize the effects of visual discomfort and reduce the strain on the eyes. This can go a long way to protect kids and adults from computer vision syndrome.
Fig: Interaction with Robo in classroom
While talking about kids, I would bring the point of adaptability which, I have mentioned earlier. Kids need way more attention and an interacting environment to learn and enjoy the learning process. Therefore, e-learning portals can take the help of robots to make the learning process more engaging. They can develop a technology where the kids will feel that teachers are interacting on one-to-one basics with them, even in the classroom of thirty or forty students. This process might help them to grab more, keeping the interest in the subject intact and growing. Since the children are not well trained to use the complicated technology, we can make it more simple, handy, and adaptable.
But if we want to make e-learning portals the future of the education system, then we have to keep other things in mind. Learning is not just about academics. Building discipline, peer-to-peer relationships, teamwork, developing a strong sense of ethics, integrity, communication skills, and leadership skills are a part of the learning process. Hence, the e-learning portals can use robots or AI to implant this in the learning process to make it wholesome.
Blackcoffer Insights 31:  Dolon Biswas, Praxis Business School


How does Big Data Help in Finance and the Growth of Large Firms?
Big Data as the word suggests means a large amount of data. Data can be both in structured as well as unstructured format. Data can be collected from various sources such as mobile phones, social media, big transactions, videos, Internet Of Things devices, financial instruments like stocks, bonds, and news. With the increasing use of Internet, the data generated by these devices is also increasing rapidly, and this data is helpful to make projections for what is upcoming. The use of data is largely dependent on the company analyzing it. New technologies such as machine learning, deep learning, artificial intelligence are data-driven technologies. Large companies use this data to enhance the client experience and execute digital transactions with greater efficiency and minimal mistakes. Organizations need to understand the values of this data otherwise they would be left behind.
Big Data for Finance.
Big Data is completely revolutionizing the way the stock market used to operate. The reaction to any business, political news is seen in microseconds even before the common people are aware of it. Trading Firms used different sets of algorithms to extract data from websites much faster than naked eyes. Investors are relying on quantitative data to make investment decisions. This helps to remove the emotional stress before taking any trade.
The exponential growth in technology is changing the way industries and companies operate. The financial sector is usually considered a data-driven sector. Activities such as mergers and acquisitions, financial statements, accounting, and tax statements require a huge amount of data to be cleaned. There are trillions if not billions of dollars moving across different markets and continents. Many firms are responsible for analyzing such data with accuracy and security to create predictions, find patterns, and create different strategies. Because systems cannot store such huge amounts of data due to lack of space and security, analysts are using cloud services. Cloud-based solutions not only reduce the cost but also help in making the data accessible for anyone in need of it. This data can be used to analyze different business models, their profitability and create financial models based on them.
Use of Big Data in Finance
Accurate Analysis : Big data has an important advantage on manpower and that is the accuracy . Human are subject to make errors which cannot be avoided , but on the other hand computers don’t make mistakes . The importance of accuracy greatly increases when dealing in the financial markets mainly in stocks , foreign currency and crypto as well. A small change in the number might lead to a huge loss. Calculated decisions need to be made on the basis of financial and economical numbers such as inflation data , manufacturing data , monthly cement and auto sales data. Computers have a greater processing capability but this can only be achieved when data given to the computer is properly structured and made use of efficiently.
Risk Management : Big Data helps organization in managing their risks by providing real time insights and the change in customer behavior. Big Data can be used largely to analyze  if any attacks are been made to the system. It detects any illogical change in the system immediately and thus alerts the officials because it constantly analyses the data coming from the servers which is next to impossible for any human to do. Big Data also reduces the number of errors while trading in the financial markets such as you can set a limit to the position sizing to the order that is to be executed . Big Data can also be used to keep an eye on the liquidity to get better insights of the cash flow and to improve liquidity management. It can also be used to analyze the credit risk management system by analyzing transaction history , available cash , credit score etc. Frauds in accounting and taxation can be avoided by constantly analyzing the data.
Consumer Analytics : Today , consumer is an integral and most prominent part of any business . Big Data helps to analyze the data of these consumers which is generated in huge amounts and give them better service. Big Data can be used for more targeted marketing with faster reactivity. Companies are trying to understand the needs of the consumer and predict the future behavior , generate more leads. Consumers can be provided with the products that they need and hence using smart marketing strategies we can eventually increase our customer size. Example:
If a consumer has less money in his current bank account than the budget he requires annually , he can be provided with a short term loan .
If a consumer is searching for cars online more than his recent searches , a car loan will be a good option
New Products and Services : Big Data can be used to make a lot of new products than the conventional ones. Big Data can useful for creating a personalized products to each and every consumer . Each consumer has different set of need and this need to be taken as an opportunity . Big Data can be used to make quantitative decision  that usually had been a difficult task for human being.
Big Data can be used to identify the customer goals , the situation of the family , the risk appetite and propose financial advise , tax advisory and wealth management using this data.
Quantitative Trading can be implemented using Big Data as it can analyze large sums of data generated from the Stock Exchange every second . Also the sentiment of the market can be analyzed with the help of Big Data by analyzing blog posts , news articles , recent searches etc.
Customer Support : Customer is the most important part in any business and hence technology should be used to enhance their satisfaction from the firm. Large amount of data generated from the consumer can be used  to analyze the customer feedback from various sources , such as call centre , sending emails etc . It helps us to identify the problems faced by the consumer and helps us to modify the product or service provided to them. Technical feedback is much more efficient than paper feedback as personalized questions can be generated for each and every consumer. This helps to increase customer loyalty and retain more customers
Faster Data Insights : Articles that is published by any particular newspaper can be analyzed in a few seconds with the help of Big Data and decisions can be made within a few milliseconds . Example :
If a journal publishes an article stating management issues in a particular company and if we are holding equity shares of the company , decision would be made in a split second to sell the stock. Time is an important aspect in Stock Market and it can cause damage if decisions are slow.
These are among the few applications of Big Data in Finance. Big Data is and will be helpful to all aspects of any business. Let it be Marketing, Finance, Operations, or Human Resource as well. Around 1.145 trillion MB of data is generated per day and this data needs to be analyzed to generate more business and acquire new customers
Blackcoffer Insights 31: Akshat Hole, AISSMS COE


Future of Work: Robot, AI and Automation
Will my Uber driver be a robot? Could the next medicine prescription be from an app? Do robots make calls for my appointment in a hair salon, or some machine learning algorithm will be my new favorite music composer, and ultimately will AI replace my Dream job in the future? Well for a quick answer there are websites like willrobotstakemyjob.com to know whether you are likely to get fired due to technology or not. So will there be a technological revolution and if yes who will be its victims.
Robot and Employment
The possibility of such a revolution has always been opposed by claiming that computers can only think linearly, and they follow directions and commands that we humans give. But this is a misconception because with the advancement of machine learning, artificial intelligence is now able to go beyond carrying out instructions and it can now form concepts and act upon them. In 2014 Google was able to train a machine-learning algorithm to recognize cat videos on YouTube but the algorithm was not taught what cats look like. It learned by observation and by making generalizations and models to draw upon like an animal does and was able to develop a universal idea of what a cat face should be. Hence, there will be a wave of the technological revolution which is capable of replacing future jobs for humans. But the question is, when will it happen? There had always been false alarms in the past. But according to the latest reports ‘future of work has arrived early due to COVID-19 and may lead to 85 million jobs getting replaced in the near future.
Any Tasks that Can Be Learned or repetitive jobs are at risk of being wiped out. Information and data processing, administrative tasks, and regular manual work will be the primary focus of machines. Warehouse and manufacturing jobs, customer service, research, data entry, Accountants, Taxi Drivers, factory workers, truckers, paralegals, radiologists are some of the jobs that will get replaced soon. Telemarketing, Bookkeeping Clerks, Compensation and Benefits Managers, Receptionists, proofreaders, Computer Support Specialists, Market Research Analysts, Advertising Salespeople, Retail Salespeople will be the victims in management fields. Digital Medicines are almost here opening the possibility of getting diagnosed and treated by artificial intelligence by recording users’ voices, locations, facial expressions, exercise, sleep, and texting activity.
Even though a technological revolution can lead to wiping out of a mass number of jobs it will create more new job opportunities. Some jobs and sectors are going to witness historical growth in the future. Recent studies show that analytical thinking, creativity, and flexibility are among the top skills needed over the next five years, while data and artificial intelligence, content creation, and cloud computing are the top emerging professions. It will also increase the efficiency of employees thus creating more profit. For example, email marketing is now one of the most efficient ways of marketing where the employer can automatically analyze the effectiveness of campaigns. A technological revolution can also create a revolution in manufacturing which is the “quintessential escalator for developing economies”. Like any other new technology, artificial intelligence and robotics will initially be met with skepticism but eventually being embraced. Technological or robotics revolution will be a milestone in human history which will lead to the creation of more opportunities and a better future for millions.
Blackcoffer Insights 29: Akshara Jayan,  Sree Chithra thirunal college of engineering ,Kerala


How AI will help the Defense Power of a country?
Defense is crucial to a nation’s well-being and progress. For this reason, every country assigns tantamount importance to it. To protect its interests, a nation has to guard its land, seas, and the air (with the addition of space recently with the anti-satellite missile). Just like other sectors, technology has impacted defense as well. Continual improvements are being made in the design of guns, missiles, submarines, ships, planes, and so on. At the same time, efforts are being made to minimize human loss during wars. One of the best ways to do so would be to employ expendable machines that could do the work. The concept that enables machines to do so is artificial intelligence.
The immediate thing that might come to our mind is a Terminator-like soldier which will completely replace humans. However, that is not even close to reality because of the difficulties involved. At present, AI cannot do that level of multitasking, cannot react to new and difficult situations, and also can be confused by the enemies. Given below is a wonderful article by Lt. Col (Ret) Paul Maxwell is the Cyber Fellow of Computer Engineering at the Army Cyber Institute at the United States Military Academy which illustrates this.
However, as the author says, AI can be used in maintenance applications to determine if a system is going to fail or needs repair. A similar kind of application is to analyze the medical condition of a soldier and predict his operability so that he can be treated for it.
The best way right now is to enhance the overall power of the armed forces; the machines must act as force multipliers, assist humans, not replace them. This is the objective that countries are presently working to achieve.
Let us consider land warfare. Soldiers have to push into the battlefield, face enemy attacks and possibly sustain fatal wounds. Moreover, few terrains are too difficult and dangerous for humans to access. This problem can be reduced considerably using the following technologies: UGV (unmanned ground vehicle) and sentry guns.
UGVs come in various sizes from small bag-sized to medium-sized tanks. This is because of the variety of roles they are employed in. The small ones are ideal for reconnaissance purposes, mine detection, bomb disposal, and small assault operations. This is because of their small size which makes them difficult to detect and also enables them to access inaccessible places. India has already developed Daksh for the above purposes. Another notable example is Gladiator UGV in the US. The US is also trying to develop ball robots for the same purposes.
Autonomous tanks are useful because they assist the conventional tanks by acting as a security cover while also carrying out assault operations or carrying essential gear. The Milrem UGV is being used by Estonian troops in Mali in Africa for carrying supplies and ammunition. Mostly the technology is still under development but once approved, it will be very useful to a nation.
A link with UGV info is given below.
Sentry guns are static devices. They have basically powered machine guns that rotate about a fixed point and fire automatically at targets using their cameras and sensors. They are very useful when it comes to the defense of a place from invading enemies. A battery of these guns can be installed at various points around the specified place. Thus sentry guns are extremely useful because they prevent the defending soldiers from becoming exposed targets. South Korea uses this system at the demilitarized zone between North Korea and South Korea.
Autonomous technologies can also be applied in water. There are certain operations such as minesweeping or stand-by where there is a high threat to the ships involved. For these, we can use autonomous boats and UGV (unmanned underwater vehicles).
Autonomous boats are useful because they can protect bigger ships that are standing idle or exposed. They can also carry out reconnaissance and assault operations. The US Navy is currently trying to make such boats operational.
Underwater, the submarine is the combat vehicle. In present times, underwater warfare has become more complex due to the adoption of stealth technologies, like acoustic dampening and anti-submarine technologies which can be deployed from the air. This also increases the number of staff on submarines required to handle such situations and increases the threat of significant casualties. However, the use of UUV would significantly reduce these problems and thus countries like the USA, China whose navies are the largest in the world, are trying to incorporate this technology to increase their dominance.
AI has been implemented in aerial attacks very successfully in the form of drones or UAVs (Unmanned Aerial Vehicle). Almost all countries have implemented this technology. These aircraft are just like remote control planes, only a little bigger. Here AI is used to identify targets on the ground and communicate with the operator. India has Rustom, Netra, Lakshya, and various other drones/UAV’s. However, the US is trying to produce an unmanned fighter jet that can escort manned fighter planes, transport aircraft, and engage in aerial combat.
All the above technologies can work either independently or can be operated remotely. When they operate independently, they collect information from the battlefield using sensors, and using AI, it makes decisions. When used remotely, i.e. operated by a person from a different place they behave pretty much like remote control devices.
The great advantage of this technology is that human intervention and thus casualty is reduced. Also, the cost of producing such technology is considerably lower than producing manned machines. The most significant use involves ‘swarming’, which is similar to the swarming of bees around an intruder. When deployed in packs, they encircle the enemy and devastate them. Loss of these machines does not cause significant loss; they are expendables, easily replaceable.
Warfare has a new front nowadays: Cyber Front. The recent Solar Winds hack is a clear example for emphasizing this. As the world becomes increasingly digital, nations or entities (can be called terrorists) will look forward to exploiting this to their advantage. Most of these attacks have been done with one objective: extract sensitive and confidential data. Using this data, a country’s money can be stolen, electrical blackouts may be caused or even worse, the country’s defense can be paralyzed for facilitating the foreign attack. Coming to the size of data, it is HUGE (in PB). Processing this kind of volume requires techniques like big data. And to identify weak spots in millions of accounts or networks, hackers might be using AI and ML (Machine learning). Just like them, cybersecurity officials have to use AI or ML to predict and simulate cyber-attacks and identify weak spots in cybersecurity.  Now it comes down to who uses better technology and techniques. AI has to counter AI.
However, there is a concern that applies to all sectors using AI: Will AI replace humans? Will it rule us? The answer is yes if we use too much AI. The same applies in the defense sector also. The applications that have been discussed here act as force multipliers as long as humans control them. AI should never replace soldiers, but perform tasks that are too risky or impossible for humans. AI should not be allowed to become too intelligent nor they should be allowed to operate independently. Machine learning should be confined only to familiarize with battlefield situations, not to think ahead of us and act arbitrarily. Before such a device fires a bullet on its own, a human must be there to see that it happens according to his will. Another concern is hacking. What if the enemy hacks these devices? Such concerns must be addressed by adding multiple layers of encryption or limiting the amount of communication possible. Take, for example, a regular ‘nonsmart’ washing machine. We cannot communicate with it in any other way other than pressing its buttons. Nor can it do anything else other than washing clothes. This could be done with all the unmanned vehicles. Communication must be restricted to the user and the machine. The device should perform only its intended duty. If a nation learns to harness and balance the use of technology, then not only its defense sector but also all the other sectors will be stronger and ensure the nation’s holistic development.
Blackcoffer Insights 29: Tejas Hegde, M.S. Ramaiah Institute Of Technology


Future of AI and Machine Roles in the Medical Sector
Have you heard the movie Big Hero 6? It’s an animated movie in which this guy named Tadashi built AI machine Baymax, the inflatable healthcare robot, which could be very in army camps and place where the medical sector is not up to the mark but there is a plot twist it was later turned into a war machine by his younger brother. The whole idea of telling this is that the concept shown in it is now a reality that would come into effect in the coming years. AI is something that has drastically put an impact on our ways of working like if we take ourselves a few years back then no one can think of payment doing through the internet or buying things by just watching their pics that is what online shopping is. AI is making everything more effective, available to all and most importantly it does not have any boundaries.
The healthcare sector and the knowledge associated are complex. Health check-ups, today, are not just limited to checking the patient’s temperature or asking questions. With the amplified number of complex diseases, even technology has to get more advance. Modern tests, complex medication processes, and restful treatment require high-end technologies for better healthcare industry.  This where AI is making its way through in the medical industry because it can bring a drastic effect on the healthcare sector in many ways –
DEVELOPMENT OF RADIOLOGY TOOLS
Radiological images obtained by MRI machines, CT scanners, and x-rays offer non-invasive visibility into the inner workings of the human body.  But many diagnostic processes still rely on physical tissue samples obtained through biopsies, which carry risks including the potential for infection.
Artificial intelligence will enable the next generation of radiology tools that are accurate and detailed enough to replace the need for tissue samples in some cases, experts predict.
MAKING PATHOLOGY IMAGES MORE PRECISE
Pathologists provide one of the most significant sources of diagnostic data for providers across the spectrum of care delivery, says Jeffrey Golden, MD, Chair of the Department of Pathology at BWH.
“Seventy percent of all decisions in healthcare are based on a pathology result,” he said.
Somewhere between 70 and 75 percent of all the data in an electronic health record are from a pathology result. So, the more accurate we get, and the sooner we get to the right diagnosis, the better we’re going to be. That’s what digital pathology and AI have the opportunity to deliver. Analytics that can drill down to the pixel level on extremely large digital images can allow providers to identify nuances that may escape the human eye.
ADVANCE WEARABLES AND PERSONAL DEVICES FOR MONITORING HEALTH
Almost all consumers now have access to devices with sensors that can collect valuable data about their health.  From smartphones with step trackers to wearables that can track a heartbeat around the clock, a growing proportion of health-related data is generated on the go. Collecting and analyzing this data – and supplementing it with patient-provided information through apps and other home monitoring devices – can offer a unique perspective into individual and population health. Artificial intelligence will play a significant role in extracting actionable insights from this large and varied treasure trove of data.
SMARTPHONES AS POWERFUL DIAGNOSTIC TOOLS
Continuing the theme of harnessing the power of portable devices, experts believe that images taken from smartphones and other consumer-grade sources will be an important supplement to clinical quality imaging – especially in underserved populations or developing nations. The quality of cell phone cameras is increasing every year and can produce images that are viable for analysis by artificial intelligence algorithms. Researchers in the United Kingdom have even developed a tool that identifies developmental diseases by analyzing images of a child’s face.  The algorithm can detect discrete features, such as a child’s jawline, eye and nose placement, and other attributes that might indicate a craniofacial abnormality.  Currently, the tool can match the ordinary images to more than 90 disorders to provide clinical decision support.
Not just this but AI will be upgrading the health care industry in way more than this, completely changing it for. AI will begin the new chapter in the health care industry, it will change the traditional method of treatment and analysis. The doctor will able to treat their patient in a much more effective way, as AI will reduce the pressure on them. The doctor will be less likely to say, “they can’t help them it’s too late” to their patient. Who knows that one day will come and just like in the home, where we’re using Alexa, the future will bring virtual assistants with embedded intelligence to the bedside for humans to use for any medical problem.
Many would say that at the end of the day AI is just manmade creation, there will be some flaw in it, we can’t fully trust them, there can be any glitch in it, or what doctor can do AI will never be able to do. In the modern world with the existence of a pandemic situation, AI will be never able to take the place of a doctor but with the assistance of AI, a doctor will able to handle the situation more efficiently and with up gradation in modern technology, they will be well prepared of a situation like covid-19, hope it will never come in future. At last, AI will usher in a new era of mankind and exciting breakthroughs in the human world, since today AI and machine play a very important role in every human’s life, nowadays it is becoming as important as water for living.
Blackcoffer Insights 29: Mayank Gupta, Netaji Subhas University of Technology


AI in healthcare to Improve Patient Outcomes
Introduction
“If anything kills over 10 million people in the next few decades, it will be a highly infectious virus rather than a war. Not missiles but microbes.” Bill Gates’s remarks at a TED conference in 2014, right after the world had avoided the Ebola outbreak. When the new, unprecedented, invisible virus hit us, it met an overwhelmed and unprepared healthcare system and oblivious population. This public health emergency demonstrated our lack of scientific consideration and underlined the alarming need for robust innovations in our health and medical facilities. For the past few years, artificial intelligence has proven to be of tangible potential in the healthcare sectors, clinical practices, translational medical and biomedical research.
After the first case was detected in China on December 31st 2019, it was an AI program developed by BlueDot that alerted the world about the pandemic. It was quick to realise AI’s ability to analyse large chunks of data could help in detecting patterns and identifying and tracking the possible carriers of the virus.
Many tracing apps use AI to keep tabs on the people who have been infected and prevent the risk of cross-infection by using AI algorithms that can track patterns and extract some features to classify or categorise them.
So how does AI do that?
IBM Watson, a sophisticated AI that works on cloud computing and natural language processing, has prominently contributed to the healthcare sector on a global level. Being a conversational AI, since 2013, Watson has helped in recommending treatments to patients suffering from cancer to ensure that they get the best treatment at optimum costs.
Researchers at Google Inc. showed that an AI system can be trained on thousands of images to achieve physician-level sensitivity.
By identifying the molecular patterns associated with disease status and its subtypes, gene expression, and protein abundance levels, machine learning methods can detect fatal diseases like cancer at an early stage. Machine Learning (ML) techniques focus mainly on analyzing structured data, which can further help in clustering patients’ traits and infer the probability of disease outcomes. Since patient traits mainly include masses of data relating to age, gender, disease history, disease-specific data like diagnostic imaging and gene expressions, etc, ML can extract features from these data inputs by constructing data analytical algorithms.
ML algorithms are either supervised or unsupervised. Unsupervised learning helps in extracting features and clustering similar features together that further leads to early detection of diseases. Clustering and principal component analysis enable grouping or clustering of similar traits together that are further used to maximize or minimize the similarity between the patients within or between the clusters. Since patient traits are recorded in multiple dimensions, such as genes, principal component analysis(PCA) creates the apparatus to reduce these dimensions which humans could have not done alone.
Supervised learning considers the outcomes of the subjects together with the traits, and further correlates the inputs with the outputs to predict the probability of getting a particular clinical event, expected value of a disease level or expected survival time, or risk of Down’s syndrome.
Biomarker panels that are mostly used to detect ovarian cancer, have outperformed the conventional statistical methods due to machine learning. In addition to this, the use of EHRs and Bayesian networks, which are a part of supervised machine learning algorithms, can predict clinical outcomes and mortality respectively.
Unstructured data such as clinical notes and texts are converted into machine-readable structured data with the help of natural language processing(NLP). NLP works with two components: text processing and classification. Text processing helps in identifying a series of disease-relevant keywords in clinical notes and then through classification are further categorized into normal and abnormal cases. Chest screening through ML and NLP has helped find abnormalities in the lungs and provide treatment to covid patients. Healthcare organizations use NLP-based chatbots to increase interactions with patients, keeping their mental health and wellness in check.
Deep learning is a modern extension of the classical neural network techniques which helps explore more complex non-linear patterns in data, using algorithms like convolution neural network, recurrent neural network, deep belief network, and deep neural network which enables more accurate clinical prediction. When it comes to genome interpretation, deep neural networks surpass the conventional methods of logistics regression and support vector machines.
Sepsis Watch is an AI system trained in deep learning algorithms that holds the capability to analyze over 32 million data points to create a patient’s risk score and identify the early stages of sepsis.
Another method known as the Learning-based Optimization of the Under Sampling Pattern( LOUPE) is based on integrating full resolution MRI scans with the convolutional neural network algorithm, which helps in creating more accurate reconstructions.
Robotic surgery is widely considered in most delicate surgeries like gynaecology and prostate surgery. Even after striking the right balance between human decisions and AI precision, robotic surgery reduces surgeon efficiency as they have to be manually operated through a console. Thus, autonomous robotic surgery is on the rise with inventions such as robotic silicon fingers that mimic the sense of touch that surgeons need to identify organs, cut tissues, etc., or robotic catheters that can navigate whether it is touching blood, tissue, or valve.
Researchers at Children’s National Hospital, Washington have already developed an AI called Smart Tissue Autonomous Robot (STAR), which performs a colon anastomosis on its own with the help of an ML-powered suturing tool, that automatically detects the patient’s breathing pattern to apply suture at the correct point.
An image of STAR during surgery.
Cloud computing in healthcare has helped in retrieving and sharing medical records safely with a reduction in maintenance costs. Through this technology doctors and various healthcare workers have access to detailed patient data that helps in speeding up analysis ultimately leading to better care in the form of more accurate information, medications, and therapies.
How can It help in Biomedical research?
Since AI can analyze literature beyond readability, it can be used to concise biomedical research. With the help of ML algorithms and NLP, AI can accelerate screening and indexing of biomedical research, by ranking the literature of interest which allows researchers to formulate and test scientific hypotheses far more precisely and quickly. Taking it to the next level, AI systems like the computational modelling assistant (CMA) helps researchers to construct simulation models from the concepts they have in mind. Such innovations have majorly contributed to topics such as tumour suppressor mechanisms and protein-protein interaction information extraction.
AI as precision medicine
Since precision medicine focuses on healthcare interventions to individuals or groups of patients based on their profile, the various AI devices pave the way to practice it more efficiently. With the help of ML, complex algorithms like large datasets can be used to predict and create an optimal treatment strategy.
Deep learning and neural networks can be used to process data in healthcare apps and keep a close watch on the patient’s emotional state, food intake, or health monitoring. 
“Omics” refers to the collective technologies that help in exploring the roles, relationships of various branches ending with the suffix “omics” such as genomics, proteomics, etc. Omics-based tests based on machine learning algorithms help find correlations and predict treatment responses, ultimately creating personalized treatments for individual patients. 
How it helps in psychology and neuro patients
For psychologists studying creativity,  AI is promising new classes of experiments that are developing data structures and programs and exploring novel theories on a new horizon. Studies show that  AI can conduct therapy sessions, e-therapy sessions, and assessments autonomously, also assisting human practitioners before, during, or after sessions. The Detection and Computational Analysis of Psychological Signal project uses ML, computer vision, and NLP to analyze language, physical gestures, and social signals to identify cues for human distress. This ground-breaking technology assesses soldiers returning from combat and recognizes those who require further mental health support. In the future, it will combine data captured during face-to-face interviews with information on sleeping, eating, and online behaviours for a complete patient view.
Stroke identification
Stroke is another frequently occurring disease that affects more than 500 million people worldwide. Thrombus,  in the vessel cerebral infarction is the major (about 85%) cause of stroke occurrence. In recent years, AI techniques have been used in numerous stroke-related studies as early detection and timely treatment along with efficient outcome prediction can help solve the problem. With AI at our disposal, large amounts of data with rich information, more complications and real-life clinical questions can be addressed in this arena. Currently, two ML algorithms- genetic fuzzy finite state machine and PCA were implemented to build a model building solution. These include a human activity recognition stage and a stroke onset detection stage. An alert stroke message is activated as soon as a movement significantly different from the normal pattern is recorded. ML methods have been applied to neuroimaging data to assist disease evaluation and predicting stroke treatment for the diagnosis.
Patient Monitoring
Today, the market for AI-based patient monitoring is impressive and monetarily enticing. It is evolving with artificial sensors, smart technologies and explores everything from brain-computer interfaces to nanorobotics. Companies with their smart-watches have engaged people to perform remote monitoring even when they are not “patients”. An obvious place to start is with wearable and embedded sensors, glucose monitors, pulse monitors, oximeters, and ECG monitors. With patient monitoring becoming crucial, AI finds numerous applications in chronic conditions, intensive care units, operating rooms, emergency rooms, and cardiac wards where timeless clinical decision-making can be measured in seconds. More advances have started to gain traction like smart prosthetics and implants. These play an impeccable role in patient management post-surgery or rehabilitation. Demographics, laboratory results and vital signs can also be used to predict cardiac arrest, transfer into the intensive care unit, or even death. In addition, an interpretable machine-learning model can assist anesthesiologists in predicting hypoxaemia events during surgery. This suggests that with deep-learning algorithms, raw patient-monitoring data could be better used to avoid information overload and alert overload while enabling more accurate clinical prediction and timely decision-making.
 Conclusion
Considering the vast range of tasks that an AI can do, it is evident that it holds deep potential in improving patient outcomes to skyrocketing levels. Using sophisticated algorithms AI can bring a revolution in the healthcare sector. Even after facing challenges like whether the technology will be able to deliver the promises, ethical measures, training physicians to use it, standard regulations etc, the role of AI in transforming the clinical practices cannot be ignored. The biggest challenge is the integration of AI in daily practice. All of these can be overcome and within that period the technologies will mature making the system far more enhanced and effective.
Blackcoffer Insights 29: Sanskriti Sunderum and Aayushi Nauhwar, SRCC, Delhi University


What if the Creation is Taking Over the Creator?
Human minds, a fascination in itself carrying the potential of tinkering nature with the pixie dust intelligence, creating and solving the mysteries and wonders with anything but admiration. However, no matter how captivating a human mind can be, it could sometimes be appalled. It could be the hunger or maybe the desire to want more, to go beyond and unravel the limitations, or maybe something like pure greed. Humans have never stopped and always keep evolving when it comes to intelligence and this is what makes them the supreme.
Intelligence calls out for supremacy and so, what if there was to evolve something that opposed a challenge to the very human minds, to their capabilities while making them question their own importance among themselves? Artificial Intelligence came as a revolution, havoc when it first came to the light. The concept of making machines does work on their own, like granting machines –The Intelligence.
The idea of making machines work like humans came back in the 19s. Back then people didn’t believe in such a thing as making a non-living thing work, think, and carry tasks on its own, not to mention, to actually surpass humans themselves in those skills. The facts are it did. By 1997. The greatest chess player, Garry Kasparov was defeated in a chess game by a machine and this is where exactly, a top skilled human lost to a mere machine created by another who by himself could’ve never defeated him. It was a rule of power, of betterment, of skills, and the granted supremacy. Were AI and Machines just tools? Equipment?  Something that helped an unskilled person with his mind and intelligence creates something that could do the skilled work for him with perfection and precision? Well initially it was, however, as time passed as humans got drawn to the puzzle of AI, a lot changed. Human research went deeper and deeper and as a result, the machines evolved with it.
At present, AI & Machines is a growing field. As it develops and improves, it has become a part of the industrial revolution. In industries, most of the laborious work that was once taken care of by humans was now replaced by machines. Naturally, with the evolution in machines, its precision, mass productivity, quality control, time efficiencies, and all the other factors made it a better choice. A choice over humans.
This led to fear, a fear of a not-so-distant future, a future where maybe machines will be so evolved that they’ll take over the need of a human employee leading to unemployment. With the population increase around the world, it became the new tech threat for the labor market. Then again… how true is it? Does AI really oppose a threat? Will adapting to technology make millions of people lose their jobs? Will it lead to mass unemployment? Will the machines really surpass humans? Will, the creation take over the creator?
No matter how fearful the future with AI may seem, in reality, it is not that scary. Truth is AI is the present reality, it is the key that holds the power to unlock a whole next level of human evolution. Technology is growing. There was a time where technology was just an idea, but today that idea has been implemented, it’s working and is carried out. Nobody could stop the advancement and growth of Artificial Intelligence, it’s a wave that is already flowing and we as the present generation and the generations to come to have to learn, to learn to swim in this flow and avoid drowning.
Many jobs will be replaced by machines, as AI evolves it’ll keep challenging human minds and their skills. With the present COVID 19 situation, contactless cashiers to robots delivering packages have already taken over the usual routine tasks. The jobs of Secretaries, Schedulers, and book-keeper are at risk too. Manufacturing units, agriculture, food services, retail, transportation & logistic, and hospitality are all a part of the AI-affected automation. At an estimation, it is said that around 20 million jobs, especially including manufacturing will be lost to robots. As AI, robotics, 3D printing, and genetics make their way in, even the architects, medical docs, and music composers feel threatened by technology. Making us question that will AI even edge us out of our brain jobs too? Now that can be terrifying.
However, as much as machines will be replacing few jobs, they’ll also be creating new jobs.  With the economic growth, innovation, and investment around 133 million jobs are said to be generated. These newly enhanced jobs are to create benefits and amplify one’s creativity, strategy, and entrepreneurial skills. So what is the catch?
Well, it’s the skills. Even though AI is creating 3 times more jobs than it is destroying, it’s the skills that count. AI surged in new job opportunities, opportunities like Senior Data Scientist, Mobile Application Developer, and SEO specialist. These jobs were once never heard of but now with AI it’s born, however, to do these jobs or for its qualification, one needs high-level skills and to acquire those skills can be an expensive and time-consuming task. The future generation might be able to cope up with it but the real struggle is to be faced by the present two generations. It’s the vulnerability between the skill gap and unemployment and the youths are the ones to be crushed the most.
Therefore, as the advancement of AI becomes inevitable there remains no choice but to adapt, learn, equip ourselves and grow with it. The companies have to work together to build an AI-ready workplace. They should collaborate with the government, educators, and non-profit organizations and work together to bring out policies that could help understand the technologies’ impacts faster while also providing the employees some security. The economic and business planning should be made considerable for minimizing the impact on local jobs and properly maximizing the opportunities.
The employees should be provided with proper tools to carry along with the new opportunities while acquiring AI-based skills for their day-to-day work. New skills should be identified and implemented for the upskilling and continual learning initiatives. Employees will have to maximize their Robotic Quotient and learn core skills. They’ll have to adapt to new working models and understand their roles in the coming future. 
Howsoever, it’s not like AI will totally take over control, even though AI proves to be a better choice, it still has its limitations at present. First, it’s expensive, secondly, manufacturing machines in bulk is not good for the environment. Machines are also very high maintenance, therefore human labor will often come cheaper and so will be considered over machines. Underdeveloped countries will find it hard to equip their people with the upskilling and reskilling required for AI workplace and so for AI to play a role in those countries, might take years. AI can also be risky and unethical, as it’s hard to figure out who to be held responsible for in cases where an AI went wrong.
No matter, how advanced AI gets, there are some skills where humans will always have an upper hand i.e., soft skills. Skills like teamwork, communication, creativity, and critical thinking are something that AI hasn’t been able to beat us up to yet and so the value of creativity, leadership, and emotional intelligence has increased. Although, with machines coming in between humans causing the lack of human-to-human interaction, the humans seem to fade away a little.
With this era, comes the need for good leaders. Leaders who are capable of handling both machines and humans together, the ones who are organized enough to manage the skilled and the unskilled employees while providing the unskilled trainees with proper training. Leaders who hold profound soft skills and encourage teamwork while working along with machines. The ones who are patient, calm, and optimized.  
In conclusion, yes AI and machines are going to be very challenging but there’s nothing humans haven’t overcome. Adaptation and up-gradation are going to be the primary factor for survival. As we witness the onset of the 4th industrial revolution, let’s buckle up our seats and race along the highway with the essential fuels (skills) so as to not let ourselves eliminated. After all, this is an unending race with infinity as the end, all we could do is try not to run out of fuel. Try not to be outdated. 
Blackcoffer Insights 29: Glady, Karunya Institute of Technology and Sciences.


What Jobs Will Robots Take From Humans in The Future?
Introduction
AI is rapidly evolving in the employment sector, particularly in matters involving business and finance. Finance, management, economics, and accounting are now among the most popular university courses globally; particularly at the graduate level, due to their high employability. However, the evolution of machinery in industries is changing that. According to research, 230,000 jobs in these sectors may be replaced by AI agents in the next 5 years. This is due to the nature of the work, as employees are responsible for tasks such as data analysis and keeping track of numerical information; which machines excel at. Large, complicated data sets can be analyzed faster and more efficiently by AI-powered computers than by people. Algorithmic trading procedures that produce automated deals are less likely to be produced without minute errors when undertaken by humans. In such matters involving industrial work, a subsection of artificial intelligence is used; namely, machine learning.
Machine learning is a term used for the application of artificial intelligence (AI) in systems, which involves them assimilating and processing information by gaining experience. It is mainly concerned with the development of technology and computer programs.
 Its improvement process is mainly divided into seven steps; which start with the collection of data. This data can be collected from an internal database or perhaps an IoT structure. The second and most time-consuming step is cleaning, preparing, and rearranging the data; which involves the recognition of outliers, trends, and missing information so that the outcome is as accurate as possible. The third step consists of formatting data; which is useful if you source your information from a variety of sources. Step four is where AI comes into place. Self-service data processing tools may be useful if they provide intelligent services for matching data attributes from distinct databases and intelligently integrating them. The data is then arranged to better represent a specific pattern. Lastly, the data is divided into two sets: one for training the algorithm and one for analysis.
There are three types of machine learning; supervised, unsupervised, and reinforcement learning. The most common of the bunch is supervised machine learning; which is based on accurately labeled data. The machine is given a collection of information, including the outcome of the operation. The rest of the information is referred to as ‘input features’, which ‘supervises’ the machine by guiding it to establish the connections between the variants.
In the case of unsupervised learning, the machine isn’t given labeled sets of data to be divided into, allowing it to recognize and create its own patterns within the information provided. Since computers have the capabilities of identifying distinguished similarities, this method helps with the classifying of such data.
Reinforcement learning comprises experience-based learning. Similar to people, they learn due to a reward and punishment system based on their actions.
These variations of system learning have recently been integrated into business and finance, which is elaborated on in the following passages.
Machine learning in finance and banking
There are many ways in which machine learning is used in finance and banking. The most common place it is used is to detect any frauds. Fraud is the most common problem in financial service companies and it accounts for billions of dollars in misfortune each year. The most common frauds are credit or debit card usage by a stranger, document forgery, and mortgage fraud.
Usually, finance companies keep an enormous amount of their data stored online, and it increases the risk of information being accessed without authorization. With expanding innovative headway, misrepresentation in the monetary business is currently viewed as a high danger to significant information. Fraud recognition frameworks in the past were planned dependent on a bunch of rules, which could be effortlessly be bypassed by current fraudsters. Therefore, most companies today leverage machine learning to combat fraudulent financial transactions.
 Machine learning works by looking over enormous informational indexes to distinguish unique or suspicious activities for additional examination by security groups. It works by looking at an exchange against other information focuses – like the client’s record history, IP address, area, and so forth.  Depending on the type of exchange taking place, the program can automatically refuse a withdrawal or purchase until the person makes a decision.
Examples of fraud detection software used by banks include Feedzai, Data visor, and Teradata
Machine learning is also used in algorithmic trading, portfolio management, loan underwriting, chatbots, and improving customer service.
Algorithmic trading is an interaction for executing orders using computerized and pre-modified exchanging guidelines to represent factors like value, timing, and volume. An algorithm is a set of directions for solving a problem. Computer calculations send little parts of the full request to the market over the long run.
In contrast to human dealers, algorithmic exchanging can investigate enormous volumes of information every day and therefore make thousands of trades every day. Machine learning settles on quick exchanging choices, which gives human traders a benefit over the market normal. Likewise, algorithmic exchanging doesn’t settle on exchanging choices dependent on feelings, which is a typical constraint among human dealers whose judgment might be influenced by feelings or individual desires. The exchanging technique is generally utilized by multifaceted investment administrators and monetary foundations to automate trading activities.
In the banking industry, organizations access a large number of shopper information, with which machine learning can be prepared to work in order to simplify the underwriting process. Machine learning calculations can settle on speedy choices on endorsing and credit scoring, and save organizations both time and monetary assets that are utilized by people.
Data scientists can train algorithms on how to analyze millions of consumer data to coordinate with information records, search for interesting special cases, and settle on a choice on whether a shopper meets all requirements for an advance or protection. For instance, the calculation can be prepared on the most proficient method to examine shopper information, like age, pay, occupation, and the buyer’s credit conduct.
The benefits of machine learning
As with any form of revolutionary technology, the usage of machine learning has been debated over as its beneficial properties have been weighed against the possible disadvantages. It’s been observed that upon correct usage, it may be used to solve a wide range of business challenges and anticipate complicated customer behavior. We’ve also seen several of the largest IT conglomerates, such as Google, Amazon, and Microsoft, introduce Cloud Machine Learning platforms.
Technology vs Human intelligence
In terms of business, ML can aid businesses by identifying consumer’s demands and formulate a pattern based on them; allowing companies to reach out to such consumers and maximize their sales. It eliminates the need for expenses on some maintenance by reducing the risks associated with unexpected breakdowns and minimizes wasteful costs to the firms.  Historical data, a process visualization tool, a flexible analytical environment, and a feedback loop may all be used to build an ML architecture. The input of data is another agitating chore for businesses in the present, which is where machine learning can step in for manual labor. This eradicates the possibility of errors caused by manual labor causing disruptions and provides employees with extra time to handle other tasks.
In the financial sector, machine learning is already utilized for portfolio management, algorithmic trading, loan underwriting, and fraud detection. Chatbots and other conversational interfaces for security, customer support, and sentiment analysis will be among the future uses of ML in banking. Another aspect of business involving artificial intelligence includes image recognition, which entails a system or program that recognizes objects, people, places, and movements in photos. It identifies photographs using an imaging system and machine recognition tech with artificial intelligence and programmed algorithms.
The downfalls of machine learning
With all the benefits of its advancement, machine learning isn’t the most perfect thing. There are several disadvantages which are information acquisition, time and resources and high errors, and wrong interpretations. One of the major hurdles is the amount of finance needed to invest in machine learning for it to be a successful project. More issues have to do with the fact that AI requires gigantic informational indexes to train on, and these ought to be unbiased, and of good quality. There can likewise be times where they have to wait for that new information to be produced. Machine learning needs sufficient opportunity to do the calculations to learn and adequately to satisfy their accuracy and relevancy. It also needs huge resources to work. This can mean extra requirements for the computer to work. Another significant problem is the capacity to precisely decipher the results produced by the calculations. You should likewise cautiously pick the algorithms to get the wanted results.
Conclusion
There have been various reports in the past and current years which claim that a significant piece of the human labor force will be replaced via robots and machines in the years to come. With excessive innovative work being led in the field of computerized reasoning, many dread that a significant job crisis will unfurl since numerous positions are all the more precisely and productively performed with the use of machines. In countries like Japan, mainly computer programs and AI is used in the secondary and tertiary sectors. From cleaning the house to depositing money in banks, everything is done by AI. However, AI cannot replace humans in the future. Humans have several capabilities which, even after several technological advancements a machine would not be able to have. These capabilities include creative thinking and creative problem solving, and human connection. For example, when a child goes to a doctor to get an injection, a nurse always relaxes the child to not be afraid of the needle. A machine’s touch would not be able to soothe a child. Another example could be how humans tend to share things with each other and be open about it, a machine will not be able to do so since it is only programmed to things it has been told to do. Like computers, AI will not replace us but would however complement us to make daily work easier and less time-consuming. Without humans themselves, there is no future for AI.
Blackcoffer Insights 29: Amara Arora and Vaanya Kaushal, Scottish High International School


Will Machine Replace The Human in the Future of Work?
“Anything that could give rise to smarter-than-human intelligence – in the form of Artificial Intelligence, brain-computer interfaces, or neuroscience-based human intelligence enhancement – wins hands down beyond contest as doing the most to change the world. Nothing else is even in the same league.” 
–Eliezer Yudkowsky, AI Researcher
There’s no denying robots and automation are increasingly part of our daily lives. Just look around the grocery store, or the highway, they are everywhere. This makes us wonder what if AI can replace human intelligence? What can we do to make ourselves relevant tomorrow? Let us try to find the answers to all these questions and more.
Let’s first understand what is Artificial Intelligence –
Artificial Intelligence or AI basically machines displaying intelligence. This can be seen from a machine playing chess or a robot answering questions on Facebook. Artificial Intelligence can be further broken down into many different types. There are AIs designed to do specific tasks, such as detecting a specific type of cancer. However, there are also AIs that can do multiple tasks, such as driving a car. There are many types of AIs. Among the top, most important fields are Machine Learning or ML, Neural Network, Computer Vision, and Natural Language Processing or NLP.
Machine Learning is the idea of machines being able to prove themselves similar to how a human being learns a new skill. Machine Learning also allows for the optimization of an existing skill. Machine Learning is used in many different fields and one such application is entertainment. Netflix uses Machine Learning to recommend more shows that you can watch based on the shows that you have already seen.
Neural Networks are algorithms that are modeled after the human brain. These algorithms think just like we do which can thereby give similar results to what a human being can give. Artificial Neural Networks are used in medical fields to diagnose cancers like lung cancer and prostate cancer.
Computer Vision is the idea that computers have visions. This allows them to see things the way human beings do or potentially better than human beings do, depending on the programming, camera used, etc. Computer Vision is used in autonomous vehicles for navigation from one place to another.
Natural Language Processing is the idea that computers can listen to what we say. An example of this is Siri. Siri is able to listen to our demands, process what it means, and provide you an answer based on what is researched.
Now that we know what an AI is and what it can do, Let’s talk about the issue.
WILL MACHINE REPLACE THE HUMAN IN THE FUTURE OF WORK?                                                 
AIs allow for the automation of jobs, thereby replacing what humans already do. This means more job loss and the concentration of wealth to the selected few people. This could mean a destabilization of society and social unrest. In addition to social unrest, AI improves over time. This means it becomes smarter, faster, and cheaper to implement and it will be better at doing repetitive things that humans currently do, such as preparing fast food. It is predicted that AI will improve so much over 50 to 100 years that AI will become super intelligent. This means that it will become even smarter than the most intelligent human on earth. According to many experts such as Elon Musk, this could cause the end of human civilization. AI could potentially start a war against humans, burn crops and do all sorts of tragedies once reserved for human functions. At that point, in theory, we can not stop it because AI would have already thought of all the obstacles that will prevent its goal. This means that we cannot unplug the machine, in effect AI will replace human intelligence.
But, will this happen in next 10 to 30 years?
NO! The field of Artificial Intelligence is sophisticated enough to do many human tasks that humans currently do. Currently, AI is not smart enough to be empathetic to humans and cannot think strategically enough to solve complex problems. AI solutions can be expensive and have to go through many different tests and standards to implement. It also takes time for AI to improve. For example, Boston Dynamics, one of the world’s top robotics company had a robot in 2009 that needed assistance to walk. Fast forward to 2019, not only the robot could walk by itself but it could jump over objects, do backflips and so much more. In addition to the timing, it takes time for the price of any new technological solution to drop to a point where it is affordable. For example, a desktop computer costs around $1000 in 1999 but now you can get a significantly more powerful laptop for the exact same price. AI will go through the same curve.
But what happens after those 10 to 30 years? Will AI make human intelligence obsolete? Maybe. As we have proven earlier AI will become faster better and cheaper. As this happens, more and more companies will use AI technology to automate more and more jobs to save money, increase productivity, and most importantly, stay competitive. As we have demonstrated, AI will become better through repetition via the use of machine learning. The only difference is that AI will be able to learn faster as time progresses due to the amount of data that is available today. It will also be able to learn from other machines or similar machines to learn how to optimize its tasks or new important skills. However, AI also just not do repetitive and routine tasks better, it will also be able to understand emotional intelligence, ethics, and creativity. This seen in three distinct example- IBM
IBM uses its IBM Watson to program the AI to create a movie trailer. Fox approached IBM and said they have a movie coming out on AI #Scifi horror. They asked IBM if their platform IBM Watson could a trailer by reviewing and watching the footage and searching for scary,
WILL MACHINE REPLACE THE HUMAN IN THE FUTURE OF WORK?                                             
Sad or happy or other moments in the movie that provoked quality emotions based on how the machine was programmed to identify such emotions in a quantifiable manner. IBM Watson was able to generate a trailer for the movie Morgan. The result, a movie trailer created by machines example – Google
IN 2018 google demonstrated an AI assistance that could take calls and do simple stuff. The AI was able to set up an appointment! What was more fascinating was that it was able to understand the nuances of the conversation. The receptionist thought it was a human being that was calling her. That is a very primitive version of what is possible with this technology. Eventually, it will be able to have conversations just as human beings do, making many sales jobs obsolete. example – AI generated art
In 2018, a Paris art collective consisting of three students used artificial intelligence to generate a portrait. It generated the portrait painting by studying a set of fifteen thousand art images on wiki art. It was estimated to be worth between seven thousand to ten thousand dollars. The painting sold at an auction for four thirty-five thousand US dollars.
However, we cannot for sure say that AI will replace human intelligence. This is because we as a society have started asking hard questions and questioning ethics. Elon Musk founded Open AI, a research lab whose whole purpose is to promote and discover artificial intelligence in a way to benefits humanity. In addition to this, there are many factors that affect the long-term outcome of AI replacing human intelligence. Like, to what degree will other humans allow for AI to take over? Depending on the field, do people even want Artificial Intelligence to help them? Or will they prefer a human counterpart? While we may not be able to control what happens in the long run, we can definitely secure our short-term future.
Here are the top five skills that will not become obsolete in the near future
Strategic and creative thinking
The ability to think outside the box is very human. There are thousands upon thousands of slightly different possible outcomes that may result from every distinguishable action that the human mind with its ability to judge from experience is programmed for these purposes in a far more sophisticated manner than AI can currently achieve. As the billionaire founder of Alibaba, Jack Ma famously said – “AI has logic, human beings have wisdom”.
Conflict resolution and negotiations
With our understanding of the complexities of human-related processes and our ability to improvise and judge, we are far better equipped to deal with conflicts than robots are ever likely to be.
WILL MACHINE REPLACE THE HUMAN IN THE FUTURE OF WORK?                                
Emotional Intelligence and Empathy
AI may be able to recognize faces and images but it can rarely successfully read the feelings of those faces. Humans, to lesser or greater degrees, are capable of an accurate analysis of emotional subtext. With the application of intuition and the use of delicately worded or elusive languages, through these methods, we are able to properly judge how a person feels.
Interpretation of Gray Areas
Robots and computers function well when presented with quantifiable data. However, once the situation enters a gray area, whether this term refers to morals, processes, or definitions robots are more likely to falter.
Critical thinking
Humans are capable of responding to more indicators of quality than computers are. While an AI system may be able to analyze documents according to the true or false statements made within the text, we can judge whether or not it is well written and analyze the implication of the use of certain words and the overall meaning of the content.
Blackcoffer Insights 29: Fiza Parveen, Shri Govindram Institute of Technology and Science, Indore


Will AI Replace Us or Work With Us?
“Machine intelligence is the last invention that humanity will ever need to make”
Nick Bostrom
To put it frankly, Artificial Intelligence will eventually replace jobs. Workers in a variety of industries, from healthcare to agriculture and manufacturing, should expect to witness hiring disruptions as a result of Artificial Intelligence.
If history has taught us anything, it is that disruptive paradigm-shifting business ideas not only make a fortune for the innovators, but they also build the groundwork for new business models, market entrants, and job opportunities which will inevitably follow. It is true that robots today or in future will eventually replace humans for many jobs, but so did innovative farming equipment for humans and horses during the industrial revolution. But that does not mean that our jobs as humans will end here. We, on the other hand, will be required to generate and provide value in whole new ways for entirely new business models as a result of these changes.
According to 71% of the businesses worldwide, Artificial Intelligence can help people overcome critical and challenging problems and live better lives. Artificial Intelligence consultants at work will be more or equally fair, according to a whopping 83% of corporate leaders. These results demonstrate that Artificial Intelligence is steadily extending its measures, yielding societal benefits and allowing citizens to live more fulfilling lives.
Increase in Automation and Jobs where humans can’t compete
Since the advent of Industry 4.0, businesses are moving at a fast pace towards automation, be it any type of industry. In 2013, researchers at oxford university did a study on the future of work. They concluded that almost one in every two jobs have a high risk of being automated by machines. Machine learning is responsible for this disruption. It is the most powerful branch of artificial intelligence. It allows machines to learn from data and mimic some of the things that humans can do.
A research was conducted by the employees of Kaggle wherein an algorithm was to be created to take images of a human eye and diagnose an eye disease known as diabetic retinopathy. Here, the winning algorithm could match the diagnosis given by human ophthalmologists. Another study was conducted wherein an algorithm should be created to grade high school essays. Here too, the winning algorithm could match the grade given by human teachers.
Thus, we can safely conclude that given the right data, machines can easily outperform human beings in tasks like these. A teacher might read 10,000 essays over a 40-year career; an ophthalmologist might see 50,000 eyes but a machine can read a million essays and see a million eyes within minutes.
Thus, it is convenient to conclude that we have no chance of competing with machines on frequent, high volume tasks. 
Tasks where machines don’t work
But there are tasks where human beings have an upper hand, and that is, in novel tasks. Machines can’t handle things they haven’t seen many times before. The fundamental rule of machine learning is that it learns from large volumes of past data. But humans don’t; we have the ability of seemingly connecting disparate threads to solve problems we haven’t seen before. 
Percy Spencer was a physicist working on radar during world war 2 where he noticed that the magnetron was melting his chocolate bar. Here, he was able to connect his understanding of electromagnetic radiation with his knowledge of cooking in order to invent the microwave oven. Now this sort of cross pollination happens to each one of us several times in a day. Thus, machines cannot compete with us when it comes to tackling novel situations. 
Now as we all know that around 92% of talented professionals believe that soft skills such as human interactions and fostering relationships matter much more than hard skills in being successful in managing a workplace. Perhaps, these are the kind of tasks that machines can never compete with humans at. 
Also, creative tasks: the copy behind a marketing campaign needs to grab customers’ attention and will have to stand out of the crowd. Business strategy means finding gaps in the market and accordingly working on them. Since machines cannot outperform humans in novel tasks, it will be humans who would be creating these campaigns and strategies. 
Human contact would be essential in care-giving and educational-related work responsibilities, and technology would take a backseat. Health screenings and customer service face-to-face communication would advocate for human contact, with Artificial Intelligence playing a supporting role. 
So, what does this mean for the future of work? The future state of any single job lies in the answer to one single question: to what extent is the job reducible to tackling frequent high-volume tasks and to what extent does it involve tackling novel situations?
Today machines diagnose diseases and grade exam papers, over the coming years they’re going to conduct audits, they’re going to read boilerplate from legal contracts. But does that mean we’re not going to be needing accountants and lawyers? Wrong. We’re still going to need them for complex tax structuring, for path breaking litigation. It will only get tougher to get these jobs as machine learning will shrink their ranks. 
Amazon has recruited more than 100,000 robots in its warehouses to help move goods and products around more effectively, and its warehouse workforce has expanded by more than 80,000 people. Humans pick and pack goods (Amazon has over 480,000,000 products on its “shelves”), while robots move orders throughout the enormous warehouses, therefore reducing “the amount of walking required of workers, making Amazon pickers more efficient and less exhausted.” Furthermore, because Amazon no longer requires aisle space for humans, the robots enable Amazon to pack shelves together like cars in rush-hour traffic.” More inventory under one roof offers better selection for customers, and a higher density of shelf space equals more inventory under one roof.
Kodak Vs Instagram
Kodak, once an undisputed giant of the photography industry, had a 90% share in the USA market in 1976, and by 1984, they were employing 1,45,000 people. But in the year 2012, they had a net worth of negative $1 billion and they had to declare bankruptcy. Why? Because they failed to predict the importance of exponential trends when it comes to technology. On the other hand, Instagram, a digital photography company started in 2012 with 13 employees and later they were sold to Facebook for $1 billion. This is so ironic because Kodak pioneered digital photography and actually invented the first digital camera but unfortunately thought of it as a mere product and didn’t pay attention towards it and this created the problem.  
We live in an era of artificial intelligence (AI), which has given us tremendous computing power, storage space, and information access. We were given the spinning wheel in the first, electricity in the second, and computers in the third industrial revolution by the exponential growth of technology.
Airbnb and its breakthrough idea!
Airbnb, which is a giant start-up and is known for enabling homeowners to rent out their homes and couches to travellers, for example, “is now creating a new Artificial Intelligence system that will empower its designers and product engineers to literally take ideas from the drawing board and convert them into actual products almost instantly.” This might be a significant breakthrough whether you’re a designer, engineer, or other type of technologist.
Differences that Automation brings onto the table: 
There are three key changes that automation can bring about at the macro level: 
Changes in capability demand
Gender imbalance in workforce redeployment
Firm reorganization. 
Sectors that might be in trouble
Artificial Intelligence isn’t just a fad. Tractica, a market research firm, published a report in 2016 that predicted “annual global revenue for artificial intelligence products and services will expand from 643.7 million in 2016 to $36.8 billion by 2025, a 57-fold increase over that time span.” As a result, it is the IT industry’s fastest-growing segment of any size.”
The reduction in need for people as a result of Artificial Intelligence and related technologies, which resulted in job layoffs, was a cause of fear. In India alone, job losses in the IT sector have reportedly reached 1,000 in the last year, owing to the integration of new and advanced technologies like artificial intelligence and machine learning. 
Most of the IT companies such as Infosys, Wipro, TCS, and Cognizant have reduced their employee base in India and are recruiting less, while engaging more personnel in the United States and investing heavily in “centres of innovation.” Artificial Intelligence and data science, which are currently the trending aspects that require fewer people and are primarily located abroad, aren’t helping the prospects of local employees. Another factor is that the computer industry is continuously growing and would develop to a size of two million workers. Unfortunately, it’s a drop in the bucket compared to what robots are doing to Information Technology’s less-skilled brothers. 
Large e-commerce sites that used to be operated by armies of people are now manned by 200 robots produced by GreyOrange, which is an Indian company based out in Gurgaon. These indefatigable robots lift and stack boxes 24 hours a day, with only a 30-minute break for recharging, and have cut employees by up to 80%. For efficiency, this is a victory but a disaster for job prospects. 
Concluding remarks
Internal re-skilling and redeployment of staff is a critical requirement of the hour. Artificial intelligence has presented Indian policymakers with epistemological, scientific, and ethical issues. This requires us to abandon regular, linear, and non-disruptive mental patterns. The tale of artificial intelligence’s influence on individuals and their occupations will only be told over time. It is up to us to upskill ourselves and look for ways to stay current with the industry’s current trends and demands. 
So, will machines be able to take over many of our jobs? The answer is a resounding yes. However, for every job that is taken over by robots, there will be an equal number of positions available for people to do. Some of these human vocations will be artistic in nature. Others will necessitate humans honing superhuman cognitive abilities. Humans and machines can form symbiotic relationships, assisting each other in doing what they do best. In the future, people and machines may be able to collaborate and work together towards a common goal for any business they work for. 
Blackcoffer Insights 29:  Syed Basir Quadri and Sanchita Khattar, K J Somaiya Institute of Management 


Will machine replace the human in the future of work?
Introduction
Where is this disruptive technology taking us? Take it or leave it, disruptive technology always creates new jobs much more than depleted jobs. You might notice certain jobs disappearing but those jobs are the jobs that transform humans to robots, to machines, and the technology is creating machines to replace them.  Technology creates the data analysis tools to manipulate and create custom scenarios using artificial intelligence (AI), Big Data and Machine Learning (ML) algorithms to predict and drive consumer behavior. Data Analytics tools, such as Google Analytics , and others are available today for free, and, if used correctly, can help organizations save millions, maybe billions of dollars of sales and marketing.
How machine will replace humans?
Before I go on, I think it’s best to level set on what constitutes machines. In the context of this article , machines describe computers and computerized equipment, like robots, that have been programmed to learn, sometimes like humans. Occasionally we call this Artificial Intelligence (AI), other times we call this machine learning, and still other times we call this robotics. And yes, these are technically different things. These bots are more efficient than humans in some specific domains and are growing smarter with each passing day. They can do some really tough tasks which are considered difficult for any human being.  But, within the broad discussion related to the future of work, these are totally interrelated. Factory floors deploy robots that are increasingly driven by machine learning algorithms such that they can adjust to people working alongside them. A machine can work efficiently only it has abundant data and information about the work which is being imparted daily to them. But with every forward step & advancement in technology, a threat is proliferating, a threat of being replaced on our work front. Every passing day is sealing some jobs for humans all over the globe. Similarly, AI is being used to turn hand-drawn sketches (done by humans) into digital source code.
Role of Machines in Companies and its future:
Companies are clearly developing their AI and robotics expertise with the idea that through these technological innovations they’ll be able to
cut costs
increase efficiencies
offer new value propositions
execute new business models or
all of the above.
Of course, it’s not just machines and creatives working together either. In another example, Amazon has employed more than 100,000 robots in its warehouses to efficiently move things around while it has increased its warehouse workforce by more than 80,000. Humans, in Amazon’s case, do the picking and packing of goods while robots move orders around the giant warehouses, essentially cutting “down on the walking required of workers, making Amazon pickers more efficient and less tired.” Plus, the robots “allow Amazon to pack shelves together like cars in rush-hour traffic because they no longer need aisle space for humans. The greater density of shelf space means more inventory under one roof, which means better selection for customers.”
Why Machines can replace humans?
During the next few decades (or maybe sooner), the notion of work and whether it is handled by a human or a virtual being will hinge on predictability. As they are starting to do today, machines will manage the routine while humans take on the unpredictable – tasks that require creativity, problem-solving, and flexibility. In this context, robotics should be seen not only as a means to improve operations efficiency but also to improve the quality of life for workers.
Although it is obvious that human factors involved in a work activity impact job automation, it is also true that highly repetitive tasks—and even mechanical ones—are ideal for robots. Besides greater efficiency and speed, automation leads to a lower risk of accidents, greater control and autonomy, and above all, fewer costs for organizations.
Are Machines more diligent than humans?
Although artificial intelligence and machine learning make us believe that robots are endowed with superior intelligence, in fact they don’t yet have the ability to learn from experience and to respond to unknowns. So as things stand, however much processing speed and automatic learning a robot has, it doesn’t beat factors innate to the human brain. Humans are still a very essential part of the process. Think about delivering services to a client. Most customer challenges are routine, but humans play a very important role in addressing new issues, solving them the first time they appear, and then consolidating the process into the system.
Machines vs. Humans – Which is premium?
While machines and humans are placed in proximity,  robots can be expensive, but this doesn’t apply to all types, especially those based on Robotic Process Automation (RPA), where the development process incorporates algorithms that significantly reduce costs.
Moreover, think of how domestic robots—be it a vacuum cleaner, a lawn mower or a pool cleaner—are increasingly part of our daily lives. This level of consumption that robotics has attained makes it affordable to automate tasks in modern homes to obtain greater control, security and comfort.
Man and Machines together
The division between humans and machines has been clear – I’m here, the machine is there – but that boundary is getting fuzzier. Smart prosthetics fuse seamlessly with our bodies, making up for lost limbs or providing additional strength, stability, or resilience, as seen in exoskeletons donned by assembly line workers.
We use our smartphones symbiotically, but what if they were integrated directly into our bodies? Think a smartphone in the form of a contact lens capable of transparently delivering augmented reality images straight to the brain. Think it sounds like science fiction? Think again. The first prototypes have already been built.
Soon, brain-computer interfaces could become seamless as well, creating a new synergistic relationship between the cloud and us. At that point, the question of who knows what would be moot; you ask me a question and I know the answer. Sometimes that answer will be stored in my own neural circuitry, but most of the time it would come from the connection of my neurons to the web. Our brain’s decision process is influenced by the way it has been “educated” by the cultural context. These external factors are influencing our decision processes to the point that in certain situations, we can legitimately claim that influence has been so strong that our brains can’t be held accountable for the choices made. The point I’m trying to make is that we humans are in symbiosis with our cultural environment and the tools – both physical and conceptual – that we have been taught to use. My guess is that the transformation will be subtle.
Conclusion
Practically speaking, robots growing to the point that they take over the world and then start creating smarter, better robots are impractical and should not even be a concern. None of this is expected in the near future, not by a long shot. If you’ve been to an ATM, waited for a PC to boot up after a catastrophic failure, or had a game crash on your X box just when you were about to reach a checkpoint, you understand that we are not in a world where machines do everything perfectly right. Before they can take over all of our jobs, they need to be able to do theirs’ flawlessly; until then, we can depend on humans to mess up our lives. 
This isn’t a win-or-lose situation. We’re going to wind up as a partner to our smarter machines, and that partnership will be fostered by our augmentation through technology. Machines will play an essential role in this augmentation and, as with any successful technology, they will fall below our level of perception. In the end, the revolution will be silent and invisible.
Blackcoffer Insights 29: Swapna. G, Nivashiniya. R, Sri Manakula Vinayagar Engineering College(SMVEC), Puducherry


How humans and machines are evolving to work together?
In future or in upcoming years humans and machines are going to work together in every field of work. In upcoming days machines will be the need for every human being. Machines [AI technology] will do the work which humans are incapable of doing. Machines will partner and co-operate with humans.
According to the professor at the university of Washington, he explained that, as a result of AI, there will be more demand for existing jobs and new jobs will be created that are unimaginable today. Human workers and machines will work together flawlessly, complementing each other. Machines will learn to carry out easier tasks such as following processes or crunching data. They will also help the humans while difficult. Machines or AI will create a great job opportunities for humans in future. John Kelly ll, executive vice president of IBM once said that “Man and Machines working together always beat or make a better decision than a man or a machine independently.”
 In future, the three sectors of our country like agriculture sector, industrial sector and service sector are going to utilize the machines. So, that their work becomes not difficult. As of now, we can only see that for agriculture purposes various kinds of machines are used which we called as a modern farming method. Some major technologies [machines] that are harvest automation, autonomous tractors, seeding, and weeing and drones. As a result, farms can do agriculture peacefully. In the industrial sector also humans and machines are working together to increase production. Various types of machines are used in industries such as packing machines, loading machine etc. humans provide instructions to the machines and maintain the management in the company. Soon robots [machines] will assist doctors with surgeries. For instance, a doctor at remote location could direct a surgical robot to perform an open heart surgery. But the approaches option and decision will be left to experience and wisdom of the doctor not the robot.
What do you think of machines if they will make humans less or more in the field? Machines will push human professionals up the skillset ladder into uniquely human skills such as creativity, social abilities, empathy, and sense-making, which machines cannot automate. As a result, machines will make the workplace more, not less for humans. However, humans have to learn new skills throughout their lives. It is said that in the future 80% of process-oriented tasks will be done by machines. Quantitative reasoning tasks will be done approximately 50% by humans and 50% by machines, while humans will continue to do more than 80% of cross-functional reasoning tasks. According to Harvard research machines, algorithms can read diagnostic scans with 92% accuracy. Humans can do it with 96% accuracy. Together, it will be 99% accurate.
Human-machine collaboration enables companies to interact with employees and customers in the novel, more effective ways. Smart machines are helping humans to expand their abilities in three ways. They can amplify our cognitive strengths; interact with customers and employees to free us from higher-level tasks, and embody human skills to extend our physical capabilities. In the research, it was found that 1,500 companies achieve the most significant performance improvement when humans and machines work together. New machine systems have beyond-human cognitive abilities, which many of us fear could potentially dehumanize the future of work. Machines will indeed automate most repetitive and physical tasks, and part of quantitative tasks such as programming and even data science. According to D.E Shaw Group and professor at the University of Washington, explained that, as a result of machines, there will be more demand for existing jobs, and new jobs will be created that are unimaginable today. This is similar to how we couldn’t imagine a web app developer decades ago, and now millions make a living doing that today.
Machines are good at doing tasks with speed, precision, and accuracy. But machines are not very good at responding to unknown situations or making judgments. That part will be left to humans. Hence, the need for both humans and machines will be there in the future. Humans and machines have divergent skill sets that, when combined can transform the way we work. Machines have already infiltrated every aspect of our lives, and we must learn to live with them. In the future, human workers will interact more closely with humans.         
Blackcoffer Insights 29: Vachika Sharma, ANAND ENGLISH MEDIUM SCHOOL
                


How neural networks can be applied in various areas in the future?
Everything You Need to Know About the Future of Neural Networks. Neural networks are arguably the technological development with the most potential currently on the horizon. Through neural networks, we could feasibly handle almost any computational or contemplative task automatically, and someday, with greater processing power than the human brain. For now, neural networks are still in their infancy, but already, they’re an impressive technology responsible for tremendous breakthroughs in everything from speech recognition to medical diagnoses. The question is, where does it go from here?
How Neural Networks Work Today
Let’s start by talking about how neural networks, or neural nets, work today in their current form. Neural nets are computer programs that are assembled from thousands to millions of units, each of which is designed to function as an artificial neuron. When being “trained,” a neural network is typically fed information, allowing it to recognize patterns like spotting familiar faces in photos or identifying the correct way to hit a tennis ball. With feedback, neural networks then work to modify the way they processed the problem, “learning” how to do better over long periods of time. When done training, neural nets can solve a wide variety of different problems. They can notice deviations in historical patterns proactively, so you can receive alerts on new events relevant to your business, they can automatically recognize trigger points in a pattern (like picking a face out of a photo or diagnosing a medical condition), and they can perform complex operations without supervision (like when playing a game).
Key Strengths of Neural Nets
There are several key strengths of neural nets that make them a favorite choice of AI developers,
Performance on problems with many variables. For a problem with a strict set of rules and requirements, and with constrained inputs, it’s easy for a machine to work out the answer. The go-to example here is a calculator; the rules of mathematics are never broken, and are relatively simple to follow. Input two variables (and two real numbers), and you can get their sum easily. But identifying speech patterns or diagnosing illnesses require far more variables; machines need to “understand” not only what they’re looking for, but how it’s differentiated from the noise, and how it might be influenced in different ways. Neural nets are ridiculously good at solving these big problems—sometimes even better than humans.
Feature engineering. Neural nets are also incredibly good at figuring out the correct features to ascribe to a problem, known as feature engineering. Let’s say you’re trying to teach an algorithm how to play (and win) a game of Go, the way Google did. Go is a game with practically limitless move possibilities and no clear way of determining whether a move is “good” or “bad” (especially in the early game). For the machine to learn effectively, it must be able to learn how to identify what makes a move more or less likely to get the machine closer to victory. Neural nets can do this; they can create new categories for consideration, and apply them to their work. Applicability. Neural nets also have the power of flexibility. Once established, they can be applied to almost anything, whether it’s helping people spot the issues interfering with their productivity or improving air traffic patterns for smoother flights. The core functionality of a neural net is to learn something efficiently, so if you have a system that can learn to recognize patterns, it could feasibly recognize patterns in almost any domain.
Key Weaknesses of Neural Nets
They said that some key weaknesses prevent neural nets from seeing full-range applications: Data requirements. For starters, all neural nets must go through a “learning” period where they start to recognize patterns and refine themselves. While we’re able to “teach” machines more efficiently than ever before, there’s still a massive data requirement before those algorithms can start to be effective. Depending on the application, this could take 10,000 discrete sets of data or more. This could substantially increase the time it takes to make a neural net effective or limit possible applications.
Expensiveness. Neural nets are also expensive and time-consuming to develop. The computational processes needed to handle all those variables and all those incoming sets of data demand CPU and GPU power beyond the scope of a normal system. This makes it a discouraging endeavor for some engineers, and drives up the price of a functional system, making it harder to use for your intended purposes. Difficulty and blindness. As you might imagine, the realities of developing a neural net are much more in-depth and complicated than can be implied with a simple, overarching definition. It’s incredibly hard to learn how to develop a neural net, and many engineers who begin the journey eventually drop out of the running. On top of that, because of the intricacies of neural nets, we often don’t have transparency to see how our algorithms are coming to their conclusions; we can determine whether their findings are accurate, but we can’t see exactly how they came to those answers, which makes it even more mystifying—even to professionals.
Long-term potential. Neural nets have already been responsible for significant advancements in the realm of AI, but in terms of long-term potential, they may not have as much power as other possibilities, like kernel methods, or even classical AI. There’s a hard limit to how efficient or complicated neural nets can get, and that upper limit is discouraging to many researchers.
What’s in Store for the Future?
With all those strengths fueling the future of neural nets and all those weaknesses complicating things, what could the future hold for this incredible technology? Integration. The weaknesses of neural nets could easily be compensated if we could integrate them with complementary technology, like symbolic functions. The hard part would be finding a way to have these systems work together to produce a common result—and engineers are already working on it.
Sheer complexity. Everything has the potential to be scaled up in terms of power and complexity. With technological advancements, we can make CPUs and GPUs cheaper and/or faster, enabling the production of bigger, more efficient algorithms. We can also design neural nets capable of processing more data, or processing data faster, so it may learn to recognize patterns with just 1,000 examples, instead of 10,000. Unfortunately, there may be an upper limit to how advanced we can get in these areas—but we haven’t reached that limit yet, so we’ll likely strive for it in the near future.
New applications. Rather than advancing vertically, in terms of faster processing power and more sheer complexity, neural nets could (and likely will) also expand horizontally, being applied to more diverse applications. Hundreds of industries could feasibly use neural nets to operate more efficiently, target new audiences, develop new products, or improve consumer safety—yet it’s criminally underutilized. Wider acceptance, wider availability, and more creativity from engineers and marketers have the potential to apply neural nets to more applications.
Obsolescence. Technological optimists have enjoyed professing the glorious future of neural nets, but they may not be the dominant form of AI or complex problem solving for much longer. Several years from now, the hard limits and key weaknesses of neural nets may stop them from being pursued. Instead, developers and consumers may gravitate toward some new approach—provided one becomes accessible enough, with enough potential to make it a worthy successor.
Regardless of what your business goals are, there’s a good chance neural nets will be able to help you achieve them—if not now, then in the very near future. Despite the shortage of developers, companies and engineers are working constantly to refine their neural net efforts, which means we’re in store for a “golden age” of neural networks (at least temporarily). It’s hard to say whether neural net development will continue indefinitely or whether some new, more efficient technology will take its place, but either way, this breakthrough in the field of AI deserves your attention
CONCLUSION
Many big tech companies today are receiving tons of data from their users, and when it comes down to profit and power or the greater good of society, it’s human nature to go for the former instead, especially if you’re in a position to choose. We live in times where our attention is being capitalized constantly. We must live smarter and act rationally to prevent surrendering our lives over to these short bursts of dopamine and expedient and trivial acts.
We can only hope that as we progress into the upcoming decades, the people who are in control of the decisions that these companies make will be for the betterment of society and civilization as a whole. And that our data will be for building systems that serve us, make us more productive, and instead of looking for ways to grab our attention, build products that can provide value and meaning to our lives.
Blackcoffer Insights 28: Monica V, SNS COLLEGE OF TECHNOLOGY


How machine learning will affect your business?
Machine learning techniques may have been used for years, but recently there has been an explosion in their applications. In fact, in a recent Q3 earnings call, Google CEO Sundar Pichai said “Machine learning is a core, transformative way by which we’re re-thinking how we’re doing everything.” And they’re far from the only business making that claim.
In the past, successful use of machine learning algorithms required bespoke algorithms and huge R&D budgets, but all that is changing. IBM Watson, Microsoft Azure, Amazon, and Alibaba all launched turnkey cloud-based machine-learning SaaS solutions in 2015. At the same time startups like Idibon, MetaMind, Dato, and MonkeyLearn have built machine learning products that companies can take advantage of.
Gartner already puts machine learning at the top of its hype curve, and no: machine learning won’t replace all of your employees with computers or suddenly double your revenue. But that doesn’t mean that it can’t give every business a competitive advantage. There are plenty of business processes that can significantly benefit from machine learning. So how does machine learning change the way businesses operate?
Fig: Machine learning techniques For Business
Bigger upfront costs
First thing’s first: Machine learning needs training data and training data costs money. Especially training data labeled by humans. Let me explain. To make machine learning work for business, the algorithm needs to see lots and lots of examples of what it’s supposed to be doing. If you want an algorithm to tell you if a sales lead is good, you need to show it lots and lots of examples of good sales leads and bad sales leads. If you want an algorithm to tag the support tickets you need to show it many examples of support tickets. If you localize your algorithm to a new language you probably need to collect lots of examples in that language. In some instances, a company may have those training sets in-house. For example, a bunch of disqualified or qualified leads. But say you haven’t labeled each of your support tickets as they’ve come in over the year. You’d need to have people either in-house or en masse via a data enrichment platform -label those tickets. The machine will then look at those judgments and start finding connections and patterns it can learn from.
Much lower ongoing costs
Machine learning is much cheaper and more efficient than people when it works well. The downside is that it often works well in 80 percent of the cases and badly in 20 percent of the cases, and lowering the 20 percent error rate is hard, if not impossible. But even an 80 percent accurate algorithm can save you a lot of money because good machine learning algorithms know where they are accurate and where they are more likely to have errors. Smart companies take the cases where the algorithm has high confidence and uses those directly while sending low confidence cases to humans. Banks have been doing this for years. When you put a check in an ATM, an algorithm tries to decipher the numbers on the check. If you have really sloppy handwriting or the ink is smudged the algorithm passes the task to a human. This design pattern saves banks lots of money while preserving a very high level of accuracy.
Your costs will drop over time
A huge benefit of machine learning is that it can turn part of your variable cost into more of a fixed cost. If you use humans to handle cases where that algorithm is struggling, you are creating the perfect training data to feed into your algorithm. This is a well-studied technique called active learning it turns out that training data labels collected on cases where the algorithm has low confidence help the algorithm learn much, much more efficiently.
As the algorithm becomes increasingly more accurate, the unit economics of your business process become better and as machine learning becomes able to handle more cases, the expensive humans are only called in on the toughest, rarest situations. That means you use the best of both human and machine intelligence in tandem: leveraging the speed and reliability of computers for the easy judgments and the fluency and expertise of humans for the difficult ones. And if that sounds like smart business, it’s because it is.
Blackcoffer Insights 28: Monica V, SNS COLLEGE OF TECHNOLOGY


Deep learning impact on areas of e-learning?
eLearning as technology becomes more affordable in higher education but having a big barrier in the cost of developing its resources. Deep learning using artificial intelligence continues to become more and more popular and having impacts on many areas of eLearning. It offers online learners of the future intuitive algorithms and automated delivery of eLearning content through modern LMS platforms. This paper aims to survey various applications of deep learning approaches for developing the resources of the eLearning platform, in which predictions, algorithms, and analytics come together to create more personalized future eLearning experiences. In addition, deep learning models for developing the contents of the eLearning platform, deep learning framework that enable deep learning systems into eLearning and its development, benefits & future trends of deep learning in eLearning, the relevant deep learning-based artificial intelligence tools, and a platform enabling the developer and learners to quickly reuse resources are clearly summarized. Thus, deep learning has evolved into developing ways to re-purpose existing resources that can mitigate the expense of content development of future eLearning.
It is natural to wonder where you might get AI tools to avoid the time and expense of developing your own. Don’t worry about the advert of AIaaS or “AI as a Service” even small education or learning & development professionals can purchase the license of AI tools and components. However, such types of tools cannot be useful for every e-learning ecosystem but may offer some enticing benefits such as adding standard AI tasks (logic, decision making) to your toolbox. Here are some of the AIaaS tools and platforms offered by famous tech giants most of which are cloud-based.
Microsoft Azure
Cloud-based AI services that can be used to build and manage AI applications like image recognition or bot-based apps
IBM’s Watson
Cloud-based AI services that can be integrated into your applications; to store and manage your own data
Google’s Tensor Flow
 An end-to-end open-source machine learning platform
Amazon Web Services
Offers a wide range of products and services on Amazon’s cloud
There are other AIaaS platforms such as DataRobot, Petuum, and H2O which shows that the field is expanding.AI will probably not make human workers obsolete, at least not for a long time To put some of your fears to bed: the robots are probably not coming for your jobs, at least not yet. Given how artificial intelligence has been portrayed in the media, in particular in some of our favorite sci-fi movies, it’s clear that the advent of this technology has created fear that AI will one day make human beings obsolete in the workforce. After all, as technology has advanced, many tasks that were once executed by human hands have become automated. It’s only natural to fear that the leap toward creating intelligent computers could herald the beginning of the end of work as we know it. But, I don’t think there is any reason to be so fatalistic. A recent paper published by the MIT Task Force on the Work of the Future entitled “Artificial Intelligence And The Future of Work,” looked closely at developments in AI and their relation to the world of work. The paper paints a more optimistic picture.
Rather than promoting the obsolescence of human labor, the paper predicts that AI will continue to drive massive innovation that will fuel many existing industries and could have the potential to create many new sectors for growth, ultimately leading to the creation of more jobs. While AI has made major strides toward replicating the efficacy of human intelligence in executing certain tasks, there are still major limitations. In particular, AI programs are typically only capable of “specialized” intelligence, meaning they can solve only one problem, and execute only one task at a time. Often, they can be rigid, and unable to respond to any changes in input, or perform any “thinking” outside of their prescribed programming. Humans, however, possess “generalized intelligence,” with the kind of problem-solving, abstract thinking, and critical judgment that will continue to be important in business. Human judgment will be relevant, if not in every task, then certainly throughout every level across all sectors. There are many other factors that could limit runaway advancement in AI. AI often requires “learning” which can involve massive amounts of data, calling into question the availability of the right kind of data, and highlighting the need for categorization and issues of privacy and security around such data. There is also the limitation of computation and processing power. The cost of electricity alone to power one supercharged language model AI was estimated at $4.6 million. Another important limitation of note is that data can itself carry bias, and be reflective of societal inequities or the implicit biases of the designers who create and input the data. If there is bias in the data that is inputted into an AI, this bias is likely to carry over to the results generated by the AI.
There has even been a bill introduced into Congress entitled the Algorithmic Accountability Act with the goal of forcing the Federal Trade Commission to investigate the use of any new AI technology for the potential to perpetuate bias. Based on these factors and many others, the MIT CCI paper argues that we are a long way from reaching a point in which AI is comparable to human intelligence, and could theoretically replace human workers entirely.  Provided there is an investment at all levels, from education to the private sector and governmental organizations—anywhere that focuses on training and upskilling workers—AI has the potential to ultimately create more jobs, not less. The question should then become not “humans or computers” but “humans and computers” involved in complex systems that advance industry and prosperity. This paper is a fascinating read for anyone hoping to dive deeper into AI and the many potential directions in which it may lead.AI Is becoming standard in all businesses, not just in the world of tech A couple of times recently, AI has come up in conversation with a client or an associate, and I’m noticing a fallacy in how people are thinking about it. There seems to be a sense for many that it is a phenomenon that is only likely to have big impacts in the tech world. In case you hadn’t noticed, the tech world is the world these days. Don’t ever forget when economist Paul Krugman said in 1998 that “By 2005 or so, it will become clear that the Internet’s impact on the economy has been no greater than the fax machine’s.” You definitely don’t want to be behind the curve when it comes to AI.  In fact, 90% of leading businesses already have ongoing investments in AI technologies. More than half of businesses that have implemented some manner of AI-driven technology report experiencing greater productivity. AI is likely to have a strong impact on certain sectors in particular:
Medical:
The potential benefits of utilizing AI in the field of medicine are already being explored. The medical industry has a robust amount of data, which can be utilized to create predictive models related to healthcare. Additionally, AI has shown to be more effective than physicians in certain diagnostic contexts.
Automotive:
We’re already seeing how AI is impacting the world of transportation and automobiles with the advent of autonomous vehicles and autonomous navigation. AI will also have a major impact on manufacturing, including within the automotive sector.
Cybersecurity:
Cybersecurity is front of mind for many business leaders, especially considering the spike in cybersecurity breaches throughout 2020. Attacks rose 600% during the pandemic as hackers capitalized on people working from home, on less secure technological systems, and Wi-Fi networks. AI and machine learning will be critical tools in identifying and predicting threats in cybersecurity. AI will also be a crucial asset for security in the world of finance, given that it can process large amounts of data to predict and catch instances of fraud.
E-Commerce:
AI will play a pivotal role in e-commerce in the future, in every sector of the industry from user experience to marketing to fulfillment and distribution. We can expect that moving forward, AI will continue to drive e-commerce, including through the use of chat-bots, shopper personalization, image-based targeting advertising, and warehouse and inventory automation.
AI can have a big impact on the job search
If you are moving forward with the hope that a hiring manager may give you the benefit of the doubt on a small misstep within the application, you might be in for a rude awakening. AI already plays a major role in the hiring process, so much so that up to 75% of resumes are rejected by an automated applicant tracking system, or ATS before they even reach a human being.  In the past, recruiters have had to devote considerable time to poring over resumes to look for relevant candidates. Data from LinkedIn shows that recruiters can spend up to 23 hours looking over resumes for one successful hire.
Increasingly, however, resume scanning is being done by AI-powered programs. In 2018, 67% of hiring managers stated that AI was making their jobs easier. Despite the increasing prevalence of automation and algorithms in the hiring process, many have been critical of the use of certain types of AI by hiring managers, based on the charge that it can perpetuate and ever create more bias in hiring. One particular example is illustrated by HireVue, a startup whose initial services included technology that aimed to use facial recognition software and psychology to determine the potential effectiveness of a candidate in a certain role. The Electronic Privacy Information Center filed a lawsuit with the Federal Trade Commission alleging that this software had the potential to perpetuate bias and prejudice. HireVue discontinued the use of facial recognition software in early 2021, and now uses audio analysis and natural language processing. It’s clear that the use of certain types of AI in the hiring process will likely be controversial as new technology develops. However, if potential employers are using AI to process your application, there is no reason that you cannot be utilizing similar technology to your advantage.
Jobscan is an excellent resource that provides similar resume scanning to what would be used by a hiring manager. By comparing your resume to a job description, Jobscan will give you information on how to tweak your resume so that it is a good match for a certain position, with the goal of “beating” an applicant tracking system (ATS).
Jobseer is a browser add-on, and another great AI-based tool for those on the job market. Based on a scan of your resume, as well as keywords and skills related to your desired jobs, Jobseer will help match you with the job listings that best fit your experience. For each listing, you get a rating based on how well you are aligned with the particular posting, as well as recommendations of skills to add to better position your resume and experience.
Rezi: Now, as a disclaimer, I would never encourage you to turn your resume writing over to a bot. But Rezi is an awesome AI-based resume builder that includes templates to help you design a resume that is sure to check the boxes when it comes to applicant tracking systems. This is a great jumping-off point to kickstart a new resume.  Another great way to use this type of tool is to generate a new resume and compare it to your current resume to see how it stacks up, and identify some areas for improvement. AI is also a great place to focus your energy if you are looking to upskill in your career, or make your professional profile more competitive in the job market, especially when you consider that AI will have such far-reaching impacts across many industries.AI and machine learning are at the top of many lists of the most important skills in today’s job market. Jobs requesting AI or machine-learning skills are expected to increase by 71% in the next five years. If you’d like to expand your knowledge base in this arena, consider some of the great free online course offerings that focus on AI skills. If you are tech-savvy, it would be wise to dive deep and learn as much as you can about interacting in the AI space. If your skills lie elsewhere, it is important to recognize that AI will have a big impact, and to the extent of your abilities, you should try to understand the fundamentals of how it functions in different sectors. AI is definitely here to stay, whether we like it or not. Personally, I don’t think we have anything to be afraid of. The best way to move forward is to be aware of and adapt to the new technology around us, AI included. This article was updated on April 16, 2021, to reflect changes in HireVue’s assessment tools.
Blackcoffer Insights 28: Monica V, SNS COLLEGE OF TECHNOLOGY


How to protect future data and its privacy?
Before the internet, information was in some ways restricted and more centralized. The only mediums of information were books, newspapers, and word of mouth, etc. But now with the advent of the internet and improvements to computer technology (Moore’s Law), information and data skyrocketed, and it has become this open-system, where information can be distributed to people without any kind of limits.
SECURING YOUR DEVICES AND NETWORKS
Encrypt your data
Various publicly available tools have taken the rocket science out of encrypting (and decrypting) email and files. Data encryption isn’t just for technology geeks; modern tools make it possible for anyone to encrypt emails and other information. “Encryption used to be the sole province of geeks and mathematicians, but a lot has changed in recent years. In particular, various publicly available tools have taken the rocket science out of encrypting (and decrypting) email and files. GPG for Mail, for example, is an open-source plug-in for the Apple Mail program that makes it easy to encrypt, decrypt, sign and verify emails using the OpenPGP standard. And for protecting files, newer versions of Apple’s OS X operating system come with FileVault, a program that encrypts the hard drive of a computer. Those running Microsoft Windows have a similar program. This software will scramble your data, but won’t protect you from government authorities demanding your encryption key under the Regulation of Investigatory Powers Act (2000), which is why some aficionados recommend TrueCrypt, a program with some very interesting facilities, which might have been useful to David Miranda,” explains John Naughton in an article for The Guardian.
Backup your data
One of the most basic, yet often overlooked, data protection tips is backing up your data. Basically, this creates a duplicate copy of your data so that if a device is lost, stolen, or compromised, you don’t also lose your important information. As the U.S. Chamber of Commerce and insurance company Nationwide points out, “According to Nationwide, 68% of small businesses don’t have a disaster recovery plan. The problem with this is the longer it takes you to restore your data, the more money you’ll lose. Gartner found that this downtime can cost companies as much as $300,000 an hour.”
The cloud provides a viable backup option
While you should use sound security practices when you’re making use of the cloud, it can provide an ideal solution for backing up your data. Since data is not stored on a local device, it’s easily accessible even when your hardware becomes compromised. “Cloud storage, where data is kept offsite by a provider, is a guarantee of adequate disaster recovery,” according to this post on TechRadar. Twitter: @techradar
Anti-malware protection is a must.
Scammers are sneaky: sometimes malware is cleverly disguised as an email from a friend or a useful website. Malware is a serious issue plaguing many computer users, and it’s known for cropping up in inconspicuous places, unbeknownst to users. Anti-malware protection is essential for laying a foundation of security for your devices. “Malware (short for malicious software) is software designed to infiltrate or damage a computer without your consent. Malware includes computer viruses, worms, trojan horses, spyware, scareware, and more. It can be present on websites and emails or hidden in downloadable files, photos, videos, freeware, or shareware. (However, it should be noted that most websites, shareware, or freeware applications do not come with malware.) The best way to avoid getting infected is to run a good anti-virus protection program, do periodic scans for spyware, avoid clicking on suspicious email links or websites. But scammers are sneaky: sometimes malware is cleverly disguised as an email from a friend or a useful website. Even the most cautious of web-surfers will likely pick up an infection at some point.,” explains Clark Howard. Twitter: @ClarkHoward
Fig: Protect your Computer from Viruses
Make your old computers’ hard drives unreadable.
Much information can be gleaned through old computing devices, but you can protect your personal data by making hard drives unreadable before disposing of them. “Make old computers’ hard drives unreadable. After you back up your data and transfer the files elsewhere, you should sanitize by disk shredding, magnetically cleaning the disk, or using software to wipe the disk clean. Destroy old computer disks and backup tapes,” according to the Florida Office of the Attorney General. Twitter: @AGPamBondi
Install operating system updates.
Operating system updates are a gigantic pain for users; it’s the honest truth. But they’re a necessary evil, as these updates contain critical security patches that will protect your computer from recently discovered threats. Failing to install these updates means your computer is at risk. “No matter which operating system you use, it’s important that you update it regularly. Windows operating systems are typically updated at least monthly, typically on so-called ‘Patch Tuesday.’ Other operating systems may not be updated quite as frequently or on a regular schedule. It’s best to set your operating system to update automatically. The method for doing so will vary depending upon your particular operating system,” says PrivacyRights.org. Twitter: @PrivacyToday
Automate your software updates.
Many software programs will automatically connect and update to defend against known risks.
In order to ensure that you’re downloading the latest security updates from operating systems and other software, enable automatic updates. “Many software programs will automatically connect and update to defend against known risks. Turn on automatic updates if that’s an available option,” suggests StaySafeOnline.org. Twitter: @StaySafeOnline
Secure your wireless network at your home or business.
A valuable tip for both small business owners and individuals or families, it’s always recommended to secure your wireless network with a password. This prevents unauthorized individuals within proximity to hijack your wireless network. Even if they’re merely attempting to get free Wi-Fi access, you don’t want to inadvertently share private information with other people who are using your network without permission. “If you have a Wi-Fi network for your workplace, make sure it is secure, encrypted, and hidden. To hide your Wi-Fi network, set up your wireless access point or router so it does not broadcast the network name, known as the Service Set Identifier (SSID). Password protect access to the router,” says FCC.gov in an article offering data protection tips for small businesses. Twitter: @FCC
Turn off your computer.
When you’re finished using your computer or laptop, power it off. Leaving computing devices on, and most often, connected to the Internet, opens the door for rogue attacks. “Leaving your computer connected to the Internet when it’s not in use gives scammers 24/7 access to install malware and commit cybercrimes. To be safe, turn off your computer when it’s not in use,” suggests CSID, a division of Experian. Twitter: @ExperianPS_NA
Fig: To Avoid from Hacking turn off your Computer
Use a firewall.
Firewalls assist in blocking dangerous programs, viruses, or spyware before they infiltrate your system.”Firewalls assist in blocking dangerous programs, viruses, or spyware before they infiltrate your system. Various software companies offer firewall protection, but hardware-based firewalls, like those frequently built into network routers, provide a better level of security,” says Geek Squad. Twitter: @GeekSquad
Practice the Principle of Least Privilege (PoLP).
Indiana University Information Technology recommends following the Principle of Least Privilege (PoLP): “Do not log into a computer with administrator rights unless you must do so to perform specific tasks. Running your computer as an administrator (or as a Power User in Windows) leaves your computer vulnerable to security risks and exploits. Simply visiting an unfamiliar Internet site with these high-privilege accounts can cause extreme damage to your computer, such as reformatting your hard drive, deleting all your files, and creating a new user account with administrative access. When you do need to perform tasks as an administrator, always follow security procedures.” Twitter: @IndianaUniv
Use “passphrases” rather than “passwords.”
What’s the difference? “…we recommend you use passphrases–a series of random words or a sentence. The more characters your passphrase has, the stronger it is.  The advantage is these are much easier to remember and type, but still hard for cyber attackers to hack.” explains SANS. Twitter: @SANSAwareness
Encrypt data on your USB drives and SIM cards.
Encrypt your SIM card in case your phone is ever stolen, or take it out if you are selling your old cell phone. Encrypting your data on your removable storage devices can make it more difficult (albeit not impossible) for criminals to interpret your personal data should your device become lost or stolen. USB drives and SIM cards are excellent examples of removable storage devices that can simply be plugged into another device, enabling the user to access all the data stored on it. Unless, of course, it’s encrypted. “Your USB drive could easily be stolen and put into another computer, where they can steal all of your files and even install malware or viruses onto your flash drive that will infect any computer it is plugged in to. Encrypt your SIM card in case your phone is ever stolen, or take it out if you are selling your old cell phone,” according to Mike Juba in an article on Business2Community. Twitter: @EZSolutionCorp
Don’t store passwords with your laptop or mobile device.
A Post-It note stuck to the outside of your laptop or tablet is “akin to leaving your keys in your car,” says The Ohio State University’s Office of the Chief Information Officer. Likewise, you shouldn’t leave your laptop in your car. It’s a magnet for identity thieves. Twitter: @OhioState
Fig: Media Sharing
Disable file and media sharing if you don’t need it.
If you don’t really need your files to be visible to other machines, disable file and media sharing completely. If you have a home wireless network with multiple devices connected, you might find it convenient to share files between machines. However, there’s no reason to make files publicly available if it’s not necessary. “Make sure that you share some of your folders only on the home network. If you don’t really need your files to be visible to other machines, disable file and media sharing completely,” says Kaspersky. Twitter: @kaspersky
Create encrypted volumes for portable, private data files.
HowToGeek offers a series of articles with tips, tricks, and tools for encrypting files or sets of files using various programs and tools. This article covers a method for creating an encrypted volume to easily transport private, sensitive data for access on multiple computers. Twitter: @howtogeeksite
Overwrite deleted files.
Deleting your information on a computing device rarely means it’s truly deleted permanently. Often, this data still exists on disk and can be recovered by someone who knows what they’re doing (such as, say, a savvy criminal determined to find your personal information). The only way to really ensure that your old data is gone forever is to overwrite it. Luckily, there are tools to streamline this process. PCWorld covers a tool and process for overwriting old data on Windows operating systems. Twitter: @pcworld
Don’t forget to delete old files from cloud backups.
If you back up your files to the cloud, remember that even though you delete them on your computer or mobile device, they’re still stored in your cloud account. If you’re diligent about backing up your data and use a secure cloud storage service to do so, you’re headed in the right direction. That said, cloud backups, and any data backups really, create an added step when it comes to deleting old information. Don’t forget to delete files from your backup services in addition to those you remove (or overwrite) on your local devices. “If you back up your files to the cloud, remember that even though you delete them on your computer or mobile device, they’re still stored in your cloud account. To completely delete the file, you’ll also need to remove it from your backup cloud account,” says re/code. Twitter: @Recode
Blackcoffer Insights 28: Monica V, SNS COLLEGE OF TECHNOLOGY


How Machines, AI, Automations, and Robo-human are Effective in Finance and Banking?
We all hear day in and day out that we amidst a technological revolution. But do we know what this really means?
Before we understand how its going to impact us, let us first discuss what these terms really mean.
A technological revolution simply means that we are in a period where better and newer technologies replace the others to get the job done faster and better. We are in an era with rapid innovations where machines are being compared to humans.
Fig: Technological Revolution
So, then what is Machine Learning?
Machine Learning is basically the application of artificial intelligence into electronic systems to enable them to learn and enhance themselves without being programmed by humans. It is the evolution and development of computer programs that can access data and then use it to advance themselves. Whether you know it or not, you use machine learning-powered applications daily.
Now, what is Artificial intelligence?
At its simplest form, artificial intelligence is a field, which combines computer science and robust datasets, to enable problem-solving. 
In simple words, Artificial Intelligence is the technology that facilitates these machines to perform human like behaviour.
Just like every other industry, machine learning is playing its role in the finance and banking industry too. In most cases where a human would perform the same task by performing the same calculations or following the same process can be taught to the machine which can now perform it by itself.
Let us discuss a few examples of the applications that we might have come in our day to day running which are a result of machine learning in this industry:
Portfolio Management
Risk Underwriting
Algorithmic Trading
Fraud Detection
Process Automation
Customer onboarding
Customer churn
Decision Making
Process Automation
Portfolio Management
In earlier days, an investor would need to consult a financial advisor to understand his/her risk appetite and advise accordingly. Today, using machine learning algorithms there exists the concept of a “Robo-Advisor” that requires any user to give certain inputs about their financial status and goals and calculates their risk tolerance and constructs and idle portfolio allocation for them. Young users today find this extremely useful rather than physically visiting an advisor and paying a fee for doing so.
Risk Underwriting
Underwriting is one of the core functions for most financial institutions especially banks and insurance companies where they are required to underwrite the risk of the customers before loaning out money or insurance policies. These underwriting activities are based on trends and thumb rules industrywide. The same has been introduced through machine learning which is able to underwrite risks today on a larger and more accurate scale.
Algorithmic Trading
Machine learning is a mathematical model that tracks market information, analyses massive data sources and study market conditions simultaneously to detect patterns which can be used for trading. This is humanly impossible to do in a fraction of time. Algorithmic systems can make millions of trades daily, often known as “high-frequency trading”. It is highly believed that deep learning is playing its role in calibrating real-time trading decisions.
Fraud Detection
With the increase in use and dependency on computers for financial transactions came the data security risk. There is an ample amount of valuable data stored online available to create potential risk. Machine learning thus helped in fraud detection by detecting anomalies in transactions and flagging them for scrutiny based on the risk factors defined by the institutions. Fraud identification in insurance claims, credit card payments, identity theft, account theft, are all areas in fraud detection that machine learning can help in.
Process Automation
Process automation is one of the most common applications of machine learning in finance. The technology has helped in replacing manual work, automate repetitive tasks to avoid redundancy, and as a result, increase productivity. Machine learning has benefitted these organizations to optimize costs, improve customer experiences, and scale up their services. Some examples of financial and banking firms using process automation are the use of chatbots, automated calls, paperwork automation, and gamified employee training.
Customer onboarding
In this highly competitive industry, customer acquisition and the customer onboarding process is highly relevant in building a good customer relationship. At any stage, during the onboarding, a slight inconvenience or delay can act as a barrier. Machine learning-enabled complete automation in this process for these financial and banking institutions. Today, from opening an account, filing for any application can be completed within a few minutes with utmost ease. With AI, customers’ behavioral patterns have been studied to improvise and make the whole process efficient and user-friendly.
Customer churn
With the multitude of offerings and availability of a plethora of options, customer stickiness is a big problem faced by financial firms. Customer churn forecasting is one of the best big data use cases. It helps in detecting customers who cancel their subscription and analyses the same to tailor products as per customer needs. Video streaming application, Netflix’s subscribers worldwide has continued to grow to reach 167 million through using machine learning analytics on their customer database.
Decision Making
Financial and banking institutions function on facilitating investments made by their customers. Organizations are constantly in search of customers from whom they can get more revenue. This is now possible through performing machine learning analysis on both structured and unstructured data which helps them make more informed decisions. It also analyses data from the website and mobile application to construct effective marketing campaigns for the targeted customers.
Future of Machine Learning in Finance
Financial monitoring, security analysis, prevention of money laundering, network security, investment predictions, personalization of customer service everything comes under the realm of the applications of machine learning in the financial and banking industry. Yet, this is just the tip of the iceberg, there is a lot more that is going to change in the future. It is now visibly imperative that while AI is beginning to create a wave of transformation across these industries and adapting to these changes s important for one’s survival. With smart technology applied everywhere, all financial firms are bound to turn into FinTech’s to stay relevant to the “silver tech generation” consisting of millennials and the GenZs.
Final thoughts
The financial services industry has entered the space of artificial intelligence and machine learning, and the pace is not surprising knowing the positive changes it has brought. Machine learning has the most use cases in finance than any other industry because of the available computer power and new machine learning tools. The greatest applications include simplifying customer engagement and accurate sales forecasting. It is only making this industry better and more efficient with each new adaptation. Machine learning algorithms have the capability to deal with a lot more than human capacity along with eliminating human error. As even the algorithms are constantly learning and innovating, they can serve as a bridge to a completely flawless automated financial system in the future. Nonetheless, the challenges of high cost and lack of resources that come along play a significant role in how early these firms can adopt these technologies. But even then, the future seems bright as the industry has enough adopters and prospects ready to explore.
Blackcoffer Insights 28: Tanisha Gupta, XLRI


How Robo Human will Impact the Future?
It’s the year 2060. An automaton in a Research Laboratory says to a Scientist, “Warning! Error Occurred Reformatting Hard Disk Now!” The scientist panics. Automaton says again,” Ha! Ha! Just Kidding! “.
Funny Right. Before some of you say that this joke isn’t realistic, “How can an Automaton tell you a joke?” But what if I tell you in 2017, “Sofia” the robot made a joke on the show Good Morning Britain! Who thought computers could tell us a joke? Hard to believe? Well, the idea of giving computers human-enjoy thinking has now become a reality. Thanks to the technological advancement in AI in the last decade.
Before diving deep into how AI can impact the future of work, let’s begin with the simple question: what’s AI? Artificial Intelligence provides machines the power to think from data. The machine uses the patterns and trends found in data and makes its decision, but cannot create thought beyond these patterns and trends.
Fig: Impact of robots on employments
With the rise of AI, humans are divided into one question. Are machines human’s friend or foe? Tech executives and politicians on conference stages, campaign rallies, and even science fiction Hollywood movies like Carbon Black, Westworld, Minority Report, and Ex Machina have given their take on this question. Some believe AI will help us solve problems while others believe that the rise of AI will result in destruction and maybe the end of the world, we all know.
Stephen Hawking made it no secret of his concern about the rise of superhuman AI that eventually would escape earth to a new planet. No, this isn’t a plot of Black Mirror. Right now, Superhumans may not be a reality, but AI is.
“Homo Deus”, the emergence of the new Digital God using AI. God must also worry, as AI might take his job.
Here’s some Career Advice, have you thought about being a Robot? The fear that AI would automate all jobs in the future eventually leaving all humans jobless has been daunting for many workers today. Statistics show that nearly 37% of workers worry about losing their jobs to robots. While another thought that many people believe is that though the rise of AI with result in automating most of the jobs in the future, however, it also will create millions of new job opportunities.
AI is already replacing most manual and repetitive tasks. For example, buying a metro ticket or a movie ticket is now almost a human-less interaction. Each year the number of industrial robot jobs increases by 14 %. At this rate, it’s predicted that the 20 million jobs in the manufacturing industry will be replaced by robots due to automation.
The coronavirus pandemic and recession have boosted the demand for automation. The Robotic Process Automation (RPA) Software industry has experienced an increase of 19.53% in the year 2021. Coronavirus pandemic has increased interest in technology that reduces human contact as minimal for making workplaces safe.
Our workplaces will look much different in the next five to ten years. AI will help humans in simplifying repetitive processes. The two most important catalysts for the future of work are the two D’s- Digitization and Datafication. Digitalization is converting data to digital formats (computer-readable). For example, text to Html, analog video to YouTube video. Digitization helps in increasing data exponentially. Datafication is quantifying human life to data and improving the data-driven business model. By 2025, it is forecasted that the digital transformation space will build in a $3,294 billion industry!
One thing is clear, no data, no future of work. What we find is that the future of data and the future of work will go hand in hand. The total volume of data in the datasphere that is created, captured, copied, and consumed in the world is predicted to reach 175 zettabytes by 2025. To give you a much better picture for understanding, if we represent the digital universe as stacks of tablets, there would be 27.25 stacks from earth to the moon.
It’s time to prepare for the data-dominated future as Industry 4.0/Fourth Industrial Revolution has begun. So, let’s see how artificial intelligence will affect the following fields:
Human Resource: Nowadays, recruiters use AI-powered tools for hiring workers. Using these tools, recruiters get insights into a candidate’s skills, personality and even check whether the candidate is fit for the organization. For example, the company AllyO first identifies high-potential candidates through assessment and smart screening, and then automatically schedules interviews using AI. HR departments at large companies receive hundreds of resumes for a job opening. Entry-level roles focusing on screening and scheduling will be automated. AI will automate specific HR jobs, not HR roles. A Deloitte study found that AI has already eliminated 800,000 low-skilled jobs in the UK, but 3.5 million new jobs were also created. Roles that focus on complex decisions like resolving disputes within a department will continue to be a very human endeavor.
Finance and Accounting: In 2015, a report from Accenture named “Finance 2020: death by digital” predicted that 40 percent of transactional accounting work would be automated by 2020. Has technology replaced the human factor? Well, AI has created new jobs involving managing the AI system and using the information to create insights. For example, accounting software has already automated bookkeeping tasks that used to be done by humans, but that’s only opened the door for former bookkeepers to learn skills needed to run and manage the software for employers and clients. Advisors are another crucial role of the accounting and finance team. Using the information gained from transactions in books, the team creates insights to improve business strategy. Owing to automation, the team spends more time analyzing numbers.
Marketing and Sales: Marketing automation has helped companies strategize the proper utilization of the company’s resources, managing time, and achieving budget targets. Marketing automation has helped to draw conclusions at a scale no marketer ever would. In this process, marketers and machines both excel in different parts. Marketers using AI tools drive more conversions in less time. Human Intelligence with technology can help identify the right customers to talk to and at the right time. Modern Marketers understand the insights from any marketing campaign and create it into effective messaging.
Engineering: Technology is changing in a blink of an eye. The technologies used five years ago in the industry have become obsolete today. Engineers will have to keep up with the technological advancements and keep upgrading their skills to stay relevant in the industry. Learning to work alongside machines and designing work such that interaction better humans and machines are better are going to be important skills for engineers in the future.
Fig: Robots takes place the job role of man.
In the 18th and 19th centuries, the rise of the industrial revolution centuries led to millions of people losing their jobs because of scientific advancements. But that also ended in creating millions of other jobs. Statisticians have said, when automation destroys jobs, people find new ones. Thus, AI holds a more optimistic picture for the future.
In the future, AI is not going to replace humans, rather make jobs more humane. AI will disrupt millions of middle and entry-level jobs in the next few years but will also create millions of additional jobs and help to boost economies.
Blackcoffer Insights 28: DAVANG SIKAND, Manipal University Jaipur


How AI will change the World?
Abstract
The way work is being done now is destined to undergo massive transformational changes which will impact humans and their ways of working dramatically. With the development of the new machine programs, A.I. is all set to take over the humans in their workplace as no other did. Now we are not only in competition with other beings but with robots too. And robots will overcome us in our fields of work.
At present we are being surrounded by A.I. from dusk till dawn, from facial recognition present in our mobile application to dating websites/applications which uses decision making as their algorithms and learn from the past data as well. It is believed that A.I. has grown over 270% over the last years.
First, let us know what A.I. is and all the fuss going on about it?
A.I. as defined by the internet is simply ‘simulation of human intelligence in machines that are programmed to think like humans and mimic their actions’. This translates to, it can work as a human being just at fast speed and with 100% accuracy.
In my definition I would define A.I. as the god form of human beings.
Now the question that arises is, what does A.I. entails for the future?
A.I. is fancy enough to continue existing in our minds all the time, but it does come with certain limitations and threats which is a cause for a peaceful sleep for most of the workers. With its introduction to different areas of the workplace, it is clear that half of the human jobs would be taken up by A.I. and then there would arise a need for more jobs for humans in new upcoming AI-based ventures in different industries. Thereby giving us our fair share back to us.
In a survey, it was found that some believed that AI will be devastating for humans and while some professionals and tech-savvy people believed that inculcating AI technology into business and our daily lives would be a remarkable step as it will lead to the flourishment of business in the future and give them a competitive edge over their rivals.
People believed that when these advanced technologies would come together to work with humans, they will produce a smarter strategic decision with productive collaborative practices. More modern technology prevailing in the organization will lead to stress reduction and produce more satisfying results thereby making the organization more efficient.
The Organizations who are in use of the AI technology responds by saying that their managers are more comfortable using A.I. and are accustomed to it and the organizations are now looking forward to having integration of higher-end technology systems, as it is believed that new technology will result in more productivity thereby making more profits in the long run.
Many jobs today require AI and humans working in collaboration, which creates a positive signal that in the future to humans would be working closely with the technology. Beyond just training and developing these machines, humans would be working in close vicinity with them and making decisions on how to act on the result that is given by the machine.
It can be said that both the AI advanced technology and human can’t remain in the workforce without each other, as the technology will produce accurate and top-notch results but it requires someone to make delivery.
e.g., in a firm, an AI-based system produces results based on historical data but there is a need for someone to analyze and communicate and present this data to the respective stakeholders whether inside or outside of the organization.
Various uses of artificial intelligence technology in day-to-day functions in regards to interactions with humans are:
Chatbots
Email curation
A.I. powered advertising
Automated image recognition
Self-sufficient driving
Voice activation and much more.
Here in this figure A represents ‘Artificial Intelligence’ and B represents ‘Humans’
Artificial intelligence as we know works on algorithms, neural networks, and deep learning which all are analytical tools that help AI in taking analytical decisions based on the data provided.
Whereas, humans on the other hand take higher-level decisions based on ‘Intuition’ sometimes, which refers to the gut feeling that generates in humans concerning any situation or challenge.
AI alone can’t work to handle critical situations on its own as it needs humans for it to reciprocate them and share the information with stakeholders, and humans alone can’t anticipate much on the accurate and fast-paced analytical solutions to the problems persisting in place of the situation.
The strength of humans and AI working in synergy can be surprisingly beneficial and advantageous to organizations.
ARTIFICIAL INTELLIGENCE AND JOBS
It is believed that machines in the future would be eating up our traditional jobs. But the reality seems to be turning otherwise.
The future trend shows that in future the AI-powered technology would take up jobs that were being done by humans but in return would produce more jobs that would require human interference with them. As and when the newer technology is approaching more and more countries are now proceeding towards GIG Economies, and so in the future, we can witness an increase in freelance jobs and the permanent labor market norms could reduce drastically.
Some of the jobs today will be replaced by AI which is in the transportation or retail commerce sector that can be 100% automated in the future years. There is an ‘Amazon Go’ store that uses this technology which goes by the name ‘Just walk out technology’ wherein there is no need for any human-induced workforce and all the operations are carried out by AI-powered technologies, which is indeed a breakthrough technology in today’s world.
Rather than eating up our jobs AI in return will be creating more jobs in the future by creating massive innovations thereby fueling up many new industries and thereby giving us our fair share of jobs back.
There will be a lot of demand placed on the upcoming young workforce which is also categorized as ‘Gen Z’. They are expected to know more about technology and would be high in demand.  These young generations are required to learn new skills which are needed to survive in the dynamic changing environment, and as most of the activities that are carried out by workers will be automated, there will be demand for people working in the back office and maintaining and developing the technology to its best versions.  
ARTIFICIAL INTELLIGENCE AND ITS CONSTRAINTS
AI powered technology has its limitations which makes it a rigid system to hold on to and also which makes it a costly affair at the initial stage.  
It was predicted that the cost of electricity to power a supercharged AI model was around $4.6 million. So, this super-powered AI can be purchased only by big fortune firms and thereby creating more value to their net worth.
One of the major limitations of AI is that it can contain biased data as the scientists who put in the data can create biasedness and so the resultant output of the same would have a biased report.
These machines as do not have neuroscience-based technology in them yet which enables them to carry human emotions to understand complex situations and a creative way out of that, they tend to have a lack of out of box thinking which in the case is rigid in themselves as they are programmed to work on a single task and they cannot perform more than a single task at the same time.
It is also believed that there exists no creativity among the computer, no matter it is fast-paced, but they are not intelligent.

PREPARING FOR THE FUTURE
Business and organization
Businesses and organizations need to understand and anticipate the opportunities that the future holds for them and they need to start training their employees based on today’s dynamic changing technology. While it’s still unclear what does the future holds for us, but the anticipation of it could benefit us in several ways.
As we are unclear about what the future looks like, we need to think in probable terms how it could turn out to be and then employ specific training programs for the employees of the organization.
The training the employees are needed to be done on a continuous and lifetime basis which means that education won’t be only limited to PG degrees but will be now a lifetime process of learning.
As the Covid-19 changed the scenario of the work patterns around the world, we now need to think strategically about the working dynamics of the future and how does it look like compared to the pre-covid and post-covid scenario.
Employees will be playing a major role in transforming the organizations and work practices in the upcoming future, so organizations need to select and recruit the best candidates among the pool and then provide them with best practices of the new machines and make proficient in their area of work. Policies need to be developed to hire the best people and then retaining the talent in the organization.
There needs to be a continuous scanning of the environment by the organizations to comprehend any new trends and assess them, not all trends will be beneficial for organizations, they must be aware of the prospects and plan for the future systematically and consistently.
CONCLUSION
There lies a possibility in future certain years from now, we could have machines who will have general human intelligence who would be able to answer deep meaningful questions asking ‘Why are the curtains blue?’, would be able to clean cars, play politics and tell jokes to us, and by using deep and machine learning programs their level of intelligence would be beyond mathematical calculations to us. That’s how good machines will be in the future, but to make ourselves competitive with machines, we would need to train ourselves for the impending ambiguous future ahead of us.
Humans and machines need to work in synergy to get beneficial and satisfying results for both parties.
Machines are indeed going take away many of our jobs, but let me make you sleep peacefully tonight, the machines aren’t arriving until we’re retired.  
Blackcoffer Insights 28: Utkarsh Mahatma, Ahmedabad university


Future of Work: How AI Has Entered the Workplace
Abstract
Artificial intelligence and employment are the burning issues nowadays workers need clarity on as we head into the longer term. This article focuses on the various impact of AI on our jobs and explains the benefits of AI in our workplace. That right there must have hit a nerve. However, everything is about to change because this article highlights some of the reasons we should not fear AI.
What is AI?
AI is the abbreviation of Artificial Intelligence. Artificial intelligence is often defined as a set of various technologies which will be brought together to permit machines to act with what appears to be human-like levels of intelligence. This includes learning rules required to make certain decisions and reasoning to arrive at certain conclusions, learning from past experiences, and self-correction.
AI is of two types.
Narrow/Weak AI – AI solution designed and trained for a specific task.
Strong AI – An AI solution whose intelligence matches that of a human brain across multiple & differentiated task.
Key Moments in the AI Journey
1940
Expectations that machines could match humans in terms of general intelligence. By that, we mean machines could have the capability to find out, Reason, and React.
1950
Alan Turing develops the Turing Test; a test to determine whether a machine is intelligent.However, it wasn’t for another 60 years or so that any program was deemed to have passed.
1955
The first time a computer virus defeats a person’s World Champion during a parlor game.
1956
John McCarthy invents the new term ‘Artificial Intelligence’ when he held the primary Academic Conference on the subject of AI.
1962
Arthur Samuel‘s machine learning checkers (draughts) program, beat a checkers master.
1980
Reinforcement Learning is introduced. This is a kind of programming that uses rewards and punishments to coach a machine to interact with its environment.
2012
A research group led by Geoffrey Hinton wins the Image Net competition – this competition requires AI to categorize about 1.2 million images into any of 10,000 different categories. The level of accuracy was adequate to that of the typical human completing an equivalent task manually.
2014
Eugene Goostman’s chatbot, a bot pretending to be a 13-year-old boy, supposedly passes the Turing Test, a test which nobody has passed before! But controversy arose with this claim as:
1. Experts claim it only lasted five minutes.
2. It had been deemed biased as Eugene’s mother tongue (Ukrainian) wasn’t equivalent to the judge’s (English), which is a plus as language is one among the few ways we will tell the difference between a person’s and machine.
2018
Alibaba’s AI Model performs better than humans during a reading and comprehension test at Stanford University, scoring 82.44 against the 82.304 scored by humans!
Concerns about AI
Artificial Intelligence (AI) is here to remain, and lots of people aren’t happy. After all, it’s hard to embrace something that would displace about 40 percent of human jobs within the next 15 years. In an interview for CBS’s hour, Kai-Fu Lee (a Chinese AI expert) also mentioned truck drivers, chauffeurs, waiters, and chefs as a number of the professions that will be disrupted.
But if you were to ask the experts, they might unwaveringly confirm that no matter all the noise, AI is here to profit us all. Case in point, an executive briefing by the McKinsey Global Institute revealed that AI and automation are creating opportunities for the economy, society, and business.
That said, it’s time to repress the widespread idea of artificial intelligence taking jobs. So, let’s highlight a number of the useful developments you’ll expect from this technological phenomenon.
AI will create jobs
While the relationship between artificial intelligence and jobs is a matter of hot debate, it is still safe to say that AI will indeed offer new opportunities. According to the planet Economic Forum report, robots and AI will create as many AI jobs as they displace. This conclusion is entirely viable as it is easy to identify some of the many careers in artificial intelligence, for example, data scientists, who evaluate the decisions made by AI algorithms to eliminate any biases. Apart from that, some other AI occupations include:
Transparency analysts: people tasked with classifying the varied sorts of opacity for algorithms. Smart-system interaction modelers: experts who develop machine behavior based on employee behavior. Machine-relations managers: people that champion the greater use of algorithms that perform well. As far because the competition for jobs between humans and robots goes, worth noting is that there are jobs that AI can’t replace. Roles that need leadership, empathy, and delegation are samples of the various jobs that are safe from automation.
AI will eliminate bias and a variety of challenges at work
Automation will stir positive change in the workplace. When AI is employed during recruitment or maybe performance management, all workers are going to be evaluated in an unbiased, fact-based manner. In turn, Human Resources managers can get to consider other essential strategic undertakings that ensure balance within the workplace.
AI can help HR departments to use machine learning (ML) in discovering where issues like bias stem from and assist them to act accordingly faster. ML shines in identifying instances of bias. In turn, this may promote fairness and variety within the work setting.
AI in the workplace will steer business-outcome strategies
The impact of AI in business is already felt, and this is often expected to continue through to the longer term. A few years from now, AI-oriented architecture is forecasted to require the lead in assisting businesses to hold out operations in additional comprehensive ways, thus shifting them from traditional data science and machine learning models. It will be necessary to maneuver to business outcomes because AI will play an important role in multiple aspects of the business. While there is no way of telling the future of artificial intelligence in business for sure, it makes sense for owners to keep up with the evolution to avoid being left behind.
AI will boost innovation within the workplace in the longer term
The workforce of the longer term will lean more towards innovation and creativity. Businesses have spent higher a part of the previous couple of years studying AI automation and the way they will leverage it to realize results fast. With statistics showing that workers spend up to 40 percent of their hours at work performing repetitive tasks, every business should consider automating any functions which will be automated. Automation is not new; machines have been replacing human labor in different areas for decades.
However, within the coming years, mundane daily tasks will become fully automated. Already, 39 percent of organizations were completely reliant on automation in 2018. With repetitive tasks taken care of, employees can focus their energies on high-value customer-oriented tasks and collaboration. The designs of workplaces and workflows will also change with the implementation of AI technology. More people will begin to figure more closely with machines as companies will strive to become more agile.
Companies that implement AI in their business strategy will experience dramatic improvements in their customer experiences, and their employees are going to be more motivated. Encouraging creativity rather than the performance of repetitive tasks gives workers more fulfillment in their jobs as well.
With another technology, AI will positively impact the world
With the web of things and AI working hand in hand, identifying trends and solving problems within the business world will become more convenient and also sustainable. AI, alongside other technologies, is predicted to vary the planet by impacting the way businesses run. With time, we’ll be ready to combine both human and artificial brains to seek out solutions to major global problems.
It will even be easier to foresee problems with more accuracy and nip them at the bud with the help of AI. But these positive impacts can only be felt if stakeholders are transparent and mindful in their use of the technology for the greater good of everyone else.
Robotics and AI will boost productivity
According to a report by PWC, 54 percent of companies confirm that the implementation of AI-driven solutions in their companies has already improved productivity. AI and automation, even once they are implemented partially, have unlimited potential for any business. Workers’ skills, attitude, training, command chain, and workflows protocols are a number of the leading productivity challenges that companies face.
Apart from increasing productivity, AI systems will help businesses to chop down on costs, improve the standard of their products or services, and make better customer profiles. As a result, companies also will get higher profits, which may be shared among stakeholders as dividends, or reinvest it back within the business. Improved productivity also means firms are going to be ready to sell their products at lower prices, thereby creating more demand among customers and more job opportunities for workers. Businesses can, therefore, use human labor to take up those jobs that have been created by AI and cannot be automated.
Final thoughts on the impact of AI on jobs
Artificial Intelligence isn’t showing any signs of slowing down. Soon, it’ll become another necessity of life, a bit like the web. But for now, more and more businesses are beginning to realize how invaluable AI automation and data interpretation are. Though machines will take some jobs initially, the roles created by automation will also keep soaring within the next few years.
Will, we’ve reached the purpose of accelerating human intelligence by artificial means? Who knows? What we all know, however, is that those that are going to be sought-after in AI and employment are individuals who have the relevant skills. There’s work that not even the machines can do; so, we should make ourselves as valuable as we can be in our field and we will be irreplaceable.
Blackcoffer Insights 28: Adrita Anan, Holy Cross College, Tejgaon, Dhaka


How machine learning used in finance and banking?
Through AI tools like natural language processing, Alexa and google assistant has led the retail industry in its rise towards conversational commerce. As if a customer was interacting with a clerk in a retail store, conversational commerce makes it possible for users to engage with software to research, purchase, or get customer assistance with products and services across a wide range of industries. With Alexa, for example, users can ask any Alexa-enabled device to add an item to an Amazon shopping cart, set a purchasing reminder when a product is running low, or carry out a complete purchase without having to access a shopping cart. The result is a seamless conversational experience that enables consumers to carry out transactions as quickly as it takes to speak a sentence.
Through AI tools like natural language processing, Alexa has led the retail industry in its rise towards conversational commerce. As if a customer was interacting with a clerk in a retail store, conversational commerce makes it possible for users to engage with software to research, purchase, or get customer assistance with products and services across a wide range of industries.
With the advent of personalized products and on-call delivery, customers have come to expect a new standard experience: fast, easy, accurate, and personalized. Accomplishing this without sacrificing your workday can be a challenge, since the data processing required to meet these needs is immense. Luckily, virtual agents (VAs), powered by conversational AI, can utilize this information faster and more accurately than humans, finding insights and automating communication to deliver an enriched customer experience. If you invest based on these improvements, you’ll find that implementing these tools delivers a powerful competitive advantage. AI has helped in automobile, education, retail and commerce, finance and banking and healthcare.
Voice AI has powered the wheels of conversational e-commerce, which has impacted the way the customer communicates with the brand in multiple industries. Brands generally build a campaign to emotionally connect with customers, for long-term growth. With Voice, brand campaigns need to be short and ones that can lead to immediate buying. Conversational e-commerce is still in its nascent stage and it is expected to grow manifold in the coming years. The future of shopping is going to Voice AI and marketers have to get on the bandwagon fast to increase their brand value and visibility. Targeting will have to be highly personalized for success.
Despite its narrow focus, conversation AI is an extremely lucrative technology for enterprises, helping businesses more profitable. While an AI chatbot is the most popular form of conversational AI, there are still many other use cases across the enterprise. 
While an exclusively chat- or voice-based shopping experience for all scenarios may never completely replace the in-person experience, conversational commerce will continue to grow as an added method of convenient and efficient communication. As users continue to become more accustomed to engaging with chatbots and voice-driven interfaces, expect more innovations in the space as brands continue to develop their unique conversation-based solutions.
Blackcoffer Insights 28: Samyak Jain


How AI will impact the future of work?
AI experts believe it’s going to be one of the main drivers of the fourth Industrial Revolution and that it has the potential to not just transform the tech sectors and going to open a new chapter of the society of the world that people try to understand themselves better rather than the outside world with AI because people who are naysayer and kind of try to drum up these doomsday scenarios are pretty irresponsible. After all, In the next, five to ten years AI is going to deliver so many improvements and the quality of our lives it is a renaissance, a golden age of machine- learning and artificial intelligence that was the realm of science fiction for the last several decades. AI is probably the most important thing humanities that have ever worked which is more profound than any work with technology, as it is important to harness the benefits and while minimizing the downside is focusing on autonomous systems like self-driving cars seen as the mother of all AI projects and has made applications like self-driving technology viable for the first time, three things happen at the same time number one data collection and data processing became easier because of better technologies right um you need data to fuel AI training and that’s been one of the big drivers the second thing that has happened is that computer processing has become faster that’s like the engine so no matter how much fuel you have if you don’t have that engine and processing the data on a timeframe that’s reasonable was just not possible and the third thing that’s happened is that new algorithms have been developed which has made AI much more powerful so #technology has been changing and developing at a pace that’s much faster than ever before and we have not been used to this rapid pace of change which means that we have not been used to thinking about how it’s going to impact our immediate future. The most important factor responsible for the growth of AI is Google and its AI what Google’s done is given all of us the power to get the relevant information we want at our fingertips this has created a shift in how things are bought but it didn’t happen overnight this started in 2004 but the major change only happens to start 2012 onwards Google’s taken away about 65% of sales people’s jobs that were primarily order takers and the ones that are remaining are likely to be gone over the next decade.
In present several AI projects are helping in diagnosing diseases better match up drugs with people depending on what they’re sick they can get treated better so it’s going to help a whole lot of people get treated and get better #healthcare than would have had access to it before if you look at self-driving cars they’re going to be safer than people driving cars and the value that machine learning is providing is actually happening beneath the surface and it is things like improved search results improved product recommendations for customers improve forecasting for inventory management and literally hundreds of other things including speech-recognition or image-recognition that the performance levels are phenomenal  or drug discovery as these biological systems are very complicated because vaccines for TB and HIV developing that’s notably enabled by this rich data advanced in biology and machine learning and recent invention in which is an application we just launched for anybody with visual impairment ass it uses the latest cutting-edge computer vision technology to give anyone the ability to see, so anyone who has dyslexia can now use AI to be able to read better and with the  latest release of Windows 10 has this capability called IJ’s which enable the eye muscle that the gaze can help to type. Like the two sides of the coin, there are negative impacts of AI as well  Bill Gates Ellen Musk also tech giants in a way their views are pessimistic, to say the least, they warned against the potential of AI to replace humans in the workplace and Ella masks even went as far as to claim that AI is the biggest existential threat to mankind. because of the loss of a job, when you think about a job or a career choice if a majority of the tasks that comprise that career choice is likely to be these vulnerable tasks then that is a career at risk in the future so what are the tasks that AI will find? hard to do anything unpredictable anything that requires skills like creative thinking or empathy or interpersonal skills but it’s important to understand tomorrow whether Google is there or not, artificial intelligence is going to progress you know technology has just nature it’s that it’s going to evolve as  technology and in particular AI can, in fact, bring more empowerment more inclusiveness and at the same time it is important to be clear-eyed about displacement and unintended consequences like any other technology and work both skills so that people can find the jobs of the future create new jobs also the policy decisions that help people as they go through this change people already unhappy because of machine learning artificial intelligence as they think  if they’re not innovative enough or not creative enough your job will be taking away by a lot of machines AI for business going to affect the future of work specifically there are jobs that are at more risk of being taken over by AI and automation there is very wide dissonance on this, there are different reports that have been shared by  Oxford study that says 47% of US jobs are at risk of automation over the next few years meanwhile the general population and workers think differently a recent study conducted by college actually identifies that 97% of workers believe that most jobs will be automated but not their own this suggests that the general public needs to be educated on which jobs are  susceptible to this risk which are not and businesses need to be aware of the forthcoming skills gap of course not all jobs are equal the Oxford study that highlights this they examined 700 participants and found the generalist occupations that require creative knowledge or innovation are at least risk the same is true for occupations in education healthcare media and arts jobs on the flip side jobs like telemarketers junior lawyers accountants are at most risk in short there is a simple rule of thumb if your job is in some way predictable or routine the risk of automation is much higher if a job doesn’t require innovation or creativity then the return on investment for companies is higher on machines than real-time employees machines are faster can’t be distracted and can work 24/7 this is actually good for creative marketers because AI and automation can serve to augment their jobs rather than substituting them as impact of emerging technologies on the creative economy they stated that artificial intelligence is changing  creative content from beginning to end by 2030 AI will be able to write high school essays code in Python composed top 40s chart songs and make creative videos but all of these advancements also come with risks and costs take a look at this report by the global Commission on the future of work in the absence of effective transition policies many people will have to accept lower skilled and lower paying jobs high-skilled workers are taking less cognitively demanding jobs displacing less educated workers and this is already happening also technological dividends are being unevenly distributed among firms a very limited amount of companies tend to dominate when it comes to big data just think about Google and Facebook today they alone are responsible for 70% of the referral marketing traffic and  receive more than 50% of total global advertising budget so the question is in businesses workers and social institutions go into the same direction if companies and public policy leaders can understand the evolving landscape they can help the workforce anticipate the upcoming challenges technology and the demographic changes are leading to a smaller workforce compared to the previous generation and a workforce that has to pursue many careers during their time of work we need to provide workers with an environment where they can continuously upskill and grow governments will have to re-evaluate the educational system we will have to continuously learn and grow and companies will have to redesign their structure and their culture around technology just like during the Industrial Revolution we are heading into a new age and the great transformation that we’re about to see by 2022 it is estimated that 20 to 25 percent of the labor force will be displaced within 10 to 20 years however this is also an opportunity for people to get ahead for which different ways have to be find to attract and retain highly skilled workers and allow them the time to up skill themselves even during work hours and it is a  good way  to develop a learning community to benefit from each other and also to use technology to supplement goal tracking and  efforts instead of as a distraction in short what we are doing is  to bridge the dissonance and it is imperative to build a  map of how AI and automation will affect  industry and  company if this is an economic imperative how do people feel about committing itself to a lifelong approach to knowledge as  these risks are important but it is important to do things like from being upfront to have ethical charters like AI safety and to be very transparent and open and how we perceive progress there and figure out global frameworks by which we can engage just like Paris agreement and climate change by using  such forums bring people together as they engage on the hard questions and it will emerge answers and on the question of whether AI is a threat or not, artificial intelligence is not  a threat because there is a rare case where people need to be proactive in regulation instead of reactive because I think by the time we are reactive in AI regulation it’s too late right now we have machine learning algorithms that can solve an incredibly complex problem beyond any human intelligence  as they are mere machines that can be given enormous data set and they come up with brilliant correlations and insights but they’re not going threaten the human population anytime soon because fish intelligent isn’t terrible but human being a smart enough to learn that skills at least to have a complete toolbox to be prepared volatility of the future adaptability.
Blackcoffer Insights 28: Mihir Bhatt, Delhi University ( SGTB KHALSA College)


All you need to know about online marketing
    Ever wondered how you get notified of the products or services you want or you have been looking for for a long time. Now how does this happen? Let me start with the simple and the most known fact- Marketing,
Marketing is a common term which everyone knows and is aware of. Marketing is the action of promoting products and services, including market research and advertising.
Traditional Marketing was working fine for all these years. So why there was a need for online marketing?
The story begins from the internet era, The online presence of customers-
There are 4.72 billion people are on internet users in the world today and the number is increasing day by day. So if the companies want to create awareness about their products and services their is a huge audience present online.
There are a lot of benefits of online marketing like a large audience, benefits of targeting on the basis of demographics, location, age, gender, and many more. Because of this diversity, almost every type of company can use the Internet to reach any audience. All would find something of their liking.
Let’s take a look at what does online marketing involves.
Types of Online marketing
Social Media Advertising
These days, almost everybody is on social media. The majority of people use Facebook, Twitter, Instagram, and other social media platforms to communicate with their friends and relatives. Some people have created companies solely based on their social media activity.
You can, however, promote your Knowledge Commerce products through social media. Whether or not you advertise on these sites.
The best social media networks for advertising-
Facebook advertising
Instagram advertising
Twitter advertising
Pinterest advertising
LinkedIn advertising
Snapchat advertising
Content Marketing
Content marketing is a strategic marketing strategy that focuses on producing and delivering useful, appropriate, and reliable content in order to attract and maintain a specific audience — and, eventually, to drive profitable consumer action.
In simple words, content marketing is a marketing strategy that producers and delivers relevant and reliable content to attract potential clients and to also retain existing clients.
PPC/Search advertising
About half of all website traffic originates from a search engine. People who use searches are often high-intent buyers. This indicates that they are searching for a particular item. They’re all set to buy the products and services.
The majority of online marketing is focused on pay-per-click (PPC). However, when you hear the word “PPC,” it refers to search ads.
Pay-per-click (PPC) advertising on search engines, social media sites, and other online venues can be extremely successful.
So the next time when you search for a product and services and you find similar ads on the internet this is a kind of internet marketing.
E-mail marketing
Email marketing is the highly successful digital marketing technique of sending emails to prospects and consumers. It allows communicating directly with the present, former, and potential customers. businesses will inform the audience about new products as they become aware of them.
Banner advertising
Banner ads are rectangular or square advertisements that appear above, in the sidebar, or below the content on websites. Usually, a banner ad leads to a sales or landing page. You will get great results by running banner ads on websites that attract members of the target audience.
Affiliate Marketing
An affiliate marketer, like a car salesperson, only gets paid when someone buys the stuff. You may not have to pay if there are no purchases.
Affiliates are free to sell your goods anywhere they choose (as long as the material follows the terms of service of the website). It’s a fantastic way to reach new markets.
Anyone can start their affiliate marketing career by registering on the different affiliate websites.
Influencer Marketing
Influencer marketing is a new trend for online marketing. They have a strong following, can inspire people to buy your goods, and are loved by their viewers.
The type of online marketing that will work best for the company will be determined by many factors, including the nature of your industry, the tastes and demographics of the target market, and budget. Market analysis will guide to the best strategy or combination of strategies for your offerings, and comprehensive performance metrics will show you which are the most effective.
Advantages of Online Marketing
Online advertising provides a large client base for a company’s services or products. All kinds of companies, from multinationals to small and medium enterprises, have access to millions of potential customers. The higher the number of users who visit your website, the more sales you can make.
One can advertise their company 24 hours a day, seven days a week, through online marketing campaigns. You just won’t have to think about employee pay or shop hours. Furthermore, time differences in different parts of the world will have no impact on your campaigns. 
In today’s ads, social media is important. This is due to the fact that customers read comments and feedback left by other customers on the internet. Businesses can easily integrate social media tools into their advertising strategies and benefit from consumers who use social media extensively.
In an online marketing process, consumers can be demographically targeted even more effectively than in an offline process. Organizations will enhance their targeting over time, have a better understanding of their consumer base, and generate exclusive deals that are only shown to certain demographics when combined with the improved analytics.
Online marketing is a rapidly expanding industry that benefits companies in a variety of ways. The number of people who purchase goods and services online is on the rise. As a result, an increasing number of businesses around the world are turning to internet marketing to communicate with consumers and advertise their goods and services.
 


Evolution of Advertising Industry
Evolution of Advertising Industry over the years.
Advertising can be described as a type of specific content broadcasting to a larger audience; the form can take several different forms, and the intended message can differ from genre to genre. The target for each medium could be different. Advertised content could be in print, radio, TV, or digital formats.
We’ll look at how the advertising market has changed over the last ten decade.
Advertising is a form of communication that aims to persuade a target audience. Typical advertising messages endorse programs, goods, concepts, people, and companies. 
First, conventional types of advertising were used to carry out advertising. Let’s take a look at the traditional forms of advertising.
Advertising can include in-flight advertising, street furniture, passenger displays, billboards, skywriting, posters, wall paintings, banners, taxi cabs, passenger screens, television, and newspaper advertisements.
Press advertising-
Other types of advertising include press advertisements in magazines and newspapers. Advertising in the classified section of a newspaper is an example of press advertising. A billboard or digital screen placed on a moving vehicle is often referred to as a mobile billboard.
Guerrilla advertising-
When a brand or product is used in a large entertainment venue, it is known as convert ads or guerrilla advertising. 
When a soft drink, a watch, or a pair of sneakers is seen or mentioned in a common film, this is an example of this.
In-store advertising-
Ad in supermarket videos, aisles, and on the inside of shopping carts is referred to as in-store advertising.
Consumers are influenced by celebrity advertisements because of the power of wealth, fame, and popularity. However, if a celebrity falls out of favor, the use of that celebrity may be detrimental to a company.
Non-commercial ads-
Religious organizations, political parties, political candidates, and special interest groups are examples of noncommercial ads.
These were the conventional forms of advertisement, but as the internet and technology progressed, the advertising industry began to play a role in helping brands establish a digital presence and advertising their products in a new way.
The advertising industry is a multibillion-dollar global company that connects producers with customers. According to the research firm eMarkerter, global media advertising spending totaled nearly $629 billion in 2018, with digital advertising accounting for nearly 44% of that amount.
For more than a decade, consumers’ perspectives have been shifted in favor of commercials. Advertisements are created based on the preferences of the target audience, and as the population has become more tech-savvy, advertising agencies have shifted their focus from conventional to digital advertising. The internet, as well as the devices, used to access it.
Internet advertising has evolved from a risky gamble to the main marketing medium for most businesses. Digital advertising continues to expand by double digits on an annual revenue basis in the United States, with overall spending exceeding $129 billion in 2019.
Mobile marketing-
Mobile advertising is a form of advertising that uses wireless devices such as smartphones, tablets, and personal digital assistants to view advertisements. In the consumer goods and retail industries, it is extremely necessary.
Mobile advertising contents tailored to particular age groups present an opportunity for the mobile advertising industry. The challenges that the mobile advertising industry faces pose a significant risk of new entrants.
Content Marketing
Content marketing is an old trend that has resurfaced. Many marketers have struggled to determine how powerful banners and display advertising on other people’s content are.
Companies are embedding their marketing pitch within the content itself, rather than serving an ad. This can take the form of publisher-tailored content that the advertiser can support or content that the advertiser publishes directly.
There are different kinds of businesses and websites that have used content marketing to grow and flourish in the industry. Content marketing is a trend that has contributed a large amount of income to the advertisement industry.
To summarise, the advertising industry has evolved through time and will continue to do so as technology advances, allowing advertisers to reach a wider audience and gain a greater understanding of the people to whom they are delivering material.
The advertising industry will continue to develop in tandem with innovation. People are also becoming more jaded when it comes to advertisements, pushing businesses to come up with new ways to convey their messages. However, advertisement has a promising future. 
 


How Data Analytics can help your business respond to the impact of COVID-19?
Before we get into the whole discussion, let’s first discuss the basic working of data analysis.
Data analysis is defined as a process of cleaning, transforming, and modeling data to discover useful information for business decision-making.
Using data analysis, we can extract useful information from the given data and then take corresponding decisions based upon the analyzed data.
During the pandemic known as COVID-19, many businesses failed to grow whereas many touched the sky, for example, the transportation of raw materials was drastically low because:
The nationwide lockdown was imposed,
Low production due to a smaller number of workers,
Storage facilities were shut down, and many more for such reasons.
On the other hand, new business/startups got a chance to compete in the market by getting early responses from the companies they wanted to tie up with or from the head of the companies that they wanted investments from.
The data analysis can help businesses in many ways such as:
Using different statistical models that are used in data analysis, the businesses can predict the approximate requirement for the product in the near future and hence they can produce it accordingly.
It makes it easier to track the requirement and produce the product accordingly.
Because of the lockdown, people had to start working from home, which became a huge advantage for the businesses as they would get quicker responses from their tie-ups.
Using data analysis, businesses can create several models and structures to measure the growth of the company and also to make devised plans to increase the revenues and decrease the losses.
Data analysis provides different analytical techniques such as:
Text Analysis
Statistical Analysis
Diagnostic Analysis
Predictive Analysis
Prescriptive Analysis
Using these techniques, businesses can analyze everything and predict almost anything.
These techniques, tools, and models, can help businesses tackle this horrendous situation that is COVID-19.
Blackcoffer Insights 27: SHAILI SHARMA, St. Xavier's College


COVID-19 Environmental impact for the future?
Dreams are powerful! Right? Be it for escaping reality, envisioning the future, or for experiencing adventures like spacewalk or time travel, we all love dreaming. So, if I ask you what do your eighties look like, what do you imagine? 
Well, once I dreamt of my dotage, and I was surprised by the world I imagined living in. Every morning, I would wake up listening to sweet chirps beside a sparkling brook that ran through the lush green mountains. In that world, older people like me wouldn’t suffer from any breathing problems. Frequent traffic jams, congested buildings, and industries emitting excessive smoke were forlorn memories. History books would include chapters on the ‘Climate action strike’, Biodiversity Restoration, and Water Conservation Campaign. In those aging times of mine, I would sit for hours by the riverside observing the blooming aquatic life. Then the swiftly jumping fishes would splash water onto my face, suddenly shattering this wonderful world that I was in love with. Coming back to the present, out of this imaginary world, now I constantly question myself, “Will this just remain a dream?” Well, I believe, 2019 coronavirus lockdown is a positive kick-start to transform my pleasant dream into the ultimate reality. The lockdown has shown how to live green, but surely it has its own implications.
Due to the COVID-19, many countries imposed lockdowns to curb the virus. The world has faced difficult times due to rising issues in the economic sector, public health infrastructure, and instability in global cooperation. These serious issues undoubtedly have to be addressed with utmost priority, and without overshadowing the existing global problems. Notwithstanding the harm to the various sectors, the environment we live in has also been quite closely impacted by the COVID pandemic and in an unprecedented manner. It is interesting to explore more about how the COVID-19 lockdown has impacted biodiversity around the globe.
During the lockdown, for the very first time humans were caged in their homes and animals no more had fear of someone heckling them. Even though some animals celebrated their freedom, many suffered due to the lack of attention. Have you wondered what happened to those already endangered species, who were suddenly left uncared for? Unfortunately, because of the home confinement/social isolation, the staff at such places was also reduced and this has increased illegal activities. Reports from Kenya, Cambodia, and the Philippines have confirmed an uptick in poaching, illegal logging, and wildlife trafficking. Data collection and studies regarding animal livelihood were halted making regular health monitoring of different species nearly impossible. Moreover, the pandemic triggered a nasty economic recession leading to a reduction in funding for wildlife preservation. Considering the severity of the unprecedented threats to biodiversity, the conservation efforts need to be funded at higher amounts than the level they were at before the pandemic.
On 22nd May 2020, we celebrated the International Day of Biological Diversity with the theme – “Our solutions are in nature”. Have you ever wondered why we need a special day to recognize the importance of biodiversity? It is because we need to stop the deliberate negligence towards the flora and fauna and realize that living happily is also their fundamental right. Though biodiversity conservation was overlooked during the lockdown, let’s see some encouraging examples where nature itself tried to heal the wounds that we’ve caused.
In the midst of this pandemic, the coronavirus lockdown has been incredibly beneficial for marine life in many parts around the globe. Reduced levels of pollution and human activities have allowed underwater ecosystems to rejuvenate themselves giving us some miraculously soothing experiences like live dolphinariums in the canals of Venice. Only a few weeks of lockdown had led to the improved quality of water in River Ganga. The returning of endangered species of turtles to the beaches in Thailand and the recovery of horseshoe crabs along the shores of New Jersey are some other examples. Though this deadly pandemic should not be considered an acceptable way to rejuvenate the endangered ecosystems, it surely has shown how positively the wild creatures respond when humans take a step back.  This pandemic is proof that “Our solutions are in nature, and this is our only option”. It surely gives us hope that like the COVID curve, we can also “Bend the curve of biodiversity loss” and encourages us to develop sustainable and earth-friendly solutions to Build Back a Greener Earth. It has taught us that Building-Back-Better is possible more quickly than we had expected and this is the right time we start focusing on ‘Green Jobs’, jobs that genuinely contribute to a sustainable world.
Like humans, nature too has emotions but is a bit more expressive in its unique ways. Colorful rainbows and mild breezes passing through tall trees might be its way of sharing happiness. It might get angry at our destructive activities causing volcanic eruptions, huge bush fires, etc. in frustration. We sometimes plan a vacation to relieve the stress and mother earth also desires the same. I wonder if mother-earth has just put down this pandemic as leave for vacation to heal herself. If it is so, then it would be better next time we willingly give her a weeklong yearly vacation by reducing human activities and recreating the lockdown scenario. Although this will affect economic growth, this will help to maintain harmony between human beings and biodiversity to avoid human sufferings as a consequence.
It’s high time now that we take collective action and build a shared future for all creatures in the ecosystem in a greener way and with more nature-based solutions. Let us all pledge to flatten the curve of deforestation, habitat destruction, and climate disruption to prevent our Mother Earth from reaching ‘the point of no return’ and ‘dire catastrophe’.  Ideas like weeklong yearly lockdown can be cherished for developing sustainable solutions to ‘shift from the grey to green economy’. With collective efforts, we can definitely Bounce Back Greener from this pandemic. When nature itself, through this lockdown has shown a trailer of Green Living, so why not create a beautiful movie out of it?
Blackcoffer Insights 27: Dhanashree Revagade and Shrenik Shingi, IIIT Pune


Environmental impact of the COVID-19 pandemic – Lesson for the Future
The Covid- 19 pandemics forced factories to shut down, flights getting canceled and a massive decrease in the global economy, with a significant decrease in Green House Gases (GHG) in many developed and developing countries.
The SARS- CoV2 came into the spotlight in December 2019 and has impacted most of the countries till then. Nearly 131 million peoples were infected worldwide and resulting in deaths of around 2.9 Million according to World Health Organisation (WHO). Most of the countries dealt with the new virus by imposing strict lockdowns and social distancing to control the spread of the virus. These policies caused adverse effects worldwide. One of the most important impacts of the Covid-19 Pandemic is on the environment.
There have been few positive impacts on the environment due to lockdown like, air pollution has decreased dramatically, as people were asked to stay in their houses due to the lockdowns. There has also been a sharp decline in environmental noise. Environmental noise can be well defined as the unwanted or harmful outdoor sounds caused by human activities like noise emitted by road traffics, air traffics, rail traffics, and industrial activities. It is one of the most important challenges in the modern era as noise pollution can cause adverse effects on humans as well as harm many animals too. The imposition of lockdown and quarantine by various nation’s governments has caused people to remain back at their homes. Because of this, the movement of people from one place to a different place has reduced significantly and the use of personal and public transport has also decreased. Due to all these changes, the environmental noise generated in most of the cities has dropped substantially.
Water Pollution at beaches have reduced significantly and many animals were spotted back in the cities, but the covid-19 virus has also generated many negative and indirect effect on the environment.
To begin with, some of the developed countries have halted their sustainability program during the pandemic. In the United States and in many other European nation-States, waste recycling plants have been suspended in many municipalities and cities due to the concern of the virus getting spread at the recycling centers. This has resulted in an increase in the use of single-use plastic bags instead of the re-usable bags by many leading restaurants, firms, and corporations. For instance, Starbucks, a leading coffee company during the month of March 2020, has announced a short-lived ban on the utilization of recyclable and reusable cups.
Furthermore, with most of the people staying indoors because of the lockdown majority of the department stores, shops, restaurants, and food outlets are closed, making an online purchase and food delivery are quite high. This has created more consumption and demand for fossil fuels due to the transportation and mobility of these goods to each individual. There has been an enormous upsurge of medical waste- because most of the products employed by healthcare professionals are usually single-use items that can be used only once before they are disposed of. Some of these wastes include used masks, personal protection kits, and gloves. For instance, nearly 200 tons of medical waste were generated during the peak of the pandemic breakout in Wuhan, China. This is 50 tones more than the average waste generated before the outbreak in Wuhan.
These organic and inorganic wastes generated due to the policies crafted by the government takes a heavy toll on the environment and can cause environmental issues like water pollution, air pollution, soil erosion, and can harm the local flora and fauna.
The demand for masks during the pandemic has become skyrocketing but the materials required for the production of these masks are highly dangerous for the environment as they are generally composed of non-woven fabrics. Polyester, Polystyrene, polyethylene, and polycarbonate, are some common materials used for the surgical mask with density lying between 20grams to 25 grams/sq. meter. These materials are mostly resistant to liquids and are plastic-based products with a really high afterlife after being discarded. If not treated properly without discarding, they end up filling the landfills and the oceans making it dangerous to aquatic lifeforms. For example, recently the environment in Hong Kong has started degrading drastically during the pandemic with the accumulation of these clinical wastes.
With recycling plants on hold, piles of small mountains of wastes and their depositions at open areas are formed due to the increasing number of unrecycled wastes generated every day. This makes the surrounding more vulnerable and also creating a high risk of air pollutions as the dumped open wastes decays into Methane (CH4), a greenhouse gas, hence increasing the risk of global warming too.  Not only the surroundings are getting affected by these careless methods, but the local people are also getting affected. If the excess methane gets accumulated in the Earth’s atmosphere due to piles of unrecycled wastes, this could result in increases in Earth’s average temperature and can be harmful to future generations.
Many protected and endangered flora and fauna are facing much greater risks due to the imposition of the lockdown. Many countries under lockdown have made people stay inside their homes. The employees, NGOs, and volunteers working in these protected areas like National Parks, Marine Conservation Zones, and wildlife sanctuaries are made to stay at home making these protected and endangered flora and fauna unattended. This has also led to an increase in many illegal activities like wildlife hunting, illegal deforestation, and fishing activities due to the absence of these people. The prohibition of eco-tourism has also led to a significant financial drop in the economy of these protected areas.
Some nations like China have asked the local authorities and native government bodies to increase the amount of disinfection routine, mainly to increase the dosage of chlorine in the wastewater treatment plant to prevent the spread of the COVID-19 virus. However, according to WHO (2020), no solid evidence has been found on the survival of the virus on the lifespan of the virus in wastewater as well as in drinking water. Despite the fact that excessive amounts of chlorine in water can cause harmful problems and issues associated with people’s health like bladder cancer and it also damages its cells.
The COVID-19 is a reminder that the health of the planet is also linked to the health of humans. Evidence proves that the virus is zoonotic, meaning it can be transmitted between people and animals. They are accountable for seventy-five percent of all emerging infectious diseases in the World. With the Virus infecting millions of people every day across the globe, Various governments and agencies’ top priority is to regulate the spread of the virus by shifting the spotlight on the management and treatment of wastes (especially the clinical and medical waste).  Likewise, at the same time, we as responsible individuals need to step up and follow the necessary guideline and precautions for the disposal of the waste and medical gears.
In Spite of various data showing that the pollutions have reduced significantly during the pandemic, History has witnessed a rise in pollution during any “post-financial crisis”.  A similar case was observed during the 2008 financial crash – although there was a temporary decrease in emissions of 1.3% was observed, but as the economy recovered in 2010, emissions were at an all-time high. After all, only through sheer mutual empathy and goodwill that the world will emerge stronger after this global pandemic. To prevent future outbreaks, we must address regularly the threats to the ecosystems and wildlife, including habitat loss, illegal trade, pollution, and climate change as human life depends on Earth’s life.
If you live near a spacious outdoor area, like the desert or an empty road lined with trees and you realize it’s the only safe, surface-less space to take a walk in, then you begin to realize the beauty of nature. The point is not to remain indoors, but to avoid being in close contact with others. When you do leave your home, whether it is for a walk in the desert or a run on your street, make sure to wipe down any surfaces you come into contact with, avoid touching your face, and frequently wash your hands.
Blackcoffer Insights 27: Saujanya Roy, Indian Maritime University, Kolkata
 


How Data Analytics and AI are used to halt the COVID-19 Pandemic?
Even though COVID-19 has not yet halted and we are facing the nth wave of the coronavirus outbreak across several countries, most notably the US, India, and Brazil. It is a fact that Data Analytics and AI are the big guns of our artillery in this fight against the COVID-19 pandemic. It has helped us in several stages of this outbreak, like the detection of its first outbreak, vaccine development and manufacturing contact tracing, and future hotspot detection. Some of these interesting applications are discussed in this article.
A lesser-known fact is that the COVID-19 outbreak was first detected in Toronto, Canada, nearly 7,230 miles away from the first outbreak, nine days before the WHO issued its warning. It was with the help of Big Data Analytics and AI, more specifically Deep Learnings (DL, a subset of Machine Learning) application in Natural Language Processing (NLP) to analyze text inputs that traced the surge of pneumonia cases in the Wuhan province of China. The specialty of DL algorithms is that they mimic the brain cells called neurons and can identify patterns in Big Data. This DL-backed software is used as inputs, reports from public health organizations, global airline ticketing data, etc. These were used to flag unusual surges and potential spreads of infectious diseases.
The next application of Big Data Analytics and AI was in the Research and Development of drugs to halt COVID-19. AI was used to analyze the protein structure of the virus, findings that were significant in the progress of vaccine development. In preliminary studies, it was found that it does not mutate as fast as other viruses such as HIV, which means that a prophylactic vaccine is a better way to proceed rather than a therapy. But there is also some evidence supporting the fact that when we find any kind of cure for it, there is a chance of the virus mutating, which is what happened and major mutations have been found in the UK, Brazil, and South Africa. AI also assisted scientists in rapidly shortlisting a set of already available vaccines that could be effective against the coronavirus.
Another interesting application of AI can be found in the selection of the right candidates, i.e. most likely to test positive for testing coronavirus in case of insufficient testing resources. This method was first exercised on Greek borders and was called project EVA. Whenever a traveler wanted to come into Greece, he had to fill out a form known as Passenger Locator Form (PLF) at least 24 hours prior to arrival, containing information on their origin country, demographics, point, and date of entry, and the intended destination. EVA then allocated testing resources according to the size of the set of passengers to be tested. After the test results, if found positive, they are put in quarantine. The results were sent back to the program for real-time learning.
The question remains how EVA made allocations, It was found that, statistically, only the origin country and the city were significant factors for screening. Ultimately, from a variety of countries and city pairs, EVA had to predict how many testing resources were to be allocated at each entry point and to particular passengers from a location is technically called the Multi-Armed Bandit (MAB) problem, and the chosen method to solve this problem was an AI algorithm called optimistic Gittins index. This algorithm identified on average 1.85x as many asymptomatic, infected travelers as random surveillance testing, and up to 2-4x as many during peak travel. After the test results, if found positive, they are put in quarantine. Following the collection of significant data through the aforementioned process, after a certain period, policies were made categorizing them separately and imposing restrictions on travelers from the specific location. This EVA as presented above was in operation from August 6th to November 1st processing around 38,500 PLFs each day and testing on an average 18.5% of households entering the country every day.
Above mentioned applications just show the tip of the iceberg and there is more to get into some of the other developments to watch for include the use of Image Recognition to identify covid based on x-ray images, the use of Deep Learning to predict the 3-D protein structure associated with COVID-19 and so on.
Blackcoffer Insights 27: Aniruddha Surse, NIT Nagpur


What is the difference between Artificial Intelligence, Machine Learning, Statistics, and Data Mining?
“Data is the new oil” has become the most important trendline of the 21st century. The reason for this is the advancements in the fields related to data analysis. The field of AI Machine Learning Statistics and Data Mining all deal with data and are developing at such a staggering pace, that these fields have become the most popular buzzwords these days.
Buzzwords are originated through technical terms but often the underlying essence is ignored through fashionable use and mainly used to impress. This is the main reason for the misconception amongst people. AI, ML, Stats, Data Mining, and many other fields related to the analysis of data are most often mistook for one and the same thing, thus all these words are often used interchangeably to convey one and the same thing. But this is not true at all!
The only similarity between these disciplines is that all of these disciplines are related to the analysis of the data and converting this data into “information”.
In academics, while learning AI Machine Learning Statistics and Data Mining, the academic approach only wander in the technical definitions and concepts but the underlying essence and the aim of the discipline remain unexplored, same is the case with most of the articles out there which try to explain the difference. Thus, becoming the primary cause of confusion between learners. Hence, this article explains the difference by explaining the philosophy and the aims of each of the disciplines rather than wandering in the technical definitions.
Then what is the difference between AI Machine Learning Statistics and Data Mining?
The essential difference between these disciplines lies in their “aims” and the approach taken to achieve those aims.
The aims of each of these fields are explained in detail in the following sections:
Artificial Intelligence:
The aim of Artificial intelligence is to understand intelligent entities. Then to satisfy this aim, we must first understand what is intelligence and what makes an agent intelligent agent? The answer to all these questions can be found by studying intelligent agents and the best example of an intelligent agent can be found by just standing in front of a mirror! Yes! Humans are the best examples of intelligent entities. Thus, in the 1900s, researchers began exploring the thought process, the reasoning process of humans as human beings were considered as an ideal intelligent agent. Mimicking human behavior became the aim of AI in the initial years of the research. After setting this goal, studies and experiments began and the most famous experiment conducted in the initial years to achieve this aim was the Turing Test! Turing defined intelligent behavior as the ability to achieve human-level performance in all cognitive tasks, sufficient to fool an interrogator. But this test received criticism as only mimicking human behavior is not exactly intelligence. Because intelligence should be related to the working of the human brain as without the human brain, intelligence has no meaning!
            Thus, mimicking human thought processes and reasoning became the transformed aim. The field of Psychology and philosophy also resonate with this aim that is to understand the human thought process. The difference is that AI not only tries to understand the thought process but to mimic it, build it. The collaboration of these fields resulted in models such as Neural Networks which try to mimic the function of neurons present in the human brain. So, basically, this initial aim was human-centered and humans were considered as the ideal intelligent agent.
Concurrently, the field of computer science was developing at a greater pace. With the advances in computer science, the experiments and theories could be easily tested and validated. As programs were being applied to solve real-life problems, it was found that computers performed better than humans at some tasks that are really complex for humans. One of the best examples of this could be the chess-playing program. An AI program defeated the world’s best chess player Garry Kasparov. This incident indicated that human intelligence is not the ultimate intelligence or else a human would have been able to defeat the AI program. This leads to a question, is human intelligence the ideal intelligence?
As computers became more advanced, they proved to be better than humans at certain complex tasks. That is why the new definition of intelligence was being related to the ability to solve cognitive tasks or problems. So, rather than considering the nature of agents, researchers began to study the nature of intelligence itself. Then the question comes how to test or validate intelligence? The best way to test intelligence is to solve cognitive problems. An agent can be said intelligent only if it can solve a complex problem. The problem-solving approach can be easily tested and validated on computers. Thus, some researchers began studying the ideal intelligence, and the selected agent to validate the experiments was the computer. So, a computer and problem-solving approach were adopted. So, the human-centered approach and computer, problem-solving approach are the two main aims of AI. Both of these fields have contributed to the field by giving valuable insights.
Both the aims are important and both of these collaboratively form the main aim of AI!
Machine Learning:
In the problem-solving approach, there is a big challenge that AI has to overcome in order to achieve its aim. Consider the example of solving a math problem. There are two cases by which intelligence can be tested in this problem-solving approach. Let us say two math problems are given for you to solve. The first problem is familiar to you and the second problem is not.
Consider the first problem. The first problem is familiar to you, that means you know how to solve such kind of problems as you have already solved some similar problems in the past. So, there comes a question, how our mind is able to solve that problem? The answer is, that you have solved similar problems in the past, thus you have learned from the past data, how to solve such problems, thus even if you haven’t seen that problem in the past, you will still be able to solve similar problems. This is one form of intelligence.
Consider another case where you are given a second problem where you have not solved such kind of problems in the past. Then to solve this problem, you will try to consciously gather and manipulate the given information so that you reach a certain conclusion. This kind of approach does not necessarily rely on the past data but completely on the reasoning process. This is the second kind of intelligence.
For AI to build intelligent agents, both of these kinds of intelligence must be developed in the agent. But, the reality is that AI has reached the point where it is able to build agents which can only learn from past data and find some useful information. AI today has not reached a point where it can build agents who can think on their own. That is the second type of intelligence.
So, the way AI is able to implement the first type of intelligence is through Machine Learning! So, the domain of AI which focuses solely on implementing the first kind of intelligence is in fact Machine Learning. That is the reason why ML is called the subset of AI! So, this is the main difference between AI and ML.
Technically speaking, “It is the field of study that gives computers the ability to learn from past data and find some meaningful conclusions, patterns without being explicitly programmed”. This statement needs some elaboration. The essence of ML is related to the process of “Generalization” and learning from past data. Generalization is an abstraction by which common properties of specific instances are formulated as general concepts or claims. Consider how we humans recognize daily life objects. If we see an animal, then we can easily recognize if it is a “dog” or a “cat”. It is a very trivial task for us but have you ever wondered how our mind is able to do it? The answer is Generalization!
If you were given a picture of a dog, you can easily recognize that it is a picture of a dog, because, our minds have abstracted the description of a dog and formulated it into a “concept” of what a dog is and these concepts became better and better as we learned from the past experiences of a dog. So, the way we think is dependent on the fact that things are represented as generalized concepts in our minds.
With generalization only, can come real “information”. So, we try to give computers the ability to generalize the “raw data” and convert it into “information” which can be patterns and trends in the data on their own.
Statistics:
This is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. Statistics tries to deal with data with the only aim that is to explain it. So, it is the study of explaining the data itself! Statistics has two main domains which are Descriptive statistics and Inferential statistics.
Descriptive statistics deals with the explanation or description of data thus the name “Descriptive” statistics. It tries to explain as much information as possible, easily about the whole large data which would be a very complex task otherwise.
 Inferential statistics try to make accurate inferences from the available small data. We use inference in many tasks in our daily lives. Consider a simple case of cooking a soup. After completing the recipe, you will taste a small sample, that is, a spoonful of the soup to check if the soup tastes good or bad. Depending upon the result of the sample, you make an inference about the whole soup that if the soup as a whole, good or bad. Similarly, in statistics, there are cases where you have to apply inference to have meaningful information. For example, consider a case where you cannot gather the whole data because it is very time-consuming and costly. In these cases, applying inference based on the available sample introduces uncertainty. That is where inferential statistics come for help.
So, the use of data in the context of uncertainty and decision-making in the face of uncertainty is what statistics deals with. So, however, and whatever the data, statistics tries to explain that data. This aim does not resonate with that of AI and ML, but statistics help these fields to correctly interpret the data!
Data Mining:  
“Data”, is not useful at all in its raw form. Consider examples of sensors used in industrial applications. These sensors might be used in a manufacturing plant to sense different properties like temperature, pressure, etc. The raw data generated by these sensors are not useful until and unless it is converted to a suitable form, then processed, analyzed to gather valuable insights, which can be used to solve a problem!
Due to its unique aim of capturing the essence of very large datasets, to gather insights, Data Mining is also referred to as “Knowledge Discovery”. That is why, Carly Fiorina, former CEO of Hewlett-Packard once said, “The goal is to turn data into information and information into Insight”. This statement completely explains the aim of Data Mining!
            So, the difference between AI Machine Learning Statistics and Data Mining lies in their aims. But the approaches taken in all of these fields, help in one way or the other in fulfilling the aims of the other fields. This is the beauty of these fields!
Blackcoffer Insights 25: Abhishek Govekar, Vishwakarma Institute of Technology, Pune, India.


How Python became the first choice for Data Science.
Data Science is gaining popularity exponentially over the past decade, and thanks to that we are now enjoying better products, recommendations, and smoother life. Data science is an interdisciplinary subject that includes statistics, math, IT, etc.
Now there is so much to do in Data Science,  so we need an arrangement where all this can be accessible in one place. It will be very hectic to go to hundreds of different resources while doing analysis or building models. But don’t worry, PYTHON is there for you.
Yes, you read it right, Python is a general programming language that can provide everything you need for Data Science. Several features that have made Python become the choice of data science in past times are:
1. Python is a progressively typed language, so the variables are defined automatically.
2. Python is more readable and uses lesser code to play out a similar task when contrasted with other programming languages.
3. Python is specifically typed. In this way, developers need to cast types manually.
4. Python is an interpreted language. This implies the program need not have complied.
5. Python is flexible, convenient, and can run on any platform effectively. It is adaptable and can be integrated with other third-party software effectively.
Now let’s see why Python become the choice of data science:
PANDAS
This library available in python makes it very easier to analyze the data, you can read a variety of data sets like CSV, XML, XLSX, JSON, etc. You can perform several operations like Groupby, sorting with the help of easily accessible objects from pandas.
NUMPY
This package helps you with any numerical operation that is needed to be performed in Data science, for example calculating Euclidean distance, finding ranks of the matrix, etc.
matplotlib AND SEABORN
These are excellent data visualization libraries available in python that produce some excellent visualization like shown here,
sklearn
It provides you state of the art machine learning algorithms for your accurate predictive analysis. Scikit–Learn is characterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering, and dimensionality reduction via a consistent interface in Python.
KERAS
Keras is an open-source software library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library. Keras is an industry-strength framework that can scale to large clusters of GPUs or an entire TPU pod.
See, Now that you have a variety of resources available here in python, then why go anywhere else. Python I also becoming the world’s most loved and most wanted programming language and it will surely help to get you the job. Data science consulting organizations are empowering their group of developers and data scientists to utilize Python as a programming language. Python has gotten well-known and the most significant programming language in an extremely brief timeframe. Data Scientists need to manage a large amount of data known as big data. With simple utilization and a huge arrangement of python libraries, Python has become a popular choice to deal with big data.
Seeing the stats we can clearly see that Python has taken over other Languages needed for data science. It has also surpassed R which is exclusively built for Data science. Isn’t this exciting.
Python in Data science has empowered data scientists to accomplish more in less time. Python is an adaptable programming language that can be effectively understood and is exceptionally amazing as well.
Python is highly adaptable and can work in any environment effectively. Additionally, with negligible changes, it can run on any operating system and can be integrated with other programming languages. These qualities have settled on Python as the top choice for developers & data scientists.
So next time you do analysis or work on any Data Science project, feel proud cause you are working with the most loved language of the world.
Blackcoffer Insights 25: Divyansh Bobade, LNCT (Bhopal)


How Google fit measure heart and respiratory rates using a phone camera?
It was in March that Tech giant Google came up with a ground-breaking announcement of Google Fit able to measure one’s heart and respiratory rates using their smartphones. This news spread like wildfire. Instantly became the talk of the town. This feature was said to be available to the Google Fit app exclusively to its Pixel phone users. Google also plans to expand to its other Android devices in the future.
This was after Google’s newest endeavor of acquiring the Fitbit for a whopping $2.1 billion. This acquisition not only does steps up a stage for a potential Google Smartwatch but also gives Google the ownerships to Fitbit’s health business and wealth of data assets. Users just need to place their head and upper torso in view of the front-facing phone camera for those who wish to measure their respiratory rate. For measuring Heart rate the user just has to place their finger on the rear-facing camera lens. Mind-blowing right!
Once the measurements have been taken the users simply have to store and save them in the Google Fit app to monitor and track their day-to-day wellness. On asked how it’s measuring these heart rate and respiratory rate, Google Health director of health technologies Shwethak Patel explained that these features rely on the sensors that have been built into the smartphone, such as its camera, microphone, and accelerometer. Thanks to increasingly power sensors even in affordable smartphones and advancements in computer vision, these features let use one smartphone’s camera to track even tiny physical signals like your chest movement to measure your respiratory rate and subtle changes in the color of your finger for your heart rate.
Pixel underwent and completed initial clinical trials to validate the algorithm cloud work in a variety of different world conditions and that too with many people while developing the features. Since our heart rate relies on approximating blood flow from color changes in someone’s fingertip, it has to account for factors such as lighting, skin tone, age. Adding to be able to measure heart and respiratory rate soon Google Fit also displays user daily stats such as daily goals, weekly goals, heart points, workout, and also sleep monitor.
Blackcoffer Insights 25: Sri Vishnu S, Kristu Jayanti College (Bangalore)


What is the future of mobile apps?
From what I have learned on the mobile apps development market recently, it is becoming harder for an Android or iOS developer to find a suitable and well-paying job in the market.
Some major reasons include the increasing supply of mobile engineers, people generally stop downloading apps, and many more.
In this article, I will go through a more detailed explanation of the recent trend mentioned above and my two cents on what mobile engineers could do to prepare themself for the upcoming challenge! and the future of mobile apps.
Coding Bootcamps Increase the Supply Side for mobile apps  Developers
Although the demand for mobile developers has not fallen sharply compared with previous years, the growth rate has slowed. On the other hand, owing to a great number of coding boot camps that target to bring up mobile and frontend engineers, a great number of developers are flooding into the developer job market.
The supply side for mobile engineers has increased rapidly in recent years, raising the standards for qualified mobile engineers. The days of a mobile engineer easily getting over 10 job offers are over.
Recode ran an article in mid-2016, that begins: “The mobile app boom kicked off in July 2008, when Apple introduced the App Store. Now it is over.”
According to data gathered and analyzed by the CakeResume team, JavaScript and Python are now the most in-demand programming languages for companies who post product development jobs on CakeResume, both together taking up almost half of the job opportunities, while mobile engineers taking up merely 10% of the job opportunities.
learn more about tech salaries in Taiwan
Tech Salaries in Taiwan 2019
Overview
People Stop Downloading mobile apps
While App usage continues to grow and revenue also ascends, the majority of consumers actually download zero apps per month
let’s face the truth, born and raised up in the era of the information explosion, individuals don’t have the leisure to check on what’s new on the app store every day. When it comes to choosing an App to download, many people feel overwhelmed by the sheer number of options available.
Take the productivity category of the App Store as an example, there are over thousands of Apps in this category, and new Apps popping up almost every week. It is almost impossible for users to dig through and learn all the apps. They just choose what is on top then click Install.
And not to mention that if you could build an awesome productivity app, Google and Apple could do it 10 times better and faster than you. In the end, there’s not as much need for individual apps to accomplish similar convenience factors.
What You Could Do to Prepare for The future Changes for mobile apps
Although the bull run for mobile development has ended, there are still over 2 billion smartphone users, 27.5 billion mobile apps downloaded, and time spent per day on mobile devices has increased rapidly in recent years. Moreover, apps offer a user experience that even ‘Responsive Websites’ are unable to provide.
In order to stay on top of the game, you will have to differentiate yourself from the pack — other mobile engineers. If you are a mobile engineer who only knows how to code and tweak your App’s UI, you aren’t that different from others after all. Below I listed a few ways that you could implement so that you could stay competitive in the job market.
1. Choose Newer Technologies When You Have the Chance
If you are an Android developer, you could start choosing a job involving Kotlin or Flutter, which are backed by Google. If you are an iOS developer, try to look for a job to get involved with using Swift.
Although older mobile coding languages, such as Java and Objective-C, still have their upsides, the main reason why Kotlin and Swift are created at first is to address the older languages issue. It means that Kotlin and swift provide many safety mechanisms available out-of-the-box while being more concise and expressive than Java and Objective-C at the same time.
Kotlin vs. Java: Which One You Should Choose for Your Next Android App
A detailed comparison of Java and Kotlin to help you decide which language will work best for your next mobile…
  Objective-C vs Swift in 2019
The article covers a brief comparison between Objective-C and Swift in 2019, with ABI Stability.
2. Build Up Your Domain Knowledge and Industry Skills
This point is especially crucial. Each industry has its specific skills to learn and refine. For example, if you are building a stock, foreign exchange, future, or options market app, you will have to understand the WebSocket protocol, which lets you transfer as much data as you like without incurring the overhead associated with traditional HTTP requests.
For the streaming media industry, possessing experience with live streaming protocols such as HLS, RTMP, WebRTC will be a must to deal with streaming-related apps.
These industry skills can really make you stand out, and adds another layer of expertise to your already pretty impressive mobile engineer title, making you more valuable in the job market.
3. Follow the Irresistible Tech Trend
The future of mobile app development will be shaped by how businesses harness mobile technology to solve people’s everyday problems.
Don’t try to fight the irresistible trend, see how you can surf the trend! Exposing yourself or trying to get a job in the field mentioned below will absolutely give you more opportunities as a mobile developer.
So what are the trends for mobile apps development?
IoT (Internet of Things)
“The driving force behind the growing mobility market is the impact of the IoT and its broad reach,” writes the experts at Maryville University online. In the future, apps will need to speak to each other, in the same way, that devices in the IoT communicate. The market needs companies that can develop custom IoT applications — sensors and devices, web apps, and both B2B and B2C mobile end-user apps.
AI (Artificial Intelligence)
According to experts at Maryville University, AI is everywhere, from predictive analytics algorithms used by retailers like Target and Amazon to anticipate shopping needs to fraud detection monitors from banks and credit cards. Some more examples: Tinder uses machine learning to increase a user’s chances to find a match. Google Maps makes the process of choosing a parking spot easier.
AR (Augmented Reality) and VR (Virtual Reality)
Tech Giants such as Apple and Google have launched their own augmented reality kits — AR Kit and AR Core resp to help app developers to create high-quality mobile apps. As for VR, the latest buzzword associated with it is v-commerce. One of the examples is Alibaba, which in November of 2016 introduced VR shopping to its customers across China.
Blackcoffer Insights 24: Nikhil Singh, St.peter's college agra


Impact of AI in health and medicine
AI allows those in training to go through naturalistic simulations in a way that simple computer-driven algorithms cannot. The advent of natural speech and the ability of an AI  computer to draw instantly on a large database of scenarios means the response to questions, decisions, or advice from a trainee can challenge in a way that humans cannot in health and medicine
Health monitoring:
Wearable health trackers-like those from FitBit, Apple, Garmin, and others- monitor health rate and activity levels. They can send alerts to the user to get more exercise and can share this information with doctors.
Technology applications and apps encourage healthier behavior in individuals and help with the proactive management of a healthy lifestyle.
AI increases the ability for healthcare professionals to better understand the day-to-day patterns and needs of the people they care, for better feedback, guidance, and support.
Medical imaging:
Machine learning algorithms can process unimaginable amounts of information in the blink of an eye and provide more precision than humans in spotting even the smallest detail in medical imaging.  A few of them are Blackford, Zebra, Enlitic, Lunit.
The company “Zebra Medical Vision” developed a new platform called profound, which analyzes all types of medical imaging reports that are able to find every sign of potential conditions such as osteoporosis, aortic aneurysms, and many more with a 90% accuracy rate.
Digital consultation:
For eg, the digital health firm #HealthTap developed “Dr. AI”, and apps like Babylon in the UK use AI to give medical consultations based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illness and asks patients to specify symptoms to triage whether they should go to the ED, urgent care, or a primary care doctor.
AI robot-assisted surgery: Robots have been used in medicine for more than 30 years. Surgical robots can either aid a human surgeon or execute operations by themselves. They’re also used in hospitals and labs for repetitive tasks, in rehabilitation, physical therapy, and in support of those with long-term conditions.     
 Health chatbots such as Babylon, Ada, and mostly close-ended communications.
Machine learning is a type of AI that allows computers to make predictions without being explicitly
Provides formal conceptual Framework for input processing and decision making in diagnosis and management
Objective decision making with less variance
High speed and efficiency
Unlock the power of big data and gain insight into patients.
Support evidence-based decision-making, improving quality, safety, and efficiency.
Coordination care and faster communication
Improve patient experiences and outcomes
Deliver value and reduce costs
Improve health system performance and optimization
Examples of AI in healthcare and medicine
#MICROSOFT: Predictive analysis in vision care
#GOOGLE: clinical decision support in breast cancer diagnosis
#IBM WATSON: precision medicine in population health management
Actionable medical insights:  An ever-increasing amount of medical data are being digitized at all public and private healthcare institutions. However, by its very nature, this kind of data is messy and unstructured. Unlike other types of business data, where traditional statistical methods can be used for quick insights, patient data is not particularly amenable to simple modeling and analytics tools.
 For eg.- Enlitic, a San Francisco-based start-up, has a mission of mixing intelligence with empathy and leverage the power of AI in health and medicine for precisely generating.
Therefore, a massive parallel effort to rationalize the legal and policy-making is needed to bring the full benefit of advancement in AI technologies into the healthcare space. As technologies and AI/ML enthusiasts, we can only hope for such a bright future where the power of this intelligence.
Blackcoffer Insights 24: Shaffy Garg, Baba Farid college of management and technology 
 


What patients like and dislike about telemedicine?
In today’s world, telemedicine technology is one of those technologies which has brought about a change. Compared to the early days there have been remarkable differences in the methods of consultation with a doctor. In the years that have passed by, consultation for a disease with a doctor was quite hectic. It involved waiting, traveling, etc. But with the advent of telemedicine opportunities, this has completely changed.
It is a rural area that has been completely blessed with the invention of telemedicine. Today a considerable amount of people are able to consult doctors remotely. Not just doctors, but specialists in various fields of medicine. This has been of great importance as far as rural people are concerned. There are a lot of telemedicine tools that have been found. There are a lot of areas like ophthalmology, oncology, dermatology, etc where the facility of telemedicine has been practiced.
Most of the patients are truly benefitting from telemedicine. Patients are pretty satisfied with the consultation they are getting. They don’t have to travel now. They can consult doctors and other specialists from remote areas. The cost of consultation has become pretty much affordable. Moreover, they get exposed to highly efficient and qualified experts in the field of medicine.
On the other hand, there can be patients who do not fully get satisfied through a virtual consultation. Rather they might feel that they need to have a direct talk with the doctor, which can boost up their confidence and also it helps to maintain a better relationship with the doctor and patient. The patients feel trust when they talk to doctors face to face. Moreover, doctors also might be able to console their patients when they have direct interaction with their patients. There could also be patients who doubt if these virtual methodologies are really trusted worthy or not. It is so because, while the patients have direct contact with their patients, the qualifications are visible to the patients. So there shall be no question of distrust.
There are major challenges like better connectivity of the internet, without which the patients will not be able to have continuous interaction with the doctors. Technical glitches may hinder the consultation too. Moreover, emergency situations cannot be addressed beyond a limit through telemedicine.
Blackcoffer Insights 23: Miriam sam, Scms school of technology and management.


How we forecast future technologies?
TECHNOLOGY is playing a dominant role in human life. And as in our daily lifestyle technology is used each and every place where human beings are present. so How we forecast future technologies? 
So, the fact is that technology is not a single immutable piece of hardware orbit chemistry. It is simply knowledge of physical relationships and it systematically applied to the useful arts. This knowledge can vary continuously over time. It can range basic phenomenon can be applied to an end product, device, or production machine in a mature operating system. Even as time passes, the performance characteristics of any machine, product, or operating system are normally improved in small continuous increments over time.
Advance in technology is usually nothing more than an accumulation of small advances not worth introducing individually to make a significant change in total technology. Forecast future technologies no matter how accurate – unless they eventually influence action.
Blackcoffer Insights 23: Arvind Pal, PACIFIC SCHOOL OF ENGINEERING, SURAT


Can robots tackle late-life loneliness?
In this era, everyone is busy in his life. No one spares time for someone. so robots tackle life loneliness? We have no time to stand and stare. This causes loneliness. Loneliness is due to fast-moving life and hurry and worry. In the 21st century, everyone is spending most of their time earning money. They are day-by-day transforming into money-earning machines. Due to his perception, we are losing our relations as we can’t give enough time to our families. They are busy earning their livelihood. They never get satisfies and always want more. Everyone is in a race of earning and then sends his children far away from himself to make them well settled in order to maintain their status. But one faces the consequences of this kind of lifestyle in his old age. People usually don’t spend time with their children when their children are in their childhood but they yearn for the company of their children in their old age. They feel lonely but have none to talk with. They are still attached to people, places, belongings, and memorable events from the past, although they understand that life cannot continue the same way as earlier, and this may easily result in feelings of loneliness and social isolation. As they become older, they need to be attended to in order to meet their daily needs. Robots can indeed serve the purpose but they can aid only mechanical care. They are just emotionless, feeling fewer entities. Robots aren’t creative. They can work only according to a pre-programmed system and till now no such software has been created that can transfer human feelings into robots. This is the major loophole of using robots to tackle late-life loneliness. They can undoubtedly serve their masters but their masters can’t share their feelings with them as they are incapable to understand or react to them. They can’t change their activity according to the occasion. They can provide anything except emotional satisfaction and without emotional gratification, loneliness can’t be tackled. A master can be attached to his robot but that attachment is only due to their service. They can’t share their thoughts with them. In this era, we are rendered lonely and in late- life feel the need for a companion with whom we can talk. We feel a need for someone who can listen to us, react, and advise us to tackle our day-to-day life problems. But we can’t find one. We have, undoubtedly, achieved great progress in terms of technology but we can never fill the void in the heart of a person struggling with his late-life loneliness with our advancements. We are in a kind of mental trance in which we experience fame, progress, wealth, etc. but when we gain our consciousness, we find ourselves lonely in this world. Relations in life are actual wealth in a person’s life. We can never enjoy such a bond with a robot. It will follow our commands, take our appropriate care but can’t react to our emotions. It can’t console us. When a man lives in loneliness for a long time, it eats up his conscience and transforms him into a machine. Robots can never become our friends, crack jokes, weep our tears, or establish an emotional connection with us. They can’t understand our feelings. Many people go into depression. Depression or the occurrence of depressive symptoms is a prominent condition amongst older people, with a significant impact on their well-being and quality of life. They remain sick and loneliness directly impacts longevity. Lonely people often think that they are no longer needed in this world and thus they want to die. It impacts their mental, psychological, social, and physical health. Robots prove to be useless in these matters. They can provide motivation, only if they have the software to do so but can’t, themselves, react to such a situation. They don’t know about anger, happiness, or sadness. They don’t themselves bring food when their master is hungry until commanded to do so. They can’t even offer a glass of water themselves. A man living without relations and without fellow feeling no longer remains a human being. He becomes none less than a machine as, due to his loneliness, he becomes mentally ill and goes into a condition like trauma where he no longer enjoys nature’s blessings, becomes happy or sad as he loses the ability to react and hence can’t act. It’s a dangerous situation. We face a plethora of challenges in assisted living facilities such as not being addressed to emotional needs, being neglected, and forcing a withdrawal from social activities. Relations in life, interaction with fellow beings, sharing of joy and happiness, and fellow feeling make a man from mere a ‘being’ to a ‘social being’ and finally a human being.
Blackcoffer Insights 22: Ishatpreet Singh, IISER (Mohali)


Embedding care robots into society and practice: Socio-technical considerations
Care Robots, as the name suggests, are robots that are used for hospitality purposes like fetching water, cracking jokes and keeping a patient in good harmony, etc.
The senior care industry has been at the forefront for quite a period. The reason being, Nuclear families becoming very busy in their schedule that one day when the parents of this nuclear family are old, children are nowhere to take care of. Instead, nursing homes have become a trend.  Parents with mental illness are being put in rehab.
As observed for a few decades robots are taking care of many activities and they are even dominating a few fields like the automobile industry. So, Machines can indeed help humankind in achieving remarkable success in many fields.
Do senior citizens love machines ?, You should look at them when they watch TV when they love to play video games on a smartphone. So, If we give them a human-like structure with the ability of a smartphone to make these people happy then it would be quite a monstrous feat.
Blackcoffer Insights 22:Sadhana Kanaparthi,  KIAMS


Management challenges for future digitalization of healthcare services
Management acts as a guide to a group of people working in the organization and coordinating their efforts, towards the attainment of the common objective.Management challenges for future digitalization of healthcare services
CHARACTERISTICS for future digitalization of healthcare
Universal: All organizations, whether it is profit-making or not, require management, for managing their activities. Hence it is universal in nature.
Goal-Oriented: Every organization is set up with a predetermined objective and management helps in reaching those goals timely, and smoothly.
Continuous Process: It is an ongoing process that tends to persist as long as the organization exists. It is required in every sphere of the organization whether it is production, human resource, finance, or marketing.
Multi-dimensional: Management is not confined to the administration of people only, but it also manages work, processes, and operations, which makes it a multi-disciplinary activity.
Group activity: An organization consists of various members who have different needs, expectations, and beliefs. Every person joins the organization with a different motive, but after becoming a part of the organization they work for achieving the same goal. It requires supervision, teamwork, and coordination, and in this way, management comes into the picture.
LEVELS OF MANAGEMENT
Top-Level Management: This is the highest level in the organizational hierarchy, which includes the Board of Directors and Chief Executives. They are responsible for defining the objectives, formulating plans, strategies, and policies.
Middle-Level Management: It is the second and most important level in the corporate ladder, as it creates a link between the top and lower-level management. It includes departmental and division heads and managers who are responsible for implementing and controlling plans and strategies which are formulated by the top executives.
Lower Level Management: Otherwise called functional or operational level management. It includes first-line managers, foremen, supervisors. As lower-level management directly interacts with the workers, it plays a crucial role in the organization because it helps in reducing wastage and idle time of the workers, improving the quality and quantity of output.
future digitalization of healthcare
    In the current day and age, a wave of digitization has taken over the world. All the emerging technologies like Artificial Intelligence and others are helping people live better and easier life. The service sector has also benefited a lot from digitization. It got further boost when Prime Minister Narendra Modi, in the year 2015, launched a campaign known as ‘Digital India.’ Among the various industries in the service sector, digitization has had a massive impact on the operation of the healthcare and diagnostic industry. It has helped in the development of this industry and enhances life for millions of people. The condition of India in terms of healthcare has been quite grim. Patients who need attention are neglected and are unable to avail proper treatment or diagnosis. However, with the coming up of digitization, there has been hoped for this industry also.
According to Dr. Keshab Panda, CEO & MD, L&T Technology Services,
there are three trends emerging in the healthcare and diagnostics ecosystem as a result of digitization.
1. Value-based healthcare
This is a model of healthcare where doctors and hospitals are paid based on patient health outcomes. The coming up of digital tools in this segment of healthcare can be considered as the starting tool. Dr. Panda further says that the implementation of advanced digital technologies in patient-care domains like- mobile health apps, telehealth, wearables, and remote monitoring can help in enhancing accessibility, boost efficiency and augment the effectiveness of treatment and preventive care.
2. New product development
The healthcare industry over the past few decades has changed drastically. With the coming of emerging technologies, new surgical procedures and medical devices have been made available.
3. Connectivity
While talking about connectivity in healthcare, Dr. Panda said that with digitization taking over the industry, it is only a matter of time when with the help of the internet and smartphones, digital healthcare will be everywhere. The internet for once has helped doctors connect to their patients and also with one another. This has helped in enhancing the doctor-patient engagement by increasing their interaction time.
Big data aggregates information about a business through formats such as social media, eCommerce, online transactions, and financial transactions, and identifies patterns and trends for future use.
For the healthcare industry, big data can provide several important benefits, including:
Lower rate of medication errors – through patient record analysis, the software can flag any inconsistencies between a patient’s health and drug prescriptions, alerting health professionals and patients when there is a potential risk of a medication error.
Facilitating Preventive Care – a high volume of people stepping into emergency rooms are recurring patients also called “frequent flyers.” They can account for up to 28% of visits. Big data analysis could identify these people and create preventive plans to keep them from returning.
More Accurate Staffing – big data’s predictive analysis could help hospitals and clinics estimate future admission rates, which helps these facilities allocate the proper staff to deal with patients. This saves money and reduces emergency room wait times when a facility is understaffed.
Challenges  for future digitalization of healthcare
 1. Cybersecurity
Although ransomware, data breaches, and other cybersecurity concerns are nothing new to the healthcare industry, the 2020 Covid-19 pandemic revealed just how vulnerable sensitive patient health information really is.
The recent growth of digital health initiatives- like telehealth doctor visits — is a major contributor to the severe increase in breached patient records. As more healthcare functions continue to move online over the next year, it’s extremely important to ensure these processes are protected from outside threats.
2. Invoicing and Payment Processing:
Medical practices are citing patient collections as their top revenue cycle management struggle as patients are becoming responsible for a larger portion of their medical bills. In order to help encourage patients to submit payments in a timely manner, providers must adhere to patient payment preferences.
To meet patient expectations and improve the user experience, ensure billing statements are patient-friendly. You should offer paperless statements and a variety of payment options (e.g. credit card, etc.) via an online patient portal and utilize the latest payment technologies, such as mobile and text-to-pay. New features like text or email reminders help effectively communicate with patients and encourage them to pay their financial obligations.
3. Patient Experience
The medical insurance landscape has experienced some significant changes in recent years. As more patients are responsible for a larger portion of their healthcare bill, they naturally demand better services from their providers.
Healthcare organizations will face tougher competition in attracting and retaining patients who demand an experience that matches the level of customer service they expect from other consumer brands.
They demand a streamlined patient experience so they can “self-service” to resolve most questions, issues, or concerns (e.g., downloading an immunization record, booking an appointment, paying their bills, or checking their account/insurance status) whenever, wherever, and however is most convenient for them.
Blackcoffer Insights 22: PRIYAM VERMA AND RIYA MALIK


Are we any closer to preventing a nuclear holocaust?
One thought always comes to my mind…
What if we lived without that dander hanging over our heads?
And you all know what is that danger is. For more than 70 years the world has faced the very real threat of nuclear war.
Nuclear war is a real and growing threat. The United States and Russia have left critical agreements and treaties, while actively planning to add new types of weapons to their arsenals.
Meanwhile, US nuclear policy remains rooted in the Cold War, increasing the risk that nuclear weapons could be used again.
It doesn’t have to be this way. With the right policy changes and a commitment to diplomacy, the United States can be a leader in reducing the nuclear threat and we can also help them. But the question arises is what are the activities are being done for the prevention of nuclear holocaust?
And they are :
Pressuring Congress to support change,
Holding the White House accountable through independent research and analysis,
Increasing public demand for changing nuclear weapon policies.
Now the question is what can we do to prevent a nuclear holocaust?
Tell government: Don’t fund a nuclear arms race.
Call for investment in public health and public security, not nuclear weapons.
Tell government: The United States should never start a nuclear war.
Urge presidential candidates to make preventing nuclear war a priority. ACTIVIST RESOURCES
nuclear weapons are extremely strong to show the power of a nation. But no one knows the drawback of this attack between the two countries. For example Hiroshima and Nagasaki, the realistic bomb blast explained the blood of each people. The crux of my presentation is that the circumstances surrounding nuclear war call for a new level of commitment by the scientific community to reduce the risk of nuclear war. To generate new options for decreasing the risk, we need analytical work by people who know the weaponry and its military uses. But that is far from enough to do the job. We also need scholars who know the superpowers in-depth; people who know other nuclear powers; people who know third-world flashpoints; people who know international relations very broadly; people who know a lot about policy formation and implementation, especially in the superpowers; people who understand human behavior under stress, especially leadership under stress; people who understand negotiation and conflict resolution people and much more. In other words, the relevant knowledge and skills cut right across all the physical, biological, behavioral, social. International Physicians for the Prevention of Nuclear War, World Health, British Medical Association, and the American Medical Association; others have contributed as well. The central point is that a kind of awakening has occurred in the medical community to the responsibility of addressing the immense nature of the threat to public health. At last, I want to conclude that: “ In a nuclear war, except evil force, no one is the winner. Science and humanity become the villain. Everyone knows that but the gamblers want to play their cards. Be aware of the nuclear gamblers “.
I hope that soon the world will be full of love and affection and there will be no place for hate and the wars will be held.
Blackcoffer Insights 21: Rutika Pagar, C.M.C.S College Nashik


Will technology eliminate the need for animal testing in drug development?
Imagine yourself as a being who can speak but no one can comprehend you. You are kept in captivity, in a place where you can hardly breathe. You are finally selected and taken out of that place, it is a happy moment for you. You can finally have your freedom but sadly instead of being free, you are prodded by rods and fed toxic substances against your will. The only thing you can do is scream in vain. Your voice is heard by everyone but it gets lost in the noise of scientific advancement. You writhe in pain as the substances take effect on your body, the pain slowly creeps into your body and you slowly succumb to it, never to wake up again.  
Since the time of Aristotle, animals have been subjected to various inhumane experiments for the sake of knowledge. These experiments though unethical were very necessary for the advancement of science. Animal testing has become an important part of experimenting on living organisms to ensure that a product is safe for humans to consume. The lab animals are generally subjected to high levels of toxicity while being kept in isolation due to which they have to suffer a lot of stress and discomfort. Animal testing has a large number of drawbacks:
Animals and human beings react differently to certain substances. For example, in 2004, Vioxx withdrew “rofexocib” after more than 80,000 people suffered heart attacks after taking the drug. Although the drug showed no such effects during animal testing, it posed threat to human beings. 
It is very costly to conduct animal testing as animals need to be provided with shelter and food over a large period until the trials are completed. 
Animal testing might not lead to a scientific breakthrough. There are various instances of animal testing failing to achieve the desired result which ultimately leads to loss of time, resources and money. 
Some products like cosmetics may not be essential to humans and therefore don’t require animal testing at all. The cosmetic industry is notorious for animal testing of products and it’s a pity that some of those products aren’t even launched in the market.
All of these things make us question ourselves. Is it even worth it? Is it possible to stop this cruelty without hampering our growth?
With the advancement in science, it can be made possible that we don’t have to use animals ever again.  
Advanced computing– With the availability of various software programs it has become easier to understand the working of a biological organism. We can replicate the entire human body and conduct various tests to get generate simulations to assess the possible effects of a chemical or biological reaction. For example, CADD (Computer-aided drug design) is becoming popular for stimulating whether a particular drug affects the cells or not. This cuts down the requirement of using animals for testing and also reduces the experimentation time as the researchers don’t have to wait for the animals to show response. 
Tissue Culture – Growing tissues in Petri dishes and in vitro cells can be a viable alternative to animal testing. Tissues and cells from brain, liver, kidney etc. is taken from an animal and kept in a suitable growth medium for a few weeks or even for a year. These cultures can be used to test the toxicity and efficacy of substances. For example: In eye irritancy tests, new animals have to be used in each experiment to check if the substance is irritating the eye or not. Many animals permanently lose their eyesight and using a new animal every time is just simply inhumane. Using bovine corneal organ culture in which the corneal tissue is cultured for three weeks before using it yields the same result without any complications. Scientists have started to use this method to test for toxicity and efficacy in cosmetics, drugs and chemical sectors. 
3-D printing– Making complex tissue structures from a single digital file is astonishing. Printing tissues can help in carrying out experiments on it and getting results similar to the real tissues. For example Earlier animal organs, charts and 3-D models were used to explain the functioning of a particular organ to medical students but now bio-printing companies have started making human tissues and organs which are used by students in various medical schools to understand the intricacies of human organs which has helped in bringing down animal experimentation to a large extent.
These bio-printed models can also be used by the researchers to conduct experiments on them without using animals for it. A French company is working on a bio-printed liver model to test liver toxicity without having to use animals at all. This is a breakthrough in the field of bio-technology as it can bring down animal experimentation by a large percentage.   
Organ-on-a-chip method – It is a fairly new technology which consists of growing cells inside chips where they imitate the structure and behavior of human organs. Researchers can test drugs on these chips and it is far cheaper and humane than testing on animals. For example, UK Company CN Bio has come with a system which consists of 10 organs on a single chip and they have tissues ranging from the liver to the brain.
Micro dosing: Humans can participate in these experiments in which they are injected with small one time doses and are constantly monitored under imaging techniques to gather information on the changes happening in the body. These tests can replace certain tests on animals and help in removing drug compounds that don’t have the desired effect on the human body. For example, COVID-19 tests were carried on human volunteers in which they were given small doses of the vaccine and kept under constant surveillance. This helped the researchers in removing the drugs that were not having the desired effect on the human body and thus they were also able to remove animal testing as one of the processes.
These alternatives can help in bringing down animal experimentation to a large extent. If science can progress without harming animals then we should go for it rather than putting them through excruciating pain. We must protect the animals as much as we can or else we would soon be living in a world without them. 
Blackcoffer Insights 21: Eeshan Singh and Sowmya K P, ICFAI Business School, Hyderabad


Will we ever understand the nature of consciousness?
Introduction
The definition of consciousness has been controversial for centuries, hence it is given the title of the ‘most familiar and yet mysterious aspects of our lives’. An idea of this concept would be an awareness in beings of their surroundings, themselves, and their own perception. The reason this part of our mind remains unascertained is that consciousness isn’t observable, unlike brain matter that is studied scientifically. the physical clarification of awareness is in a general sense incomplete: it doesn’t include what it feels to be the subject, for the subject. There likewise is by all accounts an unbridgeable illustrative gap between the physical world and our consciousness. As suggested by an incident of Eastern and Shamir traditions, consciousness is both universal and primal. Whilst we have made tremendous progress in understanding brain activity over the years, this research hasn’t been able to answer all the questions relating to the nature of emotions and experiences.
History
Beginning within the late nineteenth century, this was a time that once had psychological queries driven by a philosophical understanding of the mind, which was typically equated with consciousness. As a result, the analysis of brain and behavior naturally thought of the role of consciousness in behavioral management by the brain.
The Ancient Mayans were among the first to propose a sorted out feeling of each degree of consciousness, its purpose, and its worldly association with mankind. Since consciousness incorporates stimuli from nature as well as interior stimuli, the Mayans trusted it to be the most essential type of existence, equipped for evolution. The Incas, however, thought about consciousness as a movement of mindfulness as well as of worry for others too.
John Locke, an early philosopher, said that consciousness, and so individuality, are freelanced of all substances. He also detected that there is no reason to believe that consciousness is stuck to any specific body or mind, or that consciousness cannot be transferred from one body or mind to a different one. Karl Marx, another early thinker, denies the mind-body classification and holds that consciousness is jeopardized by the material eventualities of one’s settings. William James, an American psychologist, differentiated consciousness to a stream – unbroken and continuous despite several changes and shifts.
While the main center of a lot of the analysis moved to strictly note cable behaviors throughout the primary half of the twentieth century, analysis of human consciousness has grown staggeringly after the 1950s.
In Sigmund Freud’s psychoanalytic theory, we can see that he believed that all three levels of awareness- preconscious, conscious, and unconscious were responsible for one’s behavior and thinking. He believed that the mid itself was divided into three parts- the id, the ego, and the superego. The Id is present at birth, instinctual, and operates according to the pleasure principle. The ego underseals reality and logic and develops out of the id in infancy. Finally, the superego is an internalization of society’s moral standards and responsible for guilt. Now, the Id is regarded as unconscious, whereas the ego and superego are also conscious and preconscious. Freud constantly revised his own clinical qualities researches, however, and didn’t conduct scientific experiments and hence his work is heavily scrutinized, leaving the questions unanswered.  
Sigmund Freud’s theory differed from the other psychologists since his theories were more understandable and very easily conveyed to the people. Sigmund Freud’s work and hypotheses helped people shape their perspectives on youth, character, memory, sexuality, and therapy. However, his theories were subject to considerable criticism both now and during his own life. Whilst John Locke and William James took a more practical approach to the mystery by conducting experiments, Sigmund Freud didn’t provide any evidence to support his claims.
Brain
Today, the essential focal point of consciousness research is on understanding what consciousness implies both biologically and mentally. Issues of interest include phenomena such as perception, blindsight, brainwaves during sleep, and altered states of consciousness produced by psychoactive drugs.
A greater part of the test assesses consciousness by approaching human subjects for a verbal report of their encounters. However, to confirm the criticalness of these verbal reports, researchers must contrast them with the action that all the while happens in the brain —that is, they should search for the neural connections of consciousness.
Hope is to locate that noticeable action in a specific aspect of the brain, or a particular pattern of global brain activity, will be greatly predictive of consciousness mindfulness. A few brain imaging strategies, for example, EEG and MRI scans, have been utilized for physical proportions of brain activities in these examinations.
A few investigations have shown that movement in essential primary sensory areas of the brain isn’t adequate to create consciousness: it is workable for subjects to report an absence of awareness in any event, when areas, for example, the primary visual cortex show clear electrical reactions to a stimulus. Higher brain areas are viewed as all the more encouraging, particularly the prefrontal cortex, which is involved in a range of high order functions.
One mainstream theory implicates various examples of brain waves in creating various conditions of consciousness. Analysts can record mind waves, or drawings of electrical movement inside the cerebrum, using an electroencephalograph (EEG) and placing electrodes on the scalp. The four types of brain waves (alpha, beta, theta, and delta) each correspond with one mental state (relaxed, alert, lightly asleep, and deeply asleep)
Memory
Episodic memory can be regarded as the only form of conscious memory. This is because it is the capacity to consciously remember personally experienced events and situations in the past. The hippocampus located in the brain’s temporal lobe is responsible for this type of memory.
Consciousness also plays a part in important memory distinctions. One such distinction is the implicit and explicit characteristics; in which explicit memory is what you consciously know and implicit memory includes events you may not be conscious of.
Furthermore, several empirical findings suggest that declarative memory is related to consciousness as well; meaning that the retrieval and formation of this memory are connected to awareness. Working memory operates/maintains consciously perceived information as well since it temporarily stores and tampers with information whilst working on tasks.
The current ways of testing this information are lacking in several essential aspects, including spatial resolution, temporal resolution, or scope. Examples of such methods are PET, fMRI, EEG, implanted electrodes, etc.
PET and fMRI have temporal resolution problems, EEG is well-known to have localizability difficulties, and implanted electrodes whilst great in temporal and spatial resolution can only test a set number of neurons; that is, they are restricted in scope. Hence, huge numbers of our speculations, while testable on a fundamental level, appear to be difficult to test as of now.
Mental illness
Consciousness has an influence on the way we see objects around us, which encourages us to settle on choices about how to communicate with them. Experiencing difficulty perceiving objects is connected to a few problems, for example, agnosia (a failure to decipher visual data), Alzheimer’s disease, and autism. However, we actually don’t comprehend what visual data is basic for the mind to intentionally perceive an object.
Several different disorders of consciousness include locked-in syndrome, minimally conscious state, persistent vegetative state, chronic coma, and brain death.
Locked-in syndrome, otherwise called pseudo coma, is a condition wherein a patient is aware however can’t move or impart verbally because of complete loss of motion of essentially all voluntary muscles in the body aside from vertical eye developments and squinting. The individual is conscious and is able to speak with eye movements.
In a minimally conscious state, the patient has intermittent periods of awareness and wakefulness. Patients need to give restricted, however reproducible indications of consciousness of themself or their current circumstance. This could be following straightforward orders, comprehensible speech, or purposeful conduct.
In a persistent vegetative state, the patient has sleep-wake cycles, but lacks awareness, is not able to communicate, and only displays reflexive and non-purposeful behavior. The term refers to an organic body that is able to grow and develop devoid of intellectual activity or social intercourse
Like coma, chronic coma results generally from cortical or white-matter harm after neuronal or axonal injury, or from central brainstem sores. Usually, the metabolism in the grey matter decreases to 50-70% of the normal range. The patient lies with eyes shut and doesn’t know about self or environmental factors.
Brain death is the irreversible end of all brain activity, and function (including involuntary activity necessary to sustain life). The main cause is total necrosis of the cerebral neurons following loss of brain oxygenation. After brain death the patient lacks any sense of awareness; sleep-wake cycles or behavior, and typically look as if they are dead or are in a deep sleep state.
Future predictions and conclusion
As we can see from the history of scholar’s endeavors to study consciousness empirically, its nature is not one that can be defined using scientific methods. In the past, psychologists such as Descartes came up with dualistic theories that did not line up with the fundamental laws of physics. In the battle between realists and illusionists, taking sides is fundamentally impossible as the topic doesn’t allow for concrete evidence.
As we can observe from neural examinations of the brain to detect the causes of consciousness, there is no sensory sector of the brain that is the cause for one to be aware during a certain event even if there are clear signs of a reaction to a stimulus. Similarly, even though consciousness plays a major role in memory, we currently lack the facilities to research it fully. In the future, there is a possibility that we will be able to access such facilities, however, which could allow us to find the neural connections and its ties to mental illness as well. Not only would this help several people by making us aware of the causes of our perspective on certain things, but it would also give us a better chance of recognizing signs of possible mental illness or other issues beforehand.
As it seems, researchers continue to study this unknown part of our mind and once we are able to fund and recover from the global pandemic, we will be able to answer one of psychology’s most difficult questions.
Blackcoffer Insights 21: Amara Arora and Vaanya Kaushal, Scottish High International School 


Will we ever colonize outer space?
Nature has blessed humanity with earth, a rounded globe where all essential necessity lies in the hand of a human. Though in this 21st-century building Space shuttle and exploring space has become an obvious activity among the human raise, now with the inclusion of Start-ups like Space X once dreamed for reaching out at space now is coming true. We all have to agree upon a fact that in the Growth of industrialization we have generated a humungous carbon footprint and colonize at outer space will create a much-needed area for our survival.
With time the human race has visualized and has undergone many transformations, and these transformations may for starters were not that effective but were a major role at existence to this age. Be it a natural transformation like shifting of tectonic plates or manmade like Dumping water and damaging water bodies, somehow have affected usual life of humans so creating space of humans is what next humans are eyeing for.
All this idea for Colonizing in outer space emerged when NASA was able to launch a space shuttle with a heavy payload in the late 1990s this idea didn’t stir brain at that time but later was like apple eyes when various articles from astronauts and their writeup came into limelight and this idea saw its progression, space exploration was not a new normal, companies were petrified by the number of investments that these kinds of projects ask for. But things change with the entry of PayPal’s owner Elon Musk, in 2002 where he laid stones of Space X, with NASA providing the launchpad for their exploration.
Being space travel legitimate there where many bottlenecks knowing the nature of outer space will boost the idea of colonizing, here one has to realize the optimum. But the main target that these company put forward with themself was to not just get into other planet but to get back at earth safely, earlier these projects due to high fund was always subdued and priority was given to the rocket technology. Thus, this unimaginable quest for the search of space was far-fetched though.
Now being in 2020 we do not find it out of a place of colonizing outside of space, the kind of emergence of technologies has led to the escalation of spreading the roots of human existence, the plan for reaching out to outer space too plays a key role for this idea too. Proper Planning, Analysing, Organising, and Executing were required for these projects to be successful for deeper know how I would like to mention here an example of Space X, According to them they wud be successful for sending Rockets by 2022 and would able to colonize by 2050, For achieving these vision they have planned a total of 36 times pre-launch of the same rocket which they will be launching by 2020. Not only that they have already placed their star link satellites which will provide the basic network connectivity for their colonized world, they have planned this to that extent that they have targeted a profit of $ 22 billion yearly by 2025. Already they have sent the first manned rocket successfully what Elon musk plan is to create that infrastructure by which going to Mars with cost as a regular air flight ticket all this will lead to better chances for colonizing at another planet. With setting up their commercial flight once started to the red planet it wud just be a stepping stone for the colonizing at the outer space.
But the fundamental change that people have to undergo while living on another planet will be a change in gravity and being able to be near to live support if any technical issues arise to their suit until unless we find a similar place like the earth that is the change which humans have to adjust. Aspects like Ultraviolet rays too will play an immense role in the survival, but with the rate of technology expanding and challenging at very boundaries there are not many days left when we find solutions to these aspects but this idea of colonizing has more to do with monetary terms it has been projected that building a colony at space will cost at about $10 Trillian which is more than many countries GDP. Placing that much of an amount and getting funding for such highly anticipated projects is tough. Elon to have addressed with these questions with an open Door he believes that this cost is nothing when we find a route to colonize space and these costs will be justifiable when space exploration will reach new heights.
Also, our world is around water and the percentage of land comparison to water is too less. Whereas the population is increasing with the speed and the level of excitement in order to search for new things and the obsession with the new and innovative things never going to finish. Because in the end, we do not satisfy what we have achieved, we are only looking for other things and crave to achieve that.
In order to achieve that planning is most crucial stage. As of now we have searched 9 planets but still didn’t shortlisted the outcomes. But according to the 3-decade theory of ELON MUSK, it seems to be possible.
As the most important thing to survive is the atmospheric environment, whether is this suitable or not, and in order to make it suitable what steps had to take and maintain the suitable level of oxygen.  The missions are now what everyone is focusing about. Finding the source of water, because its 2nd crucial thing which is needed to survive.
Apart from the excitement and the things, the threats or the uncertainties are bound to happen like any kind of mammal which could occur during construction or after that. So the security for that or any protection so that uncertainties could be replicated.
After the basic things, other factors like what are the natural habitats available there, whether the plants could be grown over there or not. If yes, then what types of plants could survive there.
The environment for the animals is safe or not. Because to balance the nature we need all types of animals, natural habitat, even insects and other small things which seems to be ignorable but plays an important role in maintaining the environmental balance. For e.g.  – Ants are among the leading predators of other insects, helping to keep pest populations low.  Ants move approximately the same amount of soil as earthworms, loosening the soil in the process and increasing air and water movement into the ground.  They keep the ecosystem clean of dead insect carcasses and aid in the destruction and decomposition of plant and animal matter. So, every small thing contributes in its own way.
Another matter of concern is the level of artificial intelligence and machine learning required to maintain that kind of requirement. Because as the current scenario, we need energy to run any vehicle or even for the robotics. So, the availability of the energy and the source of energy is needed.
And apart from this, the heat of sun is equally important in order to maintain the level. Because with artificial air we can survive but not for so long. So, in order to remain healthy as well the calculation has to be done. When thinking about disease, you may have heard in the news about the concern that antibiotics (such as penicillin) – which help fight infections – may eventually stop working. Global health organisations are trying to reduce the use of antibiotics, especially for conditions that aren’t serious. This is because their overuse in recent years means that they’re becoming less effective.
But still we can survive without antibiotics. Because there are ways of dealing with diseases caused by bacteria such as isolation which is how they were treated before penicillin.
It might sound like something set firmly in the realm of fantasy, but experts in private industry and governments around the world are trying to understand how feasible it would be to establish a lunar base. Some scientists think humans could survive comfortably on the moon. In some ways, the very minimal gravity of the moon might actually be more conducive to life than the microgravity astronauts experience on the International Space Station.
Although it hasn’t been formally tested, some experts hypothesize that the small amount of gravitational force put on an astronaut’s body when on the moon could help stem some of the adverse effects like bone-density and muscle loss that space flyers experience while living in microgravity on the International Space Station.
By using the moon’s indigenous material, space agencies can save money on the cost of flying pricey missions to and from the moon’s surface. Once on the moon, instead of having to stage costly missions aimed at delivering oxygen and other necessary resources from Earth, experts might be able to actually use the material to manufacture gasses needed to sustain life on the satellite.
On earth, we are protected from radioactive solar winds and cosmic rays by our own magnet field known as the magnetosphere. But out in the space, we can’t rely on this type of protection, and in order to make that protection, it could take a lot but yes with more upgraded technology it could achieve.
Although till now the approximate value of the project is $10 trillion. But the aim is to make the fare equivalent to the normal air fare.
As once we are comfortable with all the resources, securities for uncertainties, risk analysis, and the probability of survival of the human being and many other factors after consideration, then development won’t take much time.
Because to construct a building it take normally 8-9 months. Which is totally acceptable.
But the main concern is the density of the land, i.e. whether the land has the capacity to bear that much weight or not. And the type of land is adaptable for cultivation or nor. Although land can be converted into different types of cultivation but with the help of upgraded technology.
In order to transport the equipments on the regular basis, a large amount of fuel Is required, and other factors as well and Built the propellor depo for rocket landing building various infrastructure for the city one by one will create a way forward for idea of Colonizing at outer space.
Blackcoffer Insights 21: Anubhav Sassendran and Shivay Agarwal, NEW DELHI INSTITUTE OF MANAGEMENT.


What is the chance Homo sapiens will survive for the next 500 years?
We’ve really done it this year. Like an insatiable glutton, the law of averages has come home to roost. We should’ve taken the hint when on the 1st of January, 66 people lost their lives in the Jakarta floods. What followed was like the highlights reel of a disaster movie franchise – a volcanic eruption in The Philippines, irrepressible bushfires in Australia, earthquakes in Russia, Iran, Turkey, India, and China. And speaking of China.
2020 has brought home the fragile mortality of the human race into sharp focus. As global Covid-19 deaths stoutly push past the grim 1 million marks, we have no choice but to question our place in the universe – are we the all-conquering masters of our domain, or mere tourists in a ruthlessly apathetic ecosystem? Is the human race on the ubiquitous three-part literary arc that defines every story, every life, every civilization – ascent, apex, and descent? Maybe when Michael Jackson unveiled his moonwalk in 1983, or when Barack Obama stepped into the White House as President of the United States in 2008, or indeed when MS Dhoni lifted the Cricket World Cup in 2011, we peaked, as a species, and everything since then has been a steady unraveling.
500 years is a long time. For context, the world population in 1500 AD was a mere 461 million. The 16-fold explosion since then is unprecedented in history, but we might just be at the tip of an iceberg. Though fertility rates are dropping and more and more people are foregoing the chance to have babies, we might just have crossed the threshold – the population projections for the year 2050 is 9.8 billion, and for 2100 is a whopping 11.2 billion1. Somewhere out there, Malthus is cackling in his grave. The year 2500 suddenly seems a long way off, and this conversation seems ever-more pertinent today. The Bulletin of Atomic Scientists is not optimistic – the famed Doomsday Clock they maintain is the closest to ‘midnight’ (our proximity to global catastrophe), since its inception in 1947. Global warming? Check. The threat of nuclear war? Check. Ongoing pandemic? Check, check, check.
And yet, hope floats, for three reasons. Mankind may just have its back to the wall right now, but there are three shoots of potential that might just help us make it to 2500 AD – the advent of a basket of disruptive technologies (artificial intelligence, bio-enhancement, genetic engineering), the private sector focus on space exploration and terraforming, and good old fashioned human resilience. While the first two factors will no doubt be critical to human survival, it is the third one that we must pin our hopes on – our long-demonstrated history of surviving whatever nature, the universe, or our own self-destructive tendency, throws our way.
The Next Superman?
In the 13th century, in the Italian town of Pisa, an enterprising tinkerer developed the first eyeglasses, for a local friar with weakening eyesight2. All of a sudden, there existed an external device that could amplify our senses, a tool that gave us an advantage in survival. Today, LASIK surgeries obviate the need for eyeglasses entirely. Hearing aids give the gift of auditory perception back to those who had gotten used to a world of muffled voices and unheard sounds. The iPhone routinely comes with an augmented reality tool that allows us to measure the length of objects in front of us.
But the real science starts where the imagination ends – augmented reality glasses, smart wearables, and virtual reality tools will be ubiquitous in the next few decades. But what after that? Science has the answer, and it’s both thrilling and scary. Artificial intelligence has become the stuff of fable, the filler for all questions left unanswered. But AI, combined with bio-enhancement and genetic engineering, might just lead to the evolution of what some are calling Homo nouveau3. Homo nouveau will be smarter, faster, more agile, and better equipped to adapt to what promises to be a world that is VUCA beyond our imaginations. What might such a human being look like?
They might have a small chip embedded in their brain that utilizes artificial intelligence for enhanced sensory perception. What does that mean? It means that they would be able to see better, focuses their attention for longer, hears what they want to hear, and communicate the appropriate reaction to the rest of the body. Through a chemical in the blood, this AI chip would be able to demand the appropriate response from the body. Bio-augmentation of limbs and organs, internal and external, would mean that what a person can or cannot do is no longer determined at birth, but can simply be bought. All of a sudden, the average Joe can run faster than Usain Bolt, swim better than Michael Phelps, and…fly? Maybe. Genetic engineering will be the missing link. Already there are feverish conversations about a dystopian future featuring designer babies and a digital divide that simply cannot be overcome because it is inbuilt into one’s DNA. The breakthrough with CRISPR-Cas9 might just be the key to unlock the mysteries of DNA manipulation. So what if there’s no food left? Our body AI will adjust our appetite accordingly. No water? Absorb humidity from the air through specialized pores in the skin. The human being in 2500 AD may not be how we recognize one today. That may be our only shot.
Galactic Dominance
SpaceX, led by its mercurial leader Elon Musk, has been the leader here. The SpaceX Mars Programme is based on a very simple premise – as the earth’s closest planet in terms of distance and terrestrial conditions, Mars would be our best bet for colonization. Musk has invested billions of dollars in the Mars Space Programme and remains a fervent believer in the concept. And among tech visionaries, Musk is not alone. Jeff Bezos, the richest man in the world, owns Blue Origin, which simply aims to make spaceflight cheaper through incremental technological growth. Bezos, who took an online retailer of books and turned it into an ever-expanding behemoth, is not a man who thinks small. With more and more business leaders finding spaceflight and planetary colonization a tantalizing prospect, there will inevitably be a concerted push to developing an actual colony on another planet. And when the pull-factor to this development starts hitting diminishing returns, there will be the inevitable push factor as global warming and the possible increase in the eruption of pandemics begin to take their toll. It might just become a more feasible option for people to find an alternate home, if not on Mars, then on one of Saturn’s moons.
A human colony on Mars sounds like a concept straight out of science fiction, but so did a permanent station in Antarctica until a few decades ago. Mount Everest seemed like an unapproachable summit until someone went ahead and planted a flag at the peak. Today, hundreds of people every year attempt the climb. As spaceflight becomes more reasonable, as the urge to explore supersedes the inertia of investment, we become closer to our best chance of survival – leaving planet earth behind and finding another home, perhaps one more forgiving of our follies.
The Human Spirit
The introduction to this article includes an illustrative list of disasters and misfortunes that have struck the global community this year. And yet, we survive. Businesses surge ahead, people adapt to a world of masks and social distancing, the world goes on. Earthquakes, tsunamis, pandemics, we’ve seen them all before and survived, and the human race, in its intrepid exploration of the world, pushes onward and upward. Technological visionaries envision a future that the rest of us cannot see, and then invest money in their vision. Gradually, what seemed unthinkable suddenly becomes real – the moon landing is the best possible example of that. Google launched Google Glass as the first augmented reality wearable technology, and suddenly we could foresee a future with smart eyewear. While the product didn’t quite catch on, that doesn’t mean other companies aren’t trying. Bio-augmentation is already a fast-developing industry, while CRISPR-Cas9, the “genetic scissors” is being held back only by regulatory bottlenecks. How long before the prospect of fiddling with the gene code becomes not a luxury but a necessity? The human spirit, the tendency to survive and thrive at all costs, will eventually win.
The human race will survive to 2500, of that there is no doubt. The real question is, will the person who exists in 2500, with bionic chips and a bio-enhanced body structure and a modified genetic code, be called a Homo sapien? Or is Homo nouveau the way forward?
Blackcoffer Insights 21: Ayush Kumar, XLRI Jamshedpur


Why does your business need a chatbot?
Mr. Sakamoto is a bonsai artist, lives downtown Kyoto, Japan. Bonsai is a Japanese art form that is the cultivation of small trees in a small or medium container. He is in this field of business for the last ten years and he loves what he does on a daily basis. People from different cities in Japan, come to buy his artwork. But, in the early stage of his business, he was just another regular Bosai artist. His popularity has increased because of the stories he tells for every art he is selling. His customer immediately loved him for this while buying A Bonsai art. He loved to sell his customer with his artwork and a story related to that product which resurrects his presence through his art in customers. Soon enough his business thrived and nowadays he is always busy. After the digital revolution, although his sales increased, he was not happy. He was not satisfied. Something is eating up his mind day by day. He could not understand what’s the reason behind his remorse.
A chatbot is an awesome invention of the digital revolution. It is the real-life embodiment of Customer Engagement. Every business needs this feature- will be an understatement. Because Customer engagement is the base of any business. The chatbot does the exact thing digitally. It helps customers to answer the queries and tell about the product that the business has to offer is a fully automated way with vibrant customizations.
            Mr. Sakamoto is running his business for the last fifteen years now. And now he is happy that he was not for the past couple of years. The rationale behind his happiness is the chatbot. Though he has a website and app to sell his Bonsai art, he could not engage with the customers in the way he wanted to. He has trained his employees and apprentices, but they are not providing the same service in the way he does. He was unsatisfied and the chatbot relives him from his sadness. He has done all that at affordable prices in no time.
Let’s understand the Mr. Sakamoto’s “Mondai Nai”- perspective about why chatbot is the answer of his problems.
Building the Trust with Customers
This is the fundamental and root cause for which Mr. Sakamoto has to implement the chatbot in the first place. His stories are embedded with each art. The product which he sells needs this story to be with it. His stories which were not been told to all the customers before are now sharing with this chatbot and building the trust between them. We all can agree on this one statement that trust is the most impactful aspect, maybe the best of aspects, in the relation between any customer and business-owner, whether the business is small, medium, or large.
Target the Millennial Customer
Millennials, a word that was not introduced before 1990. The generation which is stuck in between Gen-x and Gen-y. A generation of people accustomed to both digitization and old culture. They are the most targeted audience in this era. What else would be more approachable to this generation than a chatbot? It’s easy, less time consuming to use. Mr. Sakamoto is now one of the few Gen-x persons who can do business with the millennial so easily.
Better Human Interactions
Mr. Sakamoto has some employees and apprentices. He has trained them as per his needs. But there is a limit. They can’t deliver the exact things that he can or the way he does. Neither chatbot can do that exactly, it will accurately follow the instructions. Not only the chatbot solve the queries of the customer in his way, but also give a recommendation to the product in the way he wants too. Is there any more attribute he would care about that? Absolutely no!
24*7 Service
The most expensive thing for any businessman is the return of their customer with an empty hand because of his unavailability. A chatbot is a perfect way to solve it. For Mr. Sakamoto’s customized chatbot is not only present for customers in their availability, but also It was giving the response to the customer in all the time zone, all over the world. Mr. Sakamoto wouldn’t be happier for this feature.
Scaling Up Business in Globalization
In the era of globalization, what is more, helpful than a chatbot? It can be customized in any language. It can be accessed by any human being residing in any part of the world. And at the end of the day, when that foreigner is happy with the buy and the felicitation and says “Arigato” but in his own language, at that moment he feels so warm and happy.
Handling a lot of customers is really hard. He assigned employees, but the rate is growing fast. So, in the competition of the market, Mr. Sakamoto took the help of a chatbot. Now this automated customized application is handling a greater number of customers than his employees can do. His business is growing exponentially.
Now, when you found out why Mr. Sakamoto is happy and his business is growing in this competitive industry then, do you still have a question in mind that why does your business need a chatbot?
Blackcoffer Insights 20: Aniket Bhunia, Netaji Subhash Engineering College


How you lead a project or a team without any technical expertise?
Have you ever wondered what’s common between Sir Alex Ferguson, Pep Guardiola, and Jose Mourinho? I know it might look a lame question, especially to the ardent football followers. At first instinct, the answer is that they are regarded as the top managers in the football universe. #UCLwinners. But when given a closer look, these managers, weren’t that great when it came to their careers as football players. Any naïve observer can build a fallacy of causation and correlation that those who don’t have a great career as a football player will go on to become a great football manager. But is this restricted just to football? Mike Brearley, one of the greatest captains in the history of cricket and the author of the book the art of captaincy, wasn’t the prolific batsman of his time. Our former President, A P J Abdul Kalam didn’t have a political background, but was still appointed as the president of our country and went to become one of the finest presidents our country has ever had. So, can it be stated with confidence that great leaders need not be the expert in their respective fields?
Let’s understand the leadership role, types of leaders, and explore a few more perspectives before we arrive at a conclusion. 
Let’s start by first understanding what or who is a leader. A leader is not a position or a designation. It’s a virtue if employed correctly. From a janitor to the CEO, from a student to a researcher, from a content writer to a philosopher everybody is a leader. (#leadersareeverywhere) So, what are the virtues of a leader? One of the famous personality trait theories, the Big 5 personality trait theory is always linked closely with leadership. As per the various researches, the personality traits of extraversion, openness, and conscientiousness are associated with leaders more often than the other two traits. Not just these humble and empathetic are also considered as the adjectives that are frequently associated with leaders. But nowhere is technical expertise related to a leader. #NOTSOTECHNICAL
As stated by Colin Powell Leadership is the art of accomplishing more than the science of management says is possible. #It’sART If leadership is an art and leaders are the artists then more or less the leaders can be viewed as directors of the films who are organizing, arranging, managing, and directing the scenes. Another analogy for that can be the orchestra where the conductor is using the baton/stick to set the rhythm of the band. So, is it necessary for the conductor to himself or herself by a great violinist? No. When it comes to the corporate sector the leader need not be an expert on the technology but should know how the technology functions so that he/she can utilize it in the best possible way. They can allocate the task of building technology to the right technical experts. And how do the leaders identify the right person for that job? Or how do leaders know what qualities to look for in a person? #Experience. Every great leader once started off as one of the frontline workers or at the bottom of the pyramid in the hierarchical structure. Even though they might not have had the technical expertise in their respective fields, their visionary outlook, their management skills, and the experience they gathered helped them in achieving their targets. Leaders can always take the assistance of the subject matter experts to guide them in situations of technical difficulties. As per the great man, situational and trait theory of leadership, leadership is innate to the person. Leadership though can be conditioned or can be brought out in a person by rewarding him, great leaders show unconditional attachment towards their vision. Everyone knows how to read a map, but which path to choose isn’t something that can be taught. Leaders give the direction or vision to the company and drive it closer to the target that the company wants to achieve. Leadership theories don’t focus on the technical knowledge of the leaders, but rather more on people skills, their approach towards a problem, and how well they guide and hire others to get the job done, which is also evident as per the managerial skills required as per Robert Katz.
Though it can be argued that technical experts are also the great leaders in their areas, but these leaders are generally at the middle management level. As per the different leadership styles, a servant leader is the one who always tries to achieve the goal of the team. These leaders are the ones who have a management objective laid out in front of them and they drive the team closer to achieving the target by having the people-first mindset, a collaborative approach towards solving the problem. These are the people or the leaders needed at the execution level of the project or those who are too close to the employees at the bottom-most level. Since the managers at the top of the pyramid don’t get much time to invest in people they appoint the right people on their behalf to get the job done.  
Consider an IT firm. So as per the points mentioned above, does it mean that a leader or the manager shouldn’t know how to code? Depends. If the person is the first line manager then he or she should probably know how to code so that he or she can help the team in achieving the target of producing less garbage which might be one of the project’s KPIs. The project manager, who might be an operations manager, (#MBA) from an IT background, probably need not know what line of code is to be written but should know the algorithms or the logic behind the code so that he or she can validate the code from the client’s perspective. The CEO, might not be from an IT background but should know how to run an IT firm rather than the syntax of the code.
But in today’s competitive world leaders can’t be one dimensional. They need to have expertise in multiple areas to be at the top. Mike Brearley if got a chance to play a T20 match, won’t even get picked into aside, not because of his age, but because of his not so great looking batting statistics(#Thegamehaschanged). His leadership skills won’t be enough to get him into the squad. But he can be a great leader if selected to coach the top 15 captains in the world today. As we have already discussed the film directors, their job isn’t to teach the cameramen on how to set up or operate (lens settings, etc) the camera. Their job is to guide the cameramen on what angle it should be held and how he or she should coordinate with the lights crew so that a perfect shot is captured. So even if a leader may not be a technical expert of his field, but if he or she has the right approach and mindset, understands the system and its process, then he or she can hire the best talent for the job and can get the work done by guiding the talent, with his or her vision.
So, the answer to the question, which we came across in the first paragraph, is yes and no. Depending upon the position in the hierarchy (yes for a front-line manager and no for a CXO) and the type of the leader you are, the level of technical expertise needed varies. Though as stated earlier by Robert Katz, the CXO’s need to have the least amount of technical expertise, the CXO should know how the technology functions or what are the applications. What processes are to be followed to build the technology.  He or she should now what is to be done rather than how it is to be done, (#itsnothowitswhat). Otherwise, even though the company might be sitting on a gold mine but If the leader isn’t aware of that or isn’t learned enough to know what to do with it, no company can succeed even with having all the resources at their disposal. Leaders command respect and don’t demand it and that’s possible when they give value to their front-line employees, seek timely advice from the technical experts, and put the resources to the best of its uses. Otherwise, you will be surrounded by technical experts like dilberts. (#dilbert)
Blackcoffer Insights 20: Rohan Mehta, Symbiosis Institute of Operations Management


Can You Be Great Leader Without Technical Expertise
The word “leadership” can bring to mind a variety of images. For example, A political leader, pursuing a passionate, personal cause or An executive, developing his/her company’s strategy to beat the competition. Leaders help themselves and others to do the right things. They set direction, build an inspiring vision, and create something new. Leadership is about mapping out where you need to go to “win” as a team or an organization; and it is dynamic, exciting, and inspiring.
In my point of view we cannot become a good leader without technical expertise because every task in the organization it’s required a lot of skills, knowledge, consistent, enthusiastic, patience, motivating, inspiring, industrious, and critical thinking on the basis of the field we are doing. A leader should have the ability to motivate self and others, effective oral and written communication, critical thinking skills at the working team and delegating a task. Good leaders do have these abilities and if we wanted to create a future leader. They need to take in a large volume of information and to take the essential elements that define the core problem to solve. They need to organize a team to solve these problems and to communicate to a group, they need to established trust with a group and use the trust to allow the team to accomplish the work more than it could be done alone. Though all these skills we have but it’s would not sufficient to make us a great leader because to excel and utilize these abilities in practice we need a lot of technical expertise in a particular domain. For example, Like in a hospital if the head of the hospital is lead by other people rather than Doctor then the hospital would become worse at a point because the person from other fields he could not lead and understand the real scenario and mechanism that the hospital is functioning and he would not understand the staff and the patient as well. Hence, to lead the hospital Doctor is deserved who knows his own field how to lead and understand the real scenario of it. That’s why being a leader requires technical expertise to have great knowledge and understanding about his own organization. Every person if they want to become a leader they should have technical expertise on their own organization so that their organization would run smoothly, hence nobody could run other organization if we do not have the technical expertise on that field.
So, be a leader, not a boss which makes your life much better, as they’re a saying that “ A good objective of leadership is to help those who are doing poorly to do well and to help those who are doing well to do even better.”
Blackcoffer Insights 20: Caldinus Nongrang, NIT Meghalaya


How does artificial intelligence affect the environment
When we talk about AI, different people have different perspectives about it. There is one group of people, having good knowledge about the real potential of AI, who believe that AI can be a novel solution to many problems that the world is facing today. There is another group of people who are merely threatened by the thought of AI taking over the world.
The field of AI took birth when Alan Turing in 1950 had this thought “Can machines think?”. Later in the 1980s, with the adoption of “expert systems” by the companies around the world, the booming of the field of AI was initiated. Initially, it was a matter of awe for everyone to see the results of what AI can achieve. AI’s growth in the past few decades has been exponential and it has transformed the way we live, work and solve challenges. AI has made its impact in a wide variety of areas including healthcare, education, business and many more. But, one of the areas of highest impact that AI has made recently is in the environment and climate change.
Is AI Revolutionizing the way we deal with Environment and Climate Change?
AI can strengthen climate predictions, enable smarter decision-making for decarbonising industries from building to transport, and work out how to allocate renewable energy. In recent years, AI has been a game-changer in how many people, as well as organizations, deal with climate change. Microsoft believes that artificial intelligence, often encompassing machine learning and deep learning, is a “game-changer” for climate change and environmental issues. The company’s ‘AI for Earth’ program has committed $50 million over five years to create and test new applications for AI.
AI is increasingly used to manage the intermittency of renewable energy so that more can be incorporated into the grid; it can handle power fluctuations and improve energy storage as well. Wind companies are using AI to get each turbine’s propeller to produce more electricity per rotation by incorporating real-time weather and operational data.
AI can also improve energy efficiency on the city scale by incorporating data from smart meters and the Internet of Things (the internet of computing devices that are embedded in everyday objects, enabling them to send and receive data) to forecast energy demand. Besides, artificial intelligence systems can simulate potential zoning laws, building ordinances, and flood plains to help with urban planning and disaster preparedness. One vision for a sustainable city is to create an “urban dashboard” consisting of real-time data on energy and water use and availability, traffic and weather to make cities more energy-efficient and livable.
Hotter temperatures will have significant impacts on agriculture as well. Data from sensors in the field that monitor crop moisture, soil composition and temperature help AI improve production and know when crops need watering. Incorporating this information with that from drones, which are also used to monitor conditions, can help increasingly automatic AI systems know the best times to plant, spray and harvest crops, and when to head off diseases and other problems. This will result in increased efficiency, enhanced yields, and lower use of water, fertilizer and pesticides.
But, is AI as good as it looks or does it have a downside?
When we talk about the effect of AI in the environment and climate, both sides of the coin must be elucidated. So, apart from all the positives discussed above, the usage of AI also has a downside.
Last year’s World Economic Forum report showed that while AI can address some of Earth’s environmental challenges, it is important to manage it properly. According to the forum and experts in the field, AI has the potential to accelerate environmental degradation. The use of power-intensive GPUs to run machine learning training has already been cited as contributing to increased C02 emissions.
Although AI has been around for about half a century, the question of environmental impact – and other ethical issues – is only arising now because the techniques developed over decades can now be used in combination with an explosion in data and strong computational power.
For all the advances enabled by artificial intelligence, from speech recognition to self-driving cars, AI systems consume a lot of power and can generate high volumes of climate-changing carbon emissions.
A study last year found that training an off-the-shelf AI language-processing system produced 1,400 pounds of emissions – about the amount produced by flying one person roundtrip between New York and San Francisco. But there are ways to make machine learning cleaner and greener, a movement that has been called “Green AI.” Some algorithms are less power-hungry than others, for example, and many training sessions can be moved to remote locations that get most of their power from renewable sources.
The key, however, is for AI developers and companies to know how much their machine learning experiments are spewing and how much those volumes could be reduced.
What can be done?
To prevent this, the proposition by the World Economic Forum report is that advancements in “safe” AI should be pursued, to ensure that humanity is not developing AI that is harmful to the environment. Specifically, the World Economic Forum said in its report that AI developers “must incorporate the health of the natural environment as a fundamental dimension.” This means safeguarding against models that will demand the consumption of energy or natural resources beyond what is sustainable, among other factors. In a sense, all programs need to be designed with the dimension of environmental protection and improvement in mind.
In the future development of AI programs, it will also be important to note the environmental impact of creating these systems in the first place. According to an academic study on energy usage for deep learning processes, the creation of an effective AI might be costly to the environment. Nearly 300,000 kilograms of carbon dioxide equivalent emissions are created during the process of training a single model. This is basically equal to the emissions of five average cars in the United States. Considering the negative environmental impact in addition to the positive implications of AI and climate change will be crucial, moving forward.
There is always room for improvement, some pioneers of machine learning recently published a paper called Tackling Climate Change with Machine Learning was discussed at a major AI conference. David Rolnick, a postdoctoral fellow of the University of Pennsylvania said “call to arms” which means to bring researchers together.
This paper covers thirteen areas of research where machine learning can be used, including energy production, CO2 removal, education, solar geoengineering, finance and many more. The main idea is to build more energy-efficient buildings, creating new low-carbon materials, high tech monitoring of deforestation and greener transportation sources. Artificial intelligence and machine learning can be a medium but the real work is to be done by us as a caretaker of mother earth.
How about better forecasting of climate change?
 The kick start was already done by climate informatics in 2011 which regulates the collaboration of data science and climate science. It covers a wide range of topics like improving prediction of extreme events such as hurricanes, paleoclimatology, collecting data from ice cores, climate up-down scaling and its impact on us.
“There’s a lot of uncertainty,” Monteleoni
Claire Monteleoni, a computer science professor at the University of Colorado concludes that AI/ ML models generally forecast for the short-term. There is a significant difference when it comes to long-term forecasting.
The first system was developed at Princeton in the 1960s. All the AI/ ML models developed till now revolve around atmosphere, oceans, land, cryosphere or ice. AI can help to achieve new heights with the amount of data collected at this point of time, and compute more complex climate conditions using climate modeling algorithms.
What about showing the effects of extreme weather?
We all know that climate change is real and the rise in atmospheric temperature the proof, but to make it more realistic, Montreal Institute of Learning Algorithms (MILA), Microsoft and ConscientAI Labs uses Generative Adversarial Networks popularly know as GANs to simulate what mother earth will look like with the rising sea levels, atmospheric temperature, air pollution and intensive storms.
“Our goal is not to convince people climate change is real, it’s to get people who do believe it is real to do more about that,” said Victor Schmidt, a co-author of the paper and Ph.D. candidate at MILA
The project is deployed for the people to look at what their places will look like in the future when there will be a significant climate change.
Can we track carbon emissions?
Short answer, yes. Tracking of carbon emission is one of the agendas of the UN. For this year 2020, the UN’s goal is to prevent new coal plants from being built. Satellite images come into picture using deep learning techniques, data science researchers can analyze these high quality satellite images and track the emission. Google is expanding its horizon of nonprofit’s satellite imagery, including gas-powered plants emissions which can be used by researchers to track the pollution. There are countries organizations which govern CO2 emission on the ground level but these data are unreachable by global monitoring associations.
AI can automate the analysis of images of power plants to get regular updates on emissions. It also introduces new ways to measure a plant’s impact, by crunching numbers of nearby infrastructure and electricity use. That’s beneficial for gas-powered plants that don’t have the easy-to-measure plumes that coal-powered plants have.
Blackcoffer Insights 19: Shivendra Agarwal & Teena Mary (CHRIST University, Bangalore)


How to Overcome Your Fear of Making Mistakes
No one can reduce mistakes to zero, but you can learn to harness your drive to prevent them and channel it into better decision making. Use these tips to become a more effective worrier.
Don’t be afraid or ashamed of your fear. 
Our culture glorifies fearlessness. The traditional image of a leader is one who is smart, tough, and unafraid. But fear, like any emotion, has an evolutionary purpose and upside. Your concern about making mistakes is there to remind you that we’re in a challenging situation. A cautious leader has value. This is especially true in times like these. So don’t get caught up in ruminating: “I shouldn’t be so fearful.”
Don’t be ashamed or afraid of your fear of making mistakes and don’t interpret it as evidence that you’re an indecisive leader, or not bold, not visionary. If you have a natural tendency to be prevention-focused, channel it to be bold and visionary! (If you struggle to believe this, identify leaders who have done just that by figuring out how to prevent disasters.)
Use emotional agility skills. 
Fear of mistakes can paralyze people. Emotional agility skills are an antidote to this paralysis. This process starts with labeling your thoughts and feelings, such as “I feel anxious I’m not going to be able to control my customers enough to keep my staff safe.” Stating your fears out loud helps diffuse them. It’s like turning the light on in a dark room. Next comes accepting reality. For example, “I understand that people will not always behave in ideal ways.” List off every truth you need to accept. Then comes acting your values. Let’s say one of your highest values is conscientiousness. How might that value apply in this situation? For example, it might involve making sure your employees all have masks that fit them well or feel comfortable airing any grievances they have. Identify your five most important values related to decision-making in a crisis. Then ask yourself how each of those is relevant to the important choices you face.
Repeat this process for each of your fears. It will help you tolerate the fact that we sometimes need to act when the course of action isn’t clear and avoid the common anxiety trap whereby people try to reduce uncertainty to zero.
Focus on your processes. 
Worrying can help you make better decisions if you do it effectively. Most people don’t. When you worry, it should be solutions-focused, not just perseverating on the presence of a threat. Direct your worry towards behaviors that will realistically reduce the chances of failure.
We can control systems, not outcomes. What are your systems and processes for avoiding making mistakes? Direct your worries into answering questions like these: Is the data you’re relying on reliable? What are the limitations of it? How do your systems help prevent groupthink? What procedures do you have in place to help you see your blind spots? How do you ensure that you hear valuable perspectives from underrepresented stakeholders? What are your processes for being alerted to a problem quickly and rectifying it if a decision has unexpected consequences?
Broaden your thinking. 
When we’re scared of making a mistake, our thinking can narrow around that particular scenario. Imagine you’re out walking at night. You’re worried about tripping, so you keep looking down at your feet. Next thing you know you’ve walked into a lamp post. Or, imagine the person who is scared of flying. They drive everywhere, even though driving is objectively more dangerous. When you open the aperture, it can help you see your greatest fears in the broader context of all the other threats out there. This can help you get a better perspective on what you fear the most.
It might seem illogical that you could reduce your fear of making a mistake by thinking about other negative outcomes. But this strategy can help kick you into problem-solving mode and lessen the mental grip a particular fear has on you. A leader might be so highly focused on minimizing or optimizing for one particular thing, they don’t realize that other people care most about something else. Find out what other people’s priorities are.
Recognize the value of leisure.
Fear grabs us. It makes it difficult to direct our attention away. This is how it is designed to work so that we don’t ignore threats. Some people react to fear with extreme hypervigilance. They want to be on guard, at their command post, at all times. This might manifest as behavior like staying up all night to work.
That type of adrenalin-fueled behavior can have short-term value, but it can also be myopic. A different approach can be more useful for bigger picture thinking. We need leisure (and sleep!) to step back, integrate the threads of our thinking, see blindspots, and think creatively. Get some silent time. Although much maligned, a game of golf might be exactly what you need to think about tough problems holistically.
Detach from judgment-clouding noise. 
As mentioned, when people are fearful they can go into always-on monitoring mode. You may have the urge to constantly look at what everyone else is doing, to always be on social media, or check data too frequently. This can result in information overload. Your mind can become so overwhelmed that you start to feel cloudy or shut down. Recognize if you’re doing this and limit over-monitoring or over checking. Avoid panicked, frenzied behavior.
On its own, being afraid of making mistakes doesn’t make you more or less likely to make good decisions. If you worry excessively in a way that focuses only on how bad the experience of stress and uncertainty feels, you might make do or say the wrong things. However, if you understand how anxiety works at a cognitive level, you can use it to motivate careful but bold and well-reasoned choices.
Blackcoffer Insights 19: HARSH ARYA & ABHISHEK KUMAR SINGH (AKGEC, Ghaziabad)


Is Perfection the Greatest enemy of Productivity?
What’s perfection really? Does every person expect perfection from oneself or someone else? Or is it in certain amounts? Many times the certain question comes to the mind when we think about what we deem perfect. But how do we verify it?
Let us understand this through the work of three individuals who know are to be put on a task in carpentry on a budget. All are given Rs. 5000 (Assumed) to complete this task. They can’t go beyond this budget.
Manish, Vinay, and Sameer need to make a stool for themselves each. Manish has no understanding of carpentry; he learns from YouTube. Vinay took his carpentry lessons during engineering, so he has a practical idea of how to make a stool. But Sameer is a Carpenter, so he has a total understanding of the material, process, and final product.
Let us start with Manish. Understand that the task is to get their jobs done. Manish has two options here. He can either order parts and assemble it himself or order the material required such as wood, glue, nails, etc and then work on it before the assembly of the final product. The only difference is his lack of expertise in this field.
Vinay has the same options but has practical experience from his carpentry classes during the workshop in Engineering. Sameer has an advantage since he has both skill and experience perfected over a course of time.
All three are given three days for the task. Since Manish finds it easy to assemble the stool using parts procured from another carpenter; he talks to a carpenter he knows and orders 3 leg parts, 6 horizontal support bars, and one circular base which will sit on the top. Vinay thinks he will be able to shape the parts because of his experience as a student and orders several cylindrical wooden leg parts, cuboidal bars, and a thick sheet. Sameer has all the required material in his shop and he begins to work on the stool on the first day.
Manish and Vinay wait for a day for the parts to come. They begin to work on the second day. Manish uses the tricks learned from YouTube videos; uses a hammer to place the nails purchased from a nearby shop in the pre-made hole, applies glue in certain areas as directed in the video, and places weight on the assembled stool. He waits for the glue to cure.
Vinay uses borrowed drill bits, drill machines, cutters, sandpapers purchased from a shop, etc, and starts working on the wooden parts as soon as the parts arrive in the morning. He is able to recall his carpentry instructions and attempts to shape the parts based on drawing of his product he made last night. Sameer meanwhile has completed his assembly and has put the assembly to rest for the glue to cure.
On the third day, each of them has the stool ready. They evaluate their own work and notice certain facts.
Manish notices that he did complete the task but, his stool was wobbly and not glued well. It could fall the moment he sits on it. Also, he forgets to polish the wooden parts and give it a professional finish since he forgot to watch that bit of video over YouTube.
Vinay completes the task his stool is sturdy and cured well overnight. But there are minor issues with it. One of the three legs is cut short by a few mm and the stool, a tad unstable.
Sameer on the other hand completes the task with a good product. The stool is well-balanced, and polished, as he expected from him. Sameer is satisfied with his work and finds it to stool Perfect. Manish thinks he did his best considering he had no experience of the task to be done. He is satisfied with his work. But Vinay is unhappy, he could have done better and made the product better. He doesn’t find his efforts well put and thinks he wasn’t productive enough and hence his stool is nowhere perfect.
Understand that all of them had a set of skills and experience, but only Sameer had the right skills and experience to produce a perfect product. Manish and Sameer were satisfied and happy with their work. Manish made a wobbly stool which could break the moment someone sat on it but happy he gave the task a go. He thinks put the efforts and he did a good job. It was only Vinay who could not overcome the fact that his work was not perfect.
In Vinay’s case, it can be observed that he could not deliver what he learned from carpentry classes. The reason is simple, he put effort but didn’t have the experience. Experience mattered. Since Vinay thinks otherwise, he believes he has the perfect skills to complete the task but he could not deliver the results. He is not satisfied with his results and wishes to spend another day working on the stool on his own to get to his idea of the perfect result.
In this case, the idea of perfection itself is misplaced. You see, the end product made by Vinay and Manish would not match Sameer’s but the task could have been completed had they understood where their shortcomings lied. For both, it was their skill. Sameer had the perfect skill to get the expected result which resulted in better productivity, others didn’t.
And in this story, every individual put effort regardless of the result. But some were satisfied and some dissatisfied. Their satisfaction made their creations perfect in their eyes and dissatisfaction made one believe he wasn’t productive enough.  
“Perfection is subjective and productivity is utility extracted given a certain amount of effort”.
You can try to be as productive to achieve something but ultimately, it’s the satisfaction you get that defines the boundary of perfection. If you aren’t satisfied, you won’t find anything perfect. You would just waste both your time and effort over something that doesn’t suit you in the first place, as explained in the story before.
“It isn’t productivity that will be the greatest enemy but your satisfaction”.
Satisfaction will determine how you perceive perfection and defines where your threshold for perfection lies. The real enemy is dissatisfaction. You get demotivated and restless. You lose sight and discontinue only because you are unable to pass your own expectation. So, it’s the only obstacle that comes in between productivity, from time to time.
Productivity is far from being your enemy. The greatest enemy will be your expectations which lead to dissatisfaction. It will play a major role in crafting your idea of Perfection in every day.
Blackcoffer Insights 19: Jude Francis, Institute of Management Technology (IMT), Nagpur


Global financial crisis 2008 causes/effects and its solution
Brief
The Financial Crisis of 2008 started as a crisis in the subprime mortgage market (i.e. in a market where lending of loans is done to people who may have difficulty in maintaining the repayment schedule or in simple terms, loans were given out to people without proper checks and low credit scores) which ultimately led onto a huge global collapse.
The Financial Crisis of 2008 or Great Recession is considered the worst economic crisis since the Great Depression.
Financial Crisis Explained –
In the year 1996, there was a dot-com boom (or otherwise known as the dot com bubble) in the United States, a period of massive growth in the use of the internet because of which the stock market prices started increasing rapidly. However, around the year 2000, it dropped which led people and investors to withdraw their investments from the stock market rapidly. It led to the decline of the price of shares in stock markets and the interest rate plummeted to around 1% very quickly in a short span.
Investors were now looking for a brighter option than investing in stock markets.
Fig – Rise and fall of the Dot Com Bubble
As interest rates went lower and lower, real estate prices started rising and the US Govt also encouraged people to buy houses and properties. The demand for the same started rising rapidly and investors now found a great option to invest in (i.e. in Real Estate).
During that same time, Investment Banks saw an opportunity, chimed in and started buying loans from Banks in bulk and clubbed multiple loans under a complex derivative called CDO (Collateralized Debt Obligations) and started providing it to these Investors after getting a credit rating of “AAA” (Very Safe Investment) from the Credit Rating Agencies. Investors naturally fell for it and started buying these CDOs.
So now, the risk factor of these loans got transferred from the Banks to the Investment Banks and then again to the CDO Investors.
With the high buying demand of CDOs, Investment banks started demanding or pressurizing Banks to provide even more Loans so that they can provide more CDOs to the Investors. However, Banks had already provided loans to people with good credit history and regular income people. But in the hope of getting even more credit from Investment Banks, these same banks then started giving out subprime housing loans to people with low credit scores.
Approximately $174Bn worth of loans were given out during the period 2000-2007 and most of them were clubbed as CDOs with a “AAA” rating from the Credit Rating Agencies. Approximately 70% of these CDO’s were marked with a “AAA” rating.
Investment Banks and Credit Rating Agencies were now enjoying large profits during this time. Moody’s (a credit rating agency) profits increased 4x times during that period (2000-2007).
Looking at the huge profits being made by the Investment Banks and Credit Rating Agencies, Insurance companies (like AIG) now started giving out insurance on these CDOs to the investors and they called it CDS (Credit Default Swap). AIG believed that since the CDOs were rated as “AAA” (Very Safe Investment), the failure chances of these CDOs were very minimal. They misjudged or were unaware of the fact that some of the loans that were clubbed under these CDOs were Sub Prime loans.
Now CDO Investors started buying out CDS from AIG and other companies to safeguard and protect them from any losses. AIG then started making huge profits because of the premiums that the investors had to pay. But they never realized the outcome if by any chance the CDOs fail at some point in time. Thus, the risk factor again got transferred from the CDO investors to the Insurance Companies.
Fig – Flow Diagram showing how the Risk Factors got transferred
Coming to the loan borrowers now, Sub Prime Borrowers from banks were unaware of the fact of Adjustable Rate Loans (interest rate of these loans keeps changing) and thus had to pay lower interests at the start but more interest rates later on.
The borrowers started defaulting on these loans when the interest rates increased dramatically around 2007 and thus banks had to then resell those houses to make up for the loans defaulted.  Added to the problem was the fact that borrowers were not spending any amount of money from their pockets while taking loans and banks were providing the full amount of loans. Almost 50% of the borrowers did not pay anything from their own pocket and bought the home only using the housing loan.
This led to a huge increase in the defaulters of borrowers and ultimately banks had to auction houses to gain credit back. With the high-interest rates and no one to buy the auctioned houses, this ultimately caused a chain reaction and banks were no longer receiving credit. The prices of Real Estate started falling drastically and people with good credit scores who had earlier taken housing loans also started defaulting because the price of their houses/homes fell below the loan amount that they had taken earlier. Banks stopped receiving money and also because of this chain reaction, the value of CDOs ultimately came down to 0.
Some Investors went for huge losses and few Investment companies went bankrupt (ex – Lehman Brothers). Moreover, insurance companies had to pay back those investors who had taken insurance. As a result, some of the insurance companies also went bankrupt. AIG too almost lost about $100Bn in paying back the investors who had earlier insured their CDOs. However, the US Govt. finally decided to bail out AIG in order to save them from going bankrupt.
CDOs and CDSs were not regulated during that time by the Federal Reserve and thus this whole situation ultimately led to credit crunch (became very difficult to get loans) and the whole economy of the US underwent a crisis that led to a global impact all around the world. Unemployment increased manifold and many new businesses had to shut down. Global trades all around the world also saw a crisis and finally, Global Recession hit the World.
Causes of the Crisis –
Improper screening of loan borrowers resulting in subprime loan borrowers which included even borrowers with no stable income.
Credit rating agencies rated all the CDOs highly and investors were purchasing without proper monitoring. the same goes for Insurance companies.
Lax regulation of CDOs and CDS’ by federal reserve banks.
The large size of interbank lending that compounded the crisis

Effects of the Crisis –
House prices plummeted.
The unemployment rate increased all over the US and the EU.
8M people lost jobs globally
2.5M business devastated
4M houses foreclosed.
Approx. $450Bn losses (Banks and Investment Banks combined)
Ways to Avoid Such Crisis in Future –
Tighter regulations on loan screenings.
Regulations to prevent banks from growing too big and preventing systemic risks (addressing ‘Too Big to Fail’ problem)
Borrowers must be made aware of rate adjustable loans
Credit rating agencies should not rate the CDOs without proper risk assessment.
Partnership with Fintech companies to develop robust alternative credit scoring models for people with less credit history.
Strict regulation on CDOs and CDS’ by Federal Banks
Blackcoffer Insights 19: Neermalya Pratim Das, SCMHRD-Pune


Gender diversity and Equality in the tech industry
Gender diversity is an equal representation of all genders in workplaces. One of the most important sectors where there is under-representation of gender diversity is the technological industry. Gender diversity means you have an equal opportunity which is not limited by gender. But, the true reality is gender diversity is a far outcry in the world.
Gender diversity isn’t a new topic but an old and global phenomenon and technology industry is not a stranger to it.  Women are often underrepresented in the technology sector. To understand the gap, let’s look at some statistics.
The percentage of women employed across all job sectors in the US has grown to be 47%.
Out of 47%. 34% belong to the top 5 technology companies (Amazon, Apple, Facebook, Google, and Microsoft).
Women software engineer hires have only increased 2% over the last 20 years
50% of women said they have experienced gender discrimination at work.
Only 37% of tech startups have at least one woman on the board of directors.
The ratio of men to women in engineering is 5:1.
25.5% of Google’s leaders on a global scale are women.
 Even in the tech conference organizations, there were fewer female keynote speakers than male speakers.
Eight percent to 17 percent of gay and transgender workers report being passed over for a job or fired because of their sexual orientation or gender identity.
Globally, women only earn 77 cents to the dollar when compared to men.
According to a McKinsey Global Institute report, if women were to play an identical role in labor markets to that of men, as much as $28 trillion, or 26%, could be added to the global annual GDP by 2025.
According to UNESCO, over 2.7 billion women are legally restricted from having the same choice of jobs as men. And women’s labor force participation rate is 48.5 % compared to a rate of 75% for men.
After looking at these staggering numbers, we see how biased gender diversity is even in the technological sector.
Need of the hour
With rapid industrialization and education about the importance of rights, gender diversity is very important in today’s world. Gender equality represents a society that has lesser violence and provides a safer world for everyone. It is also directly proportional to sustainable development and ensures human rights to everyone. As today’s markets are increasing along with customers’ preferences, to understand customer profiles and build products we need gender diversity in our organizations. Equal representation helps in a better decision-making process for a positive impact.
Men and Women invest back into their families and having gender equality and diversity helps build values into families and households. Empowering individuals not only help themselves but also the economy of the nations. Gender equality and diversity should be built into organizations’ beliefs and values.
https://www.statista.com/chart/4467/female-employees-at-tech-companies/
Global Gender Gap Report (2017)
Why is this happening?
Perception: If we see the word “gender”, it is a socially constructed definition. And this definition changes as per different cultural norms. In a much broader term, our society is also not aware of the concept of binary and non-binary gender, which includes queer, Trans, and Intersex individuals as well. The socially acceptable thing is to have gender expressions as per our gender identity. So, when it comes to choosing their career, people tend to choose a job role that gives them higher social belongingness. E.g. We see more women(as compared to men) in nursing careers as it requires more feminine qualities.
There are different versions of this perception: which we can see either in the society(in terms of socially assigned gender roles) or in the workplace(in the form of gender discrimination, stereotypical thinking, sexism, etc.). E.g. As per their assigned roles in society, women are most likely to take care of their family and children, and this affects their income and career growth. The motherhood penalty is a term defined by sociologists which states about the inverse correlation between income level and the number of children, i.e. there is also an income difference between a mother and a non-mother employee. As per OECD data(2012), there is a 7% reduction in wages for women per child.
Lack of economic opportunities: Even in the tech industry, women are paid less than men. As per ILO data(2019), on average women are paid 20% less than men worldwide.
Even when it comes to promotion, men are preferred more. It is to be noted that the numbers are even worse when there is intersectionality involved. E.g., a transgender woman will be paid less than a white woman and so on. As per the National Centre for Transgender equality, one out of two transgender people faces adverse effects: including 23% were denied a promotion, 44% were passed over for a particular job position and 26% are fired from their workplace just because they are transgender.
Ways to improve gender diversity:
We can improve this scenario from the perspective of different stakeholders who are involved.
For Society :
Creating Tech awareness from School level: As per the HDR report( 2017), no. of boys pursuing STEM program is 97% higher than girls. One way to improve this is to introduce more strong role models to advocate for this issue. Minority gender communities should be aware of the multiple job opportunities and career growth available in this field. These job roles are not even gender-specific anymore. For developing countries like India, STEM scholarship programs can be introduced from the secondary education level.
Address Bias/Stereotypes: It is to be noted that perceptions, stereotypes, and biases are not just something we learn in school but our upbringing also creates these, the things we watch, listen, and read daily. So, it also becomes the duty of the society to create a more inclusive environment. E.g. A more privileged person should fight for the fundamental right of the less empowered or less privileged person in society.
There should be active encouragement from parents, teachers, and educators for students in STEM programs regardless of their gender identity.
For Companies:
To promote gender diversity in the workplace, companies need to focus on three things:
Unbiased Recruitment policies:
Generally, there is a lot of unconscious bias and prejudices when hiring women, trans, intersex, and queer individuals. There needs to be a diversity and inclusivity team in the HR itself that addresses these issues. A blind recruitment drive can be conducted where there is no need to mention gender in any of the documents.
Having structured questions during the interview might help to remove any unconscious biases.
Diversity sensitivity training: Workplace policies need to create a company image that hires everyone irrespective of their gender identity and can even keep it optional in all the job formalities and documents.
Conducting proper sensitivity training to employees is essential as they need to use correct pronoun/gender-neutral language.
Not just sensitivity training, strong workplace policies need to be in place which deal with any form of harassment or gender discrimination.
 To create accountability and transparency in their process, companies can also share diversity and inclusivity company data.
Retention policies:
Creating Mentorship models:
Have a mentorship program in the workplace that advocates for women, trans, intersex, and non-binary individuals as leaders and encourages role models within this community.
Need to promote diversity in the workplace via proper workplace policies and legal guidelines(e.g. conducting mandatory sensitization training for all employees or framing zero-tolerance policies.)
Including proper maternity leaves, child care facilities, and flexibility of working hours/remote working arrangements can be helpful for minority gender in keeping their work-life balance.
Develop support programs for these communities who are joining after maternity leave. It might be in terms of psychiatric or mentorship support.
Increase Pay parities: It can be done in the following ways:
Create flexibility in job roles so that women can move on to higher positions.
Build an inclusive workplace that supports pay parity— no salary difference based on gender for the same job roles.
Create a transparent performance review process and ensure that there are no unconscious biases during the recruitment process.
Doing a pay review within the organization to check how much pay difference exists before making framing policies.
Lastly, there needs to be proper government policies in place which provide a constitutional or legal framework starting from the basic education level. It may begin from increasing the gross enrollment rates of all students in schools (irrespective of their gender identity) for developing countries( like India) to have proper compensation policies in the workplace(as similar to what states like California, New York has adopted) to ensure pay parity.
Blackcoffer Insights 19: Sushmitha & Sujatha, IIM Kozhikode
 


How to overcome your fear of making mistakes?
The Covid-19 crisis and its fallout including recession, layoffs, and uneven economic pain-as well as recent protests over police brutality and demands for racial justice, have presented many of us with challenges that we’ve not encountered before. The high-stakes and unfamiliar nature of these situations have left many people feeling fearful of missteps. No one can reduce mistakes to zero, but you can learn to harness your drive to prevent them and channel it into better decision making. Use these tips to become a more effective worrier.
Don’t be afraid or ashamed of your fear. 
Our culture glorifies fearlessness. The traditional image of a leader is one who is smart, tough, and unafraid. But fear, like any emotion, has an evolutionary purpose and upside. Your concern about making mistakes is there to remind you that we’re in a challenging situation. A cautious leader has value. This is especially true in times like these. So don’t get caught up in ruminating: “I shouldn’t be so fearful.”
Don’t be ashamed or afraid of your fear of making mistakes and don’t interpret it as evidence that you’re an indecisive leader, or not bold, not visionary. If you have a natural tendency to be prevention-focused, channel it to be bold and visionary! (If you struggle to believe this, identify leaders who have done just that by figuring out how to prevent disasters.)
Use emotional agility skills.                   
Fear of mistakes can paralyze people. Emotional agility skills are an antidote to this paralysis. This process starts with labeling your thoughts and feelings, such as “I feel anxious I’m not going to be able to control my customers enough to keep my staff safe.” Stating your fears out loud helps diffuse them. It’s like turning the light on in a dark room. Next comes accepting reality. For example, “I understand that people will not always behave in ideal ways.” List off every truth you need to accept. Then comes acting your values. Let’s say one of your highest values is conscientiousness. How might that value apply in this situation? For example, it might involve making sure your employees all have masks that fit them well or feel comfortable airing any grievances they have. Identify your five most important values related to decision-making in a crisis. Then ask yourself how each of those is relevant to the important choices you face.
Repeat this process for each of your fears. It will help you tolerate the fact that we sometimes need to act when the course of action isn’t clear and avoid the common anxiety trap whereby people try to reduce uncertainty to zero.
Focus on your processes. 
Worrying can help you make better decisions if you do it effectively. Most people don’t . When you worry, it should be solutions-focused, not just perseverating on the presence of a threat. Direct your worry towards behaviors that will realistically reduce the chances of failure.
We can control systems, not outcomes. What are your systems and processes for avoiding making mistakes? Direct your worries into answering questions like these: Is the data you’re relying on reliable? What are the limitations of it? How do your systems help prevent groupthink? What procedures do you have in place to help you see your blind spots? How do you ensure that you hear valuable perspectives from underrepresented stakeholders? What are your processes for being alerted to a problem quickly and rectifying it if a decision has unexpected consequences?
Broaden your thinking. 
When we’re scared of making a mistake, our thinking can narrow around that particular scenario. Imagine you’re out walking at night. You’re worried about tripping, so you keep looking down at your feet. Next thing you know you’ve walked into a lamp post. Or, imagine the person who is scared of flying. They drive everywhere, even though driving is objectively more dangerous. When you open the aperture, it can help you see your greatest fears in the broader context of all the other threats out there. This can help you get a better perspective on what you fear the most.
It might seem illogical that you could reduce your fear of making a mistake by thinking about other negative outcomes. But this strategy can help kick you into problem-solving mode and lessen the mental grip a particular fear has on you. A leader might be so highly focused on minimizing or optimizing for one particular thing, they don’t realize that other people care most about something else. Find out what other people’s priorities are.
Recognize the value of leisure.
Fear grabs us. It makes it difficult to direct our attention away. This is how it is designed to work, so that we don’t ignore threats. Some people react to fear with extreme hypervigilance. They want to be on guard, at their command post, at all times. This might manifest as behavior like staying up all night to work.
That type of adrenalin-fueled behavior can have short-term value, but it can also be myopic. A different approach can be more useful for bigger picture thinking. We need leisure (and sleep!) to step back, integrate the threads of our thinking, see blindspots, and think creatively. Get some silent time. Although much maligned, a game of golf might be exactly what you need to think about tough problems holistically.
Detach from judgment-clouding noise. 
As mentioned, when people are fearful they can go into always-on monitoring mode. You may have the urge to constantly look at what everyone else is doing, to always be on social media, or check data too frequently. This can result in information overload. Your mind can become so overwhelmed that you start to feel cloudy or shut down. Recognize if you’re doing this and limit over-monitoring or overchecking. Avoid panicked, frenzied behavior.
On its own, being afraid of making mistakes doesn’t make you more or less likely to make good decisions. If you worry excessively in a way that focuses only on how bad the experience of stress and uncertainty feels, you might make do or say the wrong things. However, if you understand how anxiety works at a cognitive level, you can use it to motivate careful but bold and well-reasoned choices.
Blackcoffer Insights 19: HARSH ARYA & ABHISHEK KUMAR SINGH


How Small Business can survive the Coronavirus Crisis
How is Login Logout Time Tracking for Employees in Office done by AI?
When people hear AI they often think about sentient robots and magic boxes. AI today is much more mundane and simple—but that doesn’t mean it’s not powerful. Another misconception is that high-profile research projects can be applied directly to any business situation. AI done right can create an extreme return on investments (ROIs)—for instance through automation or precise prediction. But it does take thought, time, and proper implementation. We have seen that success and value generated by AI projects are increased when there is a grounded understanding and expectation of what the technology can deliver from the C-suite down.
“Artificial Intelligence (AI) is a science and a set of computational technologies that are inspired by—but typically operate quite differently from—the ways people use their nervous systems and bodies to sense, learn, reason and take action.”3 Lately there has been a big rise in the day-to-day use of machines powered by AI. These machines are wired using cross-disciplinary approaches based on mathematics, computer science, statistics, psychology, and more.4 Virtual assistants are becoming more common, most of the web shops predict your purchases, many companies make use of chatbots in their customer service and many companies use algorithms to detect fraud.
AI and Deep Learning technology employed in office entry systems will bring proper time tracking of each employee. As this system tries to learn each person with an image processing technology whose data is feed forwarded to a deep learning model where Deep learning isn’t an algorithm per se, but rather a family of algorithms that implements deep networks (many layers). These networks are so deep that new methods of computation, such as graphics processing units (GPUs), are required to train them, in addition to clusters of compute nodes. So using deep learning we can take detect the employee using face and person recognition scan and through which login, logout timing is recorded. Using an AI system we can even identify each employee’s entry time, their working hours, non-working hours by tracking the movement of an employee in the office so that system can predict and report HR for the salary for each employee based on their working hours. Our system can take feed from CCTV to track movements of employees and this system is capable of recognizing a person even he/she is being masked as in this pandemic situation by taking their iris scan. With this system installed inside the office, the following are some of the benefits:
1)Compliance/litigation needs
For several countries, regulations insist that the employer must keep documents available that can demonstrate the working hours performed by each employee. In the event of control from the labor inspectorate or a dispute with an employee, the employer must be able to explain and justify the working hours for the company. This can be made easy as our system is tracking employee movements
2)Information security needs
This is about monitoring user connection times to detect suspicious access times. In the event where compromised credentials are used to log on at 3 a.m. on a Saturday, a notification on this access could alert the IT team that an attack is possibly underway.
3)Employee login logout software
To manage and react to employees’ attendance, overtime thresholds, productivity, and suspicious access times, our system records and stores detailed and interactive reporting on users’ connection times. These records allow you to better manage users’ connection times and provide accurate, detailed data required by management.
4)If you want to avoid paying overtime, make sure that your employees respect certain working time quotas or even avoid suspicious access. Our system will alert the HR officer about each employee’s office in and out time so that they can accordingly take action.
5)Last but not least it reduces human resource needs to keep track of the records and sending the report to HR and HR officials has to check through the report so this system will reduce times and human resource needs
With the use of AI and Deep Learning technologies, we can automate some routines stuff with more functionality which humans need more resources to keep track thereby reducing time spent on manual data entry works rather companies can think of making their position high in the competitive world.
Blackcoffer Insights 33: Suriya E, Vellore Institute of Technology
Share:


